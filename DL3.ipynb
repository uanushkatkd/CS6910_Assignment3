{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uanushkatkd/CS6910_Assignment_3/blob/main/DL3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jg6JcE0kLuD",
        "outputId": "35a220fb-79a8-41ae-d7e7-15e433bc0c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-03 13:58:37--  https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.139, 173.194.203.113, 173.194.203.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mf8t6uthf9tinolnmua53mrvgasueh6m/1683122250000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=6322abff-1698-4a18-9f54-781f054654b3 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-03 13:58:44--  https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mf8t6uthf9tinolnmua53mrvgasueh6m/1683122250000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=6322abff-1698-4a18-9f54-781f054654b3\n",
            "Resolving doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14030699 (13M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  13.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-03 13:58:44 (112 MB/s) - ‘dataset.zip’ saved [14030699/14030699]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: aksharantar_sampled/\n",
            "   creating: aksharantar_sampled/asm/\n",
            "  inflating: aksharantar_sampled/asm/asm_test.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_train.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_valid.csv  \n",
            "   creating: aksharantar_sampled/ben/\n",
            "  inflating: aksharantar_sampled/ben/ben_test.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_train.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_valid.csv  \n",
            "   creating: aksharantar_sampled/brx/\n",
            "  inflating: aksharantar_sampled/brx/brx_test.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_train.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_valid.csv  \n",
            "   creating: aksharantar_sampled/guj/\n",
            "  inflating: aksharantar_sampled/guj/guj_test.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_train.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_valid.csv  \n",
            "   creating: aksharantar_sampled/hin/\n",
            "  inflating: aksharantar_sampled/hin/hin_test.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_train.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_valid.csv  \n",
            "   creating: aksharantar_sampled/kan/\n",
            "  inflating: aksharantar_sampled/kan/kan_test.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_train.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_valid.csv  \n",
            "   creating: aksharantar_sampled/kas/\n",
            "  inflating: aksharantar_sampled/kas/kas_test.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_train.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_valid.csv  \n",
            "   creating: aksharantar_sampled/kok/\n",
            "  inflating: aksharantar_sampled/kok/kok_test.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_train.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_valid.csv  \n",
            "   creating: aksharantar_sampled/mai/\n",
            "  inflating: aksharantar_sampled/mai/mai_test.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_train.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_valid.csv  \n",
            "   creating: aksharantar_sampled/mal/\n",
            "  inflating: aksharantar_sampled/mal/mal_test.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_train.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_valid.csv  \n",
            "   creating: aksharantar_sampled/mar/\n",
            "  inflating: aksharantar_sampled/mar/mar_test.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_train.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_valid.csv  \n",
            "   creating: aksharantar_sampled/mni/\n",
            "  inflating: aksharantar_sampled/mni/mni_test.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_train.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_valid.csv  \n",
            "   creating: aksharantar_sampled/ori/\n",
            "  inflating: aksharantar_sampled/ori/ori_test.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_train.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_valid.csv  \n",
            "   creating: aksharantar_sampled/pan/\n",
            "  inflating: aksharantar_sampled/pan/pan_test.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_train.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_valid.csv  \n",
            "   creating: aksharantar_sampled/san/\n",
            "  inflating: aksharantar_sampled/san/san_test.csv  \n",
            "  inflating: aksharantar_sampled/san/san_train.csv  \n",
            "  inflating: aksharantar_sampled/san/san_valid.csv  \n",
            "   creating: aksharantar_sampled/sid/\n",
            "  inflating: aksharantar_sampled/sid/sid_test.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_train.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_valid.csv  \n",
            "   creating: aksharantar_sampled/tam/\n",
            "  inflating: aksharantar_sampled/tam/tam_test.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_train.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_valid.csv  \n",
            "   creating: aksharantar_sampled/tel/\n",
            "  inflating: aksharantar_sampled/tel/tel_test.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_train.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_valid.csv  \n",
            "   creating: aksharantar_sampled/urd/\n",
            "  inflating: aksharantar_sampled/urd/urd_test.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_train.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_valid.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw' -O dataset.zip && unzip dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zp0gRaSuyE-",
        "outputId": "4f5c69e0-7950-4302-905c-938abfd9df82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "2.0.0+cu118\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.device('cuda:0'))\n",
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AmhPeAfu4V_"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import (\n",
        "    DataLoader, random_split\n",
        ")  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayQQaqJSu-3C"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aDR-jUyvbRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "efb7df72-d9ab-4c90-c1a6-67522a7430c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Dataset load and Preprocessing\\ndef data_preprocess(path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   english_texts = []\\n   target_texts = []\\n   for index, row in df.iterrows():\\n        ip_text = row[\\'English\\']\\n        op_text = row[\\'Translated\\']\\n        if ip_text == \\'\\' or op_text == \\'\\':\\n            continue\\n        op_text = \"\\t\" + op_text + \"\\n\"\\n        ip_text = \"\\t\" + ip_text + \"\\n\"\\n        \\n        english_texts.append(ip_text)\\n        target_texts.append(op_text)\\n   MAX_LEN_input = max([len(txt) for txt in english_texts])\\n   MAX_LEN_target = max([len(txt) for txt in target_texts])\\n\\n   hindi_vocab = set()\\n   english_vocab = set()\\n\\n   for word in target_texts:\\n     for char in word:\\n       hindi_vocab.add(char)\\n\\n   for word in english_texts:\\n      for char in word:\\n        english_vocab.add(char)\\n\\n   hindi_list = sorted(list(hindi_vocab))\\n   english_list = sorted(list(english_vocab))\\n\\n\\n   encoder_tokens = len(english_list)\\n   decoder_tokens = len(hindi_list)\\n\\n   print(\"###############################\",decoder_tokens)\\n   print(\"###############################\",encoder_tokens)\\n\\n\\n        # Dict for char to index\\n   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\\n   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\\n   target_token_index[\"UNK\"] = decoder_tokens\\n\\n   #print(input_token_index)\\n\\n        # Dict for index to char\\n   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\\n   #print(inv_input_token_index)\\n   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\\n   decoder_tokens+=1\\n   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\\n\\ndef get_data(path,train_path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\\n   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\\n\\n   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\\n   print(decoder_target_data.shape)\\n   for i,target_text in enumerate(df[\"Translated\"]):\\n            #print(\"Ths is i\",i,target_text)\\n\\n            target_text = \\'\\t\\'+target_text+\\'\\n\\'  \\n            for t, char in enumerate(target_text):\\n              #print(t,char)\\n              if t > 0:\\n                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\\n            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\\n   print(\"done\")\\n   return ([[input_token_index[letter] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"English\"]]),   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"Translated\"]]),decoder_target_data\\n    \\n\\ndef load_data():\\n    \\n  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\\n  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\\n  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\\n\\n\\n  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\\n  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\\n  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\\n\\n\\n  encoder_train = [torch.tensor(lst) for lst in encoder_train]\\n  decoder_train = [torch.tensor(lst) for lst in decoder_train]\\n\\n  encoder_val = [torch.tensor(lst) for lst in encoder_val]\\n  decoder_val = [torch.tensor(lst) for lst in decoder_val]\\n  \\n  encoder_test = [torch.tensor(lst) for lst in encoder_test]\\n  decoder_test = [torch.tensor(lst) for lst in decoder_test]\\n\\n  # Pad the list of tensors to make them the same length\\n  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\\n  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\\n\\n  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\\n  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\\n\\n  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\\n  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\\n  \\n  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''# Dataset load and Preprocessing\n",
        "def data_preprocess(path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   english_texts = []\n",
        "   target_texts = []\n",
        "   for index, row in df.iterrows():\n",
        "        ip_text = row['English']\n",
        "        op_text = row['Translated']\n",
        "        if ip_text == '' or op_text == '':\n",
        "            continue\n",
        "        op_text = \"\\t\" + op_text + \"\\n\"\n",
        "        ip_text = \"\\t\" + ip_text + \"\\n\"\n",
        "        \n",
        "        english_texts.append(ip_text)\n",
        "        target_texts.append(op_text)\n",
        "   MAX_LEN_input = max([len(txt) for txt in english_texts])\n",
        "   MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "   hindi_vocab = set()\n",
        "   english_vocab = set()\n",
        "\n",
        "   for word in target_texts:\n",
        "     for char in word:\n",
        "       hindi_vocab.add(char)\n",
        "\n",
        "   for word in english_texts:\n",
        "      for char in word:\n",
        "        english_vocab.add(char)\n",
        "\n",
        "   hindi_list = sorted(list(hindi_vocab))\n",
        "   english_list = sorted(list(english_vocab))\n",
        "\n",
        "\n",
        "   encoder_tokens = len(english_list)\n",
        "   decoder_tokens = len(hindi_list)\n",
        "\n",
        "   print(\"###############################\",decoder_tokens)\n",
        "   print(\"###############################\",encoder_tokens)\n",
        "\n",
        "\n",
        "        # Dict for char to index\n",
        "   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\n",
        "   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\n",
        "   target_token_index[\"UNK\"] = decoder_tokens\n",
        "\n",
        "   #print(input_token_index)\n",
        "\n",
        "        # Dict for index to char\n",
        "   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\n",
        "   #print(inv_input_token_index)\n",
        "   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\n",
        "   decoder_tokens+=1\n",
        "   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\n",
        "\n",
        "def get_data(path,train_path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\n",
        "   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\n",
        "\n",
        "   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\n",
        "   print(decoder_target_data.shape)\n",
        "   for i,target_text in enumerate(df[\"Translated\"]):\n",
        "            #print(\"Ths is i\",i,target_text)\n",
        "\n",
        "            target_text = '\\t'+target_text+'\\n'  \n",
        "            for t, char in enumerate(target_text):\n",
        "              #print(t,char)\n",
        "              if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\n",
        "            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\n",
        "   print(\"done\")\n",
        "   return ([[input_token_index[letter] for letter in list('\\t'+word+'\\n')] for word in df[\"English\"]]),\\\n",
        "   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list('\\t'+word+'\\n')] for word in df[\"Translated\"]]),decoder_target_data\n",
        "    \n",
        "\n",
        "def load_data():\n",
        "    \n",
        "  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "\n",
        "  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\n",
        "  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\n",
        "  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\n",
        "\n",
        "\n",
        "  encoder_train = [torch.tensor(lst) for lst in encoder_train]\n",
        "  decoder_train = [torch.tensor(lst) for lst in decoder_train]\n",
        "\n",
        "  encoder_val = [torch.tensor(lst) for lst in encoder_val]\n",
        "  decoder_val = [torch.tensor(lst) for lst in decoder_val]\n",
        "  \n",
        "  encoder_test = [torch.tensor(lst) for lst in encoder_test]\n",
        "  decoder_test = [torch.tensor(lst) for lst in decoder_test]\n",
        "\n",
        "  # Pad the list of tensors to make them the same length\n",
        "  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\n",
        "  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\n",
        "  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\n",
        "  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\n",
        "  \n",
        "  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "fs2dax9v-SFC",
        "outputId": "56f1ddac-2861-441c-f257-ef6404c76cbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\\n\\nprint(\"##################################################################################################\")\\nprint(\"encoder_input\",encoder_train.shape)\\nprint(\"encoder_input\",encoder_train[0])\\n\\nprint(\"decoder_target\",decoder_train.shape)\\nprint(\"encoder_input\",decoder_train[0])\\n\\n\\nprint(\"decoder_target_train\",decoder_target_train[0])\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "'''encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\n",
        "\n",
        "print(\"##################################################################################################\")\n",
        "print(\"encoder_input\",encoder_train.shape)\n",
        "print(\"encoder_input\",encoder_train[0])\n",
        "\n",
        "print(\"decoder_target\",decoder_train.shape)\n",
        "print(\"encoder_input\",decoder_train[0])\n",
        "\n",
        "\n",
        "print(\"decoder_target_train\",decoder_target_train[0])\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0DxXx1Qbqzu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, file_path, src_lang, trg_lang):\n",
        "        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n",
        "        self.src_lang = src_lang\n",
        "        self.trg_lang = trg_lang\n",
        "        self.src_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n",
        "        print(self.src_vocab)\n",
        "        self.src_vocab['<pad>'] = 0\n",
        "        self.src_vocab['<unk>'] = 1\n",
        "        self.trg_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n",
        "        \n",
        "\n",
        "\n",
        "        # Extract the unique characters in the source and target languages\n",
        "        src_chars = sorted(set(''.join(self.translations[src_lang])))\n",
        "        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n",
        "\n",
        "        # Assign an index to each character in the source and target languages\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(trg_chars)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "        \n",
        "        self.trg_vocab['<pad>'] = 0\n",
        "        self.trg_vocab['<unk>'] = 1\n",
        "        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])\n",
        "        print(self.max_src_len)\n",
        "        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])\n",
        "        print(\"trg vocab\",len(self.trg_vocab))\n",
        "        print(\"src vocab\",len(self.src_vocab))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.translations)\n",
        "\n",
        "    def target_to_one_hot(self,target_word, char_to_idx):\n",
        "        num_trg_chars = len(char_to_idx)\n",
        "        max_target_len = self.max_trg_len\n",
        "\n",
        "        # Create a tensor of zeros for the one-hot encoding\n",
        "        one_hot = torch.zeros((max_target_len, num_trg_chars))\n",
        "\n",
        "        # Encode each character in the target word as a one-hot vector\n",
        "        for i, char in enumerate(target_word):\n",
        "            char_idx = char_to_idx[char]\n",
        "            one_hot[i][char_idx] = 1\n",
        "\n",
        "        return one_hot\n",
        "    def __getitem__(self, idx):\n",
        "        src_word = self.translations.iloc[idx][self.src_lang]\n",
        "        trg_word = self.translations.iloc[idx][self.trg_lang]\n",
        "        #print(src_word)\n",
        "        #print(trg_word)\n",
        "\n",
        "        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n",
        "        trg = [self.trg_vocab.get(char, self.trg_vocab['<unk>']) for char in trg_word]\n",
        "\n",
        "        src_len = len(src)\n",
        "        trg_len = len(trg)\n",
        "\n",
        "        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n",
        "        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n",
        "\n",
        "        src.extend(src_pad)\n",
        "        trg.extend(trg_pad)\n",
        "\n",
        "        src = torch.LongTensor(src)\n",
        "        trg = torch.LongTensor(trg)\n",
        "        trg_one_hot = self.target_to_one_hot(trg_word,self.char_to_idx)\n",
        "        #print(trg_one_hot.shape)\n",
        "\n",
        "        return src, trg, src_len, trg_len,trg_one_hot\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilyi5ifgw-gy",
        "outputId": "9d15d527-854e-453f-b0c1-5e64882d767e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "24\n",
            "trg vocab 66\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "22\n",
            "trg vocab 63\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "26\n",
            "trg vocab 65\n",
            "src vocab 28\n",
            "Source word: torch.Size([64, 26])\n",
            "Target word: torch.Size([64, 20])\n",
            "Target one hot word: torch.Size([64, 20, 63])\n",
            "Source length: tensor([ 7,  7,  4,  8, 11, 11,  9,  5,  9,  4,  5,  3, 11,  5,  7,  5,  6,  6,\n",
            "        10,  8,  8,  7,  8,  4,  8,  6,  5,  9, 10,  6,  8,  6,  7,  9,  5,  9,\n",
            "        12,  6,  6, 12, 11, 11,  8,  4,  6,  5,  9, 10,  7,  9,  5,  8,  6,  5,\n",
            "         8,  5,  9,  4, 14,  7,  8,  5, 10,  5])\n"
          ]
        }
      ],
      "source": [
        "# Print the source word, target word, and source length\n",
        "\n",
        "train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "train_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_train.csv', 'English', 'Devanagari')\n",
        "val_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_valid.csv', 'English', 'Devanagari')\n",
        "test_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_test.csv', 'English', 'Devanagari')\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(test_loader):\n",
        "    \n",
        "    print('Source word:',src.shape)\n",
        "    print('Target word:', trg.shape)\n",
        "    print('Target one hot word:', trg_one_hot.shape)\n",
        "\n",
        "    #print('Source length:', src_len)\n",
        "    print('Source length:', trg_len)\n",
        "    if batch_idx==0:\n",
        "      break\n",
        "\n",
        "#print(\"decoder_target_train\",decoder_target_train[1::])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EldcLxiXvmBC"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, cell_type):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        \n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "        \n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers, cell_type):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        \n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "        \n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(src)\n",
        "        \n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        decoder_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs[t] = decoder_output\n",
        "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
        "            top1 = decoder_output.argmax(1)\n",
        "            decoder_input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geZvxxqWw8ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1085aeaa-0976-4305-b100-245e13d40d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 4.210180282592773\n",
            "Epoch: 1, Batch: 0, Loss: 1.6010288000106812\n",
            "Epoch: 2, Batch: 0, Loss: 1.6040430068969727\n",
            "Epoch: 3, Batch: 0, Loss: 1.6406605243682861\n",
            "Epoch: 4, Batch: 0, Loss: 1.5708993673324585\n",
            "Epoch: 5, Batch: 0, Loss: 1.7897568941116333\n",
            "Epoch: 6, Batch: 0, Loss: 1.7383331060409546\n",
            "Epoch: 7, Batch: 0, Loss: 1.5816158056259155\n",
            "Epoch: 8, Batch: 0, Loss: 1.3727948665618896\n",
            "Epoch: 9, Batch: 0, Loss: 1.5963302850723267\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "EPOCHS = 10\n",
        "trg_pad_idx=0\n",
        "\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM2CzcISvpyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43cca3d-d68d-46d2-eac8-f0f22fda8fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 4.1905717849731445\n",
            "Epoch: 1, Batch: 0, Loss: 1.9107811450958252\n",
            "Epoch: 2, Batch: 0, Loss: 1.8196845054626465\n",
            "Epoch: 3, Batch: 0, Loss: 1.6872750520706177\n",
            "Epoch: 4, Batch: 0, Loss: 1.5151541233062744\n",
            "Epoch: 5, Batch: 0, Loss: 1.4219584465026855\n",
            "Epoch: 6, Batch: 0, Loss: 1.528254747390747\n",
            "Epoch: 7, Batch: 0, Loss: 1.746031403541565\n",
            "Epoch: 8, Batch: 0, Loss: 1.6486035585403442\n",
            "Epoch: 9, Batch: 0, Loss: 1.6557128429412842\n"
          ]
        }
      ],
      "source": [
        "#Training and check accuracy function\n",
        "\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "EPOCHS = 10\n",
        "trg_pad_idx=0\n",
        "\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uplKL9EnvwhV"
      },
      "outputs": [],
      "source": [
        "# final train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxByKGy-vyYZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpaqJJn20FTDSoBJxg7pOm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}