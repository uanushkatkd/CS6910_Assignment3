{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-20T20:14:34.419966Z","iopub.status.busy":"2023-05-20T20:14:34.419237Z","iopub.status.idle":"2023-05-20T20:14:37.895372Z","shell.execute_reply":"2023-05-20T20:14:37.894362Z","shell.execute_reply.started":"2023-05-20T20:14:34.419925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","2.0.0\n","cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","print(torch.device('cuda:0'))\n","print(torch.__version__)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:14:43.542949Z","iopub.status.busy":"2023-05-20T20:14:43.542370Z","iopub.status.idle":"2023-05-20T20:14:43.793975Z","shell.execute_reply":"2023-05-20T20:14:43.793086Z","shell.execute_reply.started":"2023-05-20T20:14:43.542918Z"},"trusted":true},"outputs":[],"source":["# imports\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import pandas as pd\n","import math\n","import torch\n","import torchvision\n","import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n","import torchvision.datasets as datasets  # Standard datasets\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n","from torch import optim  # For optimizers like SGD, Adam, etc.\n","from torch import nn  # All neural network modules\n","from torch.utils.data import (\n","    DataLoader, random_split\n",")  # Gives easier dataset managment by creating mini batches etc.\n","from tqdm import tqdm  # For nice progress bar!\n","\n","from torchvision.datasets import ImageFolder\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pathlib\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:14:45.638402Z","iopub.status.busy":"2023-05-20T20:14:45.637949Z","iopub.status.idle":"2023-05-20T20:14:45.664138Z","shell.execute_reply":"2023-05-20T20:14:45.661394Z","shell.execute_reply.started":"2023-05-20T20:14:45.638348Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=1):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","seed_everything()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:26.123742Z","iopub.status.busy":"2023-05-20T20:24:26.123306Z","iopub.status.idle":"2023-05-20T20:24:26.147215Z","shell.execute_reply":"2023-05-20T20:24:26.146278Z","shell.execute_reply.started":"2023-05-20T20:24:26.123710Z"},"trusted":true},"outputs":[],"source":["# Data Preprocessing\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","'''\n","Class Vovabulary is used to create vocabulary from the training dataset.\n","'''\n","class Vocabulary:\n","    \"\"\"\n","    Args:\n","      file_path (string): The path to the CSV file containing the training data.\n","      src_lang (string): The name of the source language.\n","      trg_lang (string): The name of the target language.\n","\n","    Raises:\n","      ValueError: If the file_path does not exist.\n","\n","    \"\"\"\n","    def __init__(self, file_path, src_lang, trg_lang):\n","        # Read the CSV file into a Pandas DataFrame.\n","        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n","        # It will drop any rows with missing values\n","        self.translations.dropna()\n","        self.src_lang = src_lang\n","        self.trg_lang = trg_lang\n","        # Create a dictionary that maps each character in the source language to an integer index.\n","        self.trg_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n","        # Create a dictionary that maps each character in the target language to an integer index.\n","        self.src_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n","        \n","        # Add special tokens to the vocabularies.\n","        self.trg_vocab['<'] = 0\n","        self.src_vocab['<'] = 0\n","\n","        self.trg_vocab['<unk>'] = 2\n","        self.src_vocab['<pad>'] = 1\n","        self.trg_vocab['<pad>'] = 1\n","        \n","        self.src_vocab['<unk>'] = 2\n","        \n","        # Extract the unique characters in the source and target languages\n","        src_chars = sorted(set(''.join(self.translations[src_lang])))\n","        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n","\n","        # Assign an index to each character in the source and target languages\n","        self.t_char_to_idx = {char: idx+3 for idx, char in enumerate(trg_chars)}\n","        self.t_char_to_idx['<unk>']=2\n","        self.t_idx_to_char = {idx: char for char, idx in self.t_char_to_idx.items()}\n","        \n","        self.s_char_to_idx = {char: idx+3 for idx, char in enumerate(src_chars)}\n","        self.s_char_to_idx['<unk>']=2\n","        self.s_idx_to_char = {idx: char for char, idx in self.s_char_to_idx.items()}\n","        \n","      \n","\n","\n","    def get(self):\n","         # This function returns the source and target vocabularies, as well as the dictionaries that map characters to integer indexes and vice versa.\n","        return self.src_vocab,self.trg_vocab,self.t_char_to_idx,self.t_idx_to_char,self.s_char_to_idx,self.s_idx_to_char\n","        \n","\n","\n","class TransliterationDataset(Dataset):\n","    \"\"\"\n","    Args:\n","      file_path (string): The path to the CSV file containing the training data.\n","      src_lang (string): The name of the source language.\n","      trg_lang (string): The name of the target language.\n","      src_vocab (Vocabulary): The vocabulary for the source language.\n","      trg_vocab (Vocabulary): The vocabulary for the target language.\n","\n","    Raises:\n","      ValueError: If the file_path does not exist.\n","\n","    \"\"\"\n","    def __init__(self, file_path, src_lang, trg_lang,src_vocab,trg_vocab,t_char_to_idx):\n","        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n","        self.translations.dropna()\n","    \n","        self.src_lang = src_lang\n","        self.t_char_to_idx = t_char_to_idx\n","        self.trg_lang = trg_lang\n","        self.src_vocab = src_vocab\n","        self.trg_vocab = trg_vocab\n","        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])+1\n","        #print(\"max src len\",self.max_src_len)\n","        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])+1\n","        #print(\"max trg len\",self.max_trg_len)\n","    def __len__(self):\n","        return len(self.translations)\n","\n","    def target_to_one_hot(self, target_word, char_to_idx):\n","        num_trg_chars = len(char_to_idx)\n","        max_target_len = self.max_trg_len\n","        # Create a tensor of zeros for the one-hot encoding\n","        one_hot = torch.zeros((max_target_len, num_trg_chars))\n","        # Encode each character in the target word as a one-hot vector\n","        for i, char in enumerate(target_word):\n","            #print(i,char)\n","            char_idx = char_to_idx[char if char in  char_to_idx else '<unk>']\n","            #print(char_idx)\n","            one_hot[i][char_idx] = 1\n","        return one_hot\n","\n","    def __getitem__(self, idx):\n","        src_word = self.translations.iloc[idx][self.src_lang]\n","        trg_word = self.translations.iloc[idx][self.trg_lang]\n","        #print(src_word)\n","        # Initialize the start-of-word token\n","        sow=0\n","        \n","        # Convert source and target words to lists of vocabulary indices\n","        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n","        trg = [self.trg_vocab.get(char, self.src_vocab['<unk>']) for char in trg_word]\n","        # Insert the start-of-word token at the beginning\n","        src.insert(0, sow)\n","        trg.insert(0, sow)\n","\n","        src_len = len(src)\n","        trg_len = len(trg)\n","\n","        # Pad the source and target sequences with the <pad> token\n","        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n","        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n","\n","        # Extend the source and target sequences with padding\n","        src.extend(src_pad)\n","        trg.extend(trg_pad)\n","\n","        # Convert source and target sequences to tensors\n","        src = torch.LongTensor(src)\n","        trg = torch.LongTensor(trg)\n","        #trg_one_hot = self.target_to_one_hot(trg_word, self.trg_vocab)\n","        #src_one_hot = self.target_to_one_hot(src_word, self.src_vocab)\n","\n","        # This will return encoded source word ,target word and their length\n","        return src, trg, src_len, trg_len\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:26.893838Z","iopub.status.busy":"2023-05-20T20:24:26.893183Z","iopub.status.idle":"2023-05-20T20:24:26.902180Z","shell.execute_reply":"2023-05-20T20:24:26.901146Z","shell.execute_reply.started":"2023-05-20T20:24:26.893804Z"},"trusted":true},"outputs":[],"source":["def load_data(bs):\n","    '''\n","    This function loads data into batches provided the batch size as an argument.\n","    '''\n","    # Define the paths for the train, validation, and test CSV files\n","    train_path  =\"/kaggle/input/aksharantar/hin_train.csv\"\n","    val_path  =\"/kaggle/input/aksharantar/hin_valid.csv\"\n","    test_path  =\"/kaggle/input/aksharantar/hin_test.csv\"\n","\n","    # Create a vocabulary object and retrieve the source and target vocabularies,\n","    # character-to-index and index-to-character mappings\n","    vocab = Vocabulary(train_path, 'src', 'trg')\n","    src_vocab,trg_vocab,t_char_to_idx,t_idx_to_char,s_char_to_idx,s_idx_to_char=vocab.get()\n","    #print(len(src_vocab))\n","    #print(len(trg_vocab))\n","    #print(\"char to idc outside\",char_to_idx)\n","\n","\n","    # Create train, validation, and test datasets using TransliterationDataset\n","    # with the appropriate source and target vocabularies and mappings\n","    train_dataset = TransliterationDataset(train_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n","    val_dataset = TransliterationDataset(val_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n","    test_dataset = TransliterationDataset(test_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n","    \n","    # Create train, validation, and test data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n","    \n","    return train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char\n","\n","\n","    #Training and check accuracy function\n"," \n","  \n","#train_loader,test_loader,val_loader,idx_to_char=load_data(32)\n","#print(idx_to_char)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:27.636248Z","iopub.status.busy":"2023-05-20T20:24:27.635844Z","iopub.status.idle":"2023-05-20T20:24:27.671472Z","shell.execute_reply":"2023-05-20T20:24:27.670424Z","shell.execute_reply.started":"2023-05-20T20:24:27.636213Z"},"trusted":true},"outputs":[],"source":["# Model\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):\n","        super(Encoder, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.embedded_size=embedded_size\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.cell_type = cell_type\n","        self.bidirectional=bidirectional\n","        self.dropout = nn.Dropout(dp)\n","\n","        self.fc_h = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.fc_c = nn.Linear(hidden_dim * 2, hidden_dim)\n","        if self.bidirectional:\n","            self.dir=2\n","        else:\n","            self.dir=1  \n","        \n","        self.embedding = nn.Embedding(input_dim,embedded_size)\n","        if cell_type == 'rnn':\n","              self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)\n","        elif cell_type == 'lstm':\n","              self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)\n","        elif cell_type == 'gru':\n","              self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers,bidirectional=bidirectional)\n","        else:\n","            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n","        \n","        \n","\n","    def forward(self, src):\n","        embedded = self.dropout(self.embedding(src))\n","        if self.bidirectional:\n","            if self.cell_type == 'lstm':\n","                output, (hidden, cell) = self.rnn(embedded)\n","                hidden = self.fc_h(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n","                cell = self.fc_c(torch.cat((cell[0:1], cell[1:2]), dim=2))\n","                return output, (hidden, cell)\n","\n","            else:\n","                output, hidden = self.rnn(embedded)\n","            #print(\"hidden en\",hidden[0:1])\n","            #print(\"hidden en\",hidden[1:2])\n","                hidden = self.fc_h(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n","                return output,hidden\n","        \n","        else:\n","            if self.cell_type == 'lstm':\n","                output, (hidden, cell) = self.rnn(embedded)\n","                return output, (hidden, cell)\n","            else:\n","                output, hidden = self.rnn(embedded)\n","                return output,hidden\n","\n","\n","        \n","        \n","class Decoder(nn.Module):\n","    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):\n","        super(Decoder, self).__init__()\n","        \n","        self.output_dim = output_dim\n","        self.embedded_size=embedded_size\n","        \n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.cell_type = cell_type\n","        #self. encoder_hidden_dim= encoder_hidden_dim\n","        self.bidirectional=bidirectional\n","        self.dropout = nn.Dropout(dp)\n","        if self.bidirectional:\n","            self.dir=2\n","        else:\n","            self.dir=1  \n","\n","        self.embedding = nn.Embedding(output_dim,embedded_size)\n","        \n","        if cell_type == 'rnn':\n","            self.rnn = nn.RNN((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)\n","        elif cell_type == 'lstm':\n","            self.rnn = nn.LSTM((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)\n","        elif cell_type == 'gru':\n","            self.rnn = nn.GRU((hidden_dim*self.dir)+embedded_size, hidden_dim, num_layers)\n","        else:\n","            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n","\n","        self.energy = nn.Linear((hidden_dim *(self.dir+1) ), 1)\n","        self.dropout = nn.Dropout(dp)\n","        self.softmax = nn.Softmax(dim=0)\n","        self.relu = nn.ReLU()\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","        \n","    def forward(self, input,encoder_states,hidden):\n","        input = input.unsqueeze(0)\n","        #print(\"decoder input shape inside\",input.shape)\n","        embedded = self.dropout(self.embedding(input))\n","                \n","        sequence_length = encoder_states.shape[0]\n","        #hl = encoder_output.shape[2]\n","        \n","        #bs = hidden.shape[1]\n","        #print(\"sequence length\",sequence_length)\n","        h_reshaped = hidden[0].repeat(sequence_length,1,1)\n","        #h_reshaped = h_reshaped.unsqueeze(0)\n","               \n","        #print(\"hidden\",hidden[0].shape)\n","        #print(\"encoder output\",encoder_states.shape)\n","\n","        #print(\"h_reshaped\",h_reshaped.shape)\n","        # h_reshaped: (seq_length, N, hidden_size*2)\n","       # encoder_states =encoder_states.permute(1,0,2)\n","        \n","\n","\n","        energy = self.relu(self.energy(torch.cat((h_reshaped,encoder_states), dim=2)))\n","        # energy: (seq_length, N, 1)\n","        #print(\"energy\",energy.shape)\n","        \n","        attention = self.softmax(energy)\n","\n","        attention =attention.permute(1,2,0)\n","        encoder_states =encoder_states.permute(1,0,2)\n","        \n","        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n","\n","        rnn_input = torch.cat((context_vector, embedded), dim=2)\n","        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n","\n","        # Concatenate the last hidden state of the encoder from both directions\n","        #print(\"decoder embedded shape inside\",embedded.shape)\n","        #print(\"decoder hidden shape inside\",hidden.shape)\n","\n","        output, hidden = self.rnn(rnn_input, hidden)\n","        #print(\"==============================================\")\n","        \n","        #output = output.squeeze(0)\n","        output = self.fc_out(output)\n","        output = output.squeeze(0)\n","        #output = F.log_softmax(output, dim=1)\n","\n","        return output, hidden\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,cell_type,bidirectional):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.cell_type=cell_type\n","        self.bidirectional=bidirectional\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = trg.shape[1]\n","        #print(batch_size)\n","        max_len = trg.shape[0]\n","        #print(max_len)\n","        trg_vocab_size = self.decoder.output_dim\n","        #print(trg_vocab_size)\n","        #print(\"====================================================\")\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n","        \n","        encoder_states, encoder_hidden = self.encoder(src)\n","        #print(\"encoder hidden shape\",encoder_hidden.shape)\n","        #Grab 1st input\n","        #print(\"hidden concat shape\",hidden_concat.shape)\n","\n","        #print(\"=====================================================\")\n","        decoder_input = trg[0]\n","        #print(\"decoder input shape\",decoder_input.shape)\n","        \n","        for t in range(1,max_len ):\n","            decoder_output, decoder_hidden = self.decoder(decoder_input,encoder_states,encoder_hidden)\n","\n","            outputs[t] = decoder_output\n","            max_pr=decoder_output.argmax(1)\n","            \n","            decoder_input=trg[t] if random.random()<teacher_forcing_ratio else max_pr\n","\n","        return outputs\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:28.373138Z","iopub.status.busy":"2023-05-20T20:24:28.372286Z","iopub.status.idle":"2023-05-20T20:24:28.379779Z","shell.execute_reply":"2023-05-20T20:24:28.378641Z","shell.execute_reply.started":"2023-05-20T20:24:28.373102Z"},"trusted":true},"outputs":[],"source":["def indices_to_string(trg, t_idx_to_char):\n","    \"\"\"Converts a batch of indices to strings using the given index-to-char mapping\n","    Args:\n","    trg(Tensor):encoder words of size batch_size x sequence length\n","    t_idx_to_char(Dict.): index to char mapping\n","    \n","    \"\"\"\n","    strings = []\n","    bs=trg.shape[0]\n","    sq=trg.shape[1]\n","    for i in range(bs):\n","        chars = []\n","      #print(i)\n","      # Convert the sequence of indices to a sequence of characters using the index-to-char mapping\n","        for j in range(sq):\n","            if trg[i,j].item() in t_idx_to_char:\n","                char = t_idx_to_char[trg[i,j].item()]\n","                chars.append(char)\n","      # Join the characters into a string\n","        string = ''.join(chars)\n","      #print(string)\n","        # Append the string to the list of strings\n","        strings.append(string)\n","    return strings\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:29.057646Z","iopub.status.busy":"2023-05-20T20:24:29.056473Z","iopub.status.idle":"2023-05-20T20:24:29.070177Z","shell.execute_reply":"2023-05-20T20:24:29.069089Z","shell.execute_reply.started":"2023-05-20T20:24:29.057603Z"},"trusted":true},"outputs":[],"source":["\n","def calculate_word_level_accuracy(model,t_idx_to_char,data_loader, criterion):\n","    '''\n","    This function will calculate word level accuracy after each epoch.\n","    Args:\n","        model: The trained model\n","        t_idx_to_char: Mapping from target indices to characters\n","        data_loader: Data loader for the validation/test dataset\n","        criterion: Loss criterion used for training the model\n","\n","\n","    '''\n","    model.eval()\n","    num_correct = 0\n","    num_total = 0\n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n","            # Convert target indices to string for comparison\n","            string_trg=indices_to_string(trg,t_idx_to_char)\n","            # Move tensors to the device\n","            src = src.permute(1, 0)\n","            trg = trg.permute(1, 0)\n","            src = src.to(device)\n","            trg = trg.to(device)\n","            # Perform forward pass through the model\n","            output = model(src, trg, 0)\n","            # turn off teacher forcing\n","            output = output[1:].reshape(-1, output.shape[2])\n","            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n","\n","            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n","            #print(\"trg after reshape\",trg.shape)\n","            \n","            # Calculate the loss\n","            output = output.to(device)\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","            \n","            batch_size = trg_len.shape[0]\n","            #print(\"bs\", batch_size)\n","            seq_length = int(trg.numel() / batch_size)\n","            \n","\n","            # Convert the output to predicted characters\n","            predicted_indices = torch.argmax(output, dim=1)\n","            predicted_indices = predicted_indices.reshape(seq_length,-1)\n","            predicted_indices = predicted_indices.permute(1, 0)\n","            # Convert predicted indices to strings\n","            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n","            #print(string_pred)\n","            #print(string_trg)\n","            \n","            for i in range(batch_size):\n","                num_total+=1\n","                # Compare the predicted string with the target string\n","                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n","                    num_correct+=1\n","\n","    print(\"Total\",num_total)\n","    print(\"Correct\",num_correct*2)\n","    # Calculate word-level accuracy and average loss\n","    return ((num_correct*2) /num_total) * 100, (epoch_loss/(len(data_loader)))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:29.962744Z","iopub.status.busy":"2023-05-20T20:24:29.962357Z","iopub.status.idle":"2023-05-20T20:24:29.976610Z","shell.execute_reply":"2023-05-20T20:24:29.975669Z","shell.execute_reply.started":"2023-05-20T20:24:29.962715Z"},"trusted":true},"outputs":[],"source":["\n","def calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,data_loader, criterion):\n","    '''\n","    This function is just extension of above function and\n","    will calculate word level accuracy as well as store the correct and \n","    incorrect words into list .\n","    We'll call this function after training only once for test data.\n","    Args:\n","        model: The trained model\n","        t_idx_to_char: Mapping from target indices to characters\n","        s_idx_to_char: Mapping from source indices to characters\n","        data_loader: Data loader for the validation/test dataset\n","        criterion: Loss criterion used for training the model\n","\n","\n","    '''\n","    model.eval()\n","    num_correct = 0\n","    num_total = 0\n","    epoch_loss = 0\n","    c_trg=[]\n","    c_src=[]\n","    c_pred=[]\n","    \n","    i_trg=[]\n","    i_src=[]\n","    i_pred=[]\n","    \n","\n","    with torch.no_grad():\n","        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n","            # Convert target indices to string for comparison\n","            string_trg=indices_to_string(trg,t_idx_to_char)\n","            string_src=indices_to_string(src,s_idx_to_char)\n","            \n","            # Move tensors to the device\n","            src = src.permute(1, 0)\n","            trg = trg.permute(1, 0)\n","            src = src.to(device)\n","            trg = trg.to(device)\n","            # Perform forward pass through the model\n","            output = model(src, trg, 0)\n","            # turn off teacher forcing\n","            output = output[1:].reshape(-1, output.shape[2])\n","            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n","\n","            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n","            #print(\"trg after reshape\",trg.shape)\n","            \n","            # Calculate the loss\n","            output = output.to(device)\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","            \n","            batch_size = trg_len.shape[0]\n","            #print(\"bs\", batch_size)\n","            seq_length = int(trg.numel() / batch_size)\n","            \n","\n","            # Convert the output to predicted characters\n","            predicted_indices = torch.argmax(output, dim=1)\n","            predicted_indices = predicted_indices.reshape(seq_length,-1)\n","            predicted_indices = predicted_indices.permute(1, 0)\n","            # Convert predicted indices to strings\n","            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n","            #print(string_pred)\n","            #print(string_trg)\n","            \n","            for i in range(batch_size):\n","                num_total+=1\n","                # Compare the predicted string with the target string\n","                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n","                    c_trg.append(string_trg[i])\n","                    c_src.append(string_src[i])\n","                    c_pred.append(string_pred[i][:len(string_trg[i])])\n","                    num_correct+=1\n","                else :\n","                    i_trg.append(string_trg[i])\n","                    i_src.append(string_src[i])\n","                    i_pred.append(string_pred[i][:len(string_trg[i])])\n","                  \n","\n","\n","    print(\"Total\",num_total)\n","    print(\"Correct\",num_correct)\n","    # Calculate word-level accuracy and average loss\n","    return (num_correct /num_total) * 100, (epoch_loss/(len(data_loader))),c_trg,c_src,c_pred,i_trg,i_src,i_pred\n","    \n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:24:31.812443Z","iopub.status.busy":"2023-05-20T20:24:31.811728Z","iopub.status.idle":"2023-05-20T20:53:45.121169Z","shell.execute_reply":"2023-05-20T20:53:45.120182Z","shell.execute_reply.started":"2023-05-20T20:24:31.812406Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Batch: 0, Training...\n","Epoch: 0, Batch: 1000, Training...\n","Total 4096\n","Correct 334\n","Epoch: 0, Loss: 0.9334558361768722, Val Acc: 8.154296875, Val loss: 1.513887230772525\n","Epoch: 1, Batch: 0, Training...\n","Epoch: 1, Batch: 1000, Training...\n","Total 4096\n","Correct 605\n","Epoch: 1, Loss: 0.6403921652585268, Val Acc: 14.7705078125, Val loss: 1.4289126237854362\n","Epoch: 2, Batch: 0, Training...\n","Epoch: 2, Batch: 1000, Training...\n","Total 4096\n","Correct 763\n","Epoch: 2, Loss: 0.5456497328355908, Val Acc: 18.6279296875, Val loss: 1.4448016765527427\n","Epoch: 3, Batch: 0, Training...\n","Epoch: 3, Batch: 1000, Training...\n","Total 4096\n","Correct 826\n","Epoch: 3, Loss: 0.4992097784765065, Val Acc: 20.166015625, Val loss: 1.3552684905007482\n","Epoch: 4, Batch: 0, Training...\n","Epoch: 4, Batch: 1000, Training...\n","Total 4096\n","Correct 861\n","Epoch: 4, Loss: 0.4592827731370926, Val Acc: 21.0205078125, Val loss: 1.3345112325623631\n","Epoch: 5, Batch: 0, Training...\n","Epoch: 5, Batch: 1000, Training...\n","Total 4096\n","Correct 880\n","Epoch: 5, Loss: 0.4377509455103427, Val Acc: 21.484375, Val loss: 1.3260849197395146\n","Epoch: 6, Batch: 0, Training...\n","Epoch: 6, Batch: 1000, Training...\n","Total 4096\n","Correct 935\n","Epoch: 6, Loss: 0.4181798862759024, Val Acc: 22.8271484375, Val loss: 1.3803635640069842\n","Epoch: 7, Batch: 0, Training...\n","Epoch: 7, Batch: 1000, Training...\n","Total 4096\n","Correct 933\n","Epoch: 7, Loss: 0.40317520214244723, Val Acc: 22.7783203125, Val loss: 1.3472997760400176\n","Epoch: 8, Batch: 0, Training...\n","Epoch: 8, Batch: 1000, Training...\n","Total 4096\n","Correct 934\n","Epoch: 8, Loss: 0.3861089991033077, Val Acc: 22.802734375, Val loss: 1.3996625593863428\n","Epoch: 9, Batch: 0, Training...\n","Epoch: 9, Batch: 1000, Training...\n","Total 4096\n","Correct 921\n","Epoch: 9, Loss: 0.3705684397555888, Val Acc: 22.4853515625, Val loss: 1.3362483219243586\n","Epoch: 10, Batch: 0, Training...\n","Epoch: 10, Batch: 1000, Training...\n","Total 4096\n","Correct 936\n","Epoch: 10, Loss: 0.36085540568456054, Val Acc: 22.8515625, Val loss: 1.3939013700000942\n","Epoch: 11, Batch: 0, Training...\n","Epoch: 11, Batch: 1000, Training...\n","Total 4096\n","Correct 974\n","Epoch: 11, Loss: 0.3501567451097071, Val Acc: 23.779296875, Val loss: 1.3641186244785786\n","Epoch: 12, Batch: 0, Training...\n","Epoch: 12, Batch: 1000, Training...\n","Total 4096\n","Correct 947\n","Epoch: 12, Loss: 0.33524262906983493, Val Acc: 23.1201171875, Val loss: 1.3928861212916672\n","Epoch: 13, Batch: 0, Training...\n","Epoch: 13, Batch: 1000, Training...\n","Total 4096\n","Correct 964\n","Epoch: 13, Loss: 0.327688192659989, Val Acc: 23.53515625, Val loss: 1.421377758961171\n","Epoch: 14, Batch: 0, Training...\n","Epoch: 14, Batch: 1000, Training...\n","Total 4096\n","Correct 962\n","Epoch: 14, Loss: 0.3252102614939213, Val Acc: 23.486328125, Val loss: 1.4115946693345904\n","Epoch: 15, Batch: 0, Training...\n","Epoch: 15, Batch: 1000, Training...\n","Total 4096\n","Correct 978\n","Epoch: 15, Loss: 0.31586157727986575, Val Acc: 23.876953125, Val loss: 1.41078622918576\n","Epoch: 16, Batch: 0, Training...\n","Epoch: 16, Batch: 1000, Training...\n","Total 4096\n","Correct 971\n","Epoch: 16, Loss: 0.3094708347693086, Val Acc: 23.7060546875, Val loss: 1.4017384205944836\n","Epoch: 17, Batch: 0, Training...\n","Epoch: 17, Batch: 1000, Training...\n","Total 4096\n","Correct 976\n","Epoch: 17, Loss: 0.30348696357570587, Val Acc: 23.828125, Val loss: 1.4452460454776883\n","Epoch: 18, Batch: 0, Training...\n","Epoch: 18, Batch: 1000, Training...\n","Total 4096\n","Correct 979\n","Epoch: 18, Loss: 0.2979402997158468, Val Acc: 23.9013671875, Val loss: 1.4455265854485333\n","Epoch: 19, Batch: 0, Training...\n","Epoch: 19, Batch: 1000, Training...\n","Total 4096\n","Correct 995\n","Epoch: 19, Loss: 0.28996584612876175, Val Acc: 24.2919921875, Val loss: 1.4055600338615477\n","Epoch: 20, Batch: 0, Training...\n","Epoch: 20, Batch: 1000, Training...\n","Total 4096\n","Correct 996\n","Epoch: 20, Loss: 0.28556812135502696, Val Acc: 24.31640625, Val loss: 1.427688925061375\n","Epoch: 21, Batch: 0, Training...\n","Epoch: 21, Batch: 1000, Training...\n","Total 4096\n","Correct 969\n","Epoch: 21, Loss: 0.2845371905947104, Val Acc: 23.6572265625, Val loss: 1.4653364247642457\n","Epoch: 22, Batch: 0, Training...\n","Epoch: 22, Batch: 1000, Training...\n","Total 4096\n","Correct 972\n","Epoch: 22, Loss: 0.2799325099727139, Val Acc: 23.73046875, Val loss: 1.4216785901226103\n","Epoch: 23, Batch: 0, Training...\n","Epoch: 23, Batch: 1000, Training...\n","Total 4096\n","Correct 977\n","Epoch: 23, Loss: 0.27658882934134454, Val Acc: 23.8525390625, Val loss: 1.4466612613759935\n","Epoch: 24, Batch: 0, Training...\n","Epoch: 24, Batch: 1000, Training...\n","Total 4096\n","Correct 991\n","Epoch: 24, Loss: 0.2711858875444159, Val Acc: 24.1943359375, Val loss: 1.4742125747725368\n","Total 4096\n","Correct 847\n"]}],"source":["\n","# Define hyperparameters\n","INPUT_DIM = 29\n","OUTPUT_DIM = 67\n","embedding_size=256\n","HIDDEN_DIM = 512\n","NUM_LAYERS = 1\n","CELL_TYPE = 'lstm'\n","BATCH_SIZE = 32\n","LEARNING_RATE = 0.001\n","TEACHER_FORCING_RATIO = 0.7\n","EPOCHS = 25\n","\n","dropout=0.2\n","bidirectional=False\n","opt='adam'\n","\n","\n","# Load data and create data loaders\n","train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char=load_data(BATCH_SIZE)\n","#print(len(test_loader))\n","#print(len(train_loader))\n","#print(len(val_loader))\n","# Instantiate the Encoder and Decoder models\n","encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n","decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n","\n","# Instantiate the Seq2Seq model with the Encoder and Decoder models\n","model = Seq2Seq(encoder, decoder,CELL_TYPE,bidirectional).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.NAdam(model.parameters(), lr=LEARNING_RATE)\n","#optimizer=optimizer(model,opt,LEARNING_RATE)\n","\n","\n","\n","\n","# Train the model\n","for epoch in range(EPOCHS):\n","    epoch_loss = 0\n","    model.train()\n","\n","    for batch_idx, (src, trg, src_len, trg_len) in enumerate(train_loader):\n","        #print(batch_idx)\n","        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n","        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n","\n","        src = src.to(device)\n","        trg = trg.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg, TEACHER_FORCING_RATIO)\n","        \n","        # Ignore the first element of the output, which is initialized as all zeros\n","        # since we use it to store the output for the start-of-sequence token\n","        #print(output.shape[2])\n","        \n","        output = output[1:].reshape(-1, output.shape[2])\n","        #print(output.shape)\n","        #print(trg.shape)\n","        trg = trg[1:].reshape(-1)\n","        \n","        loss = criterion(output, trg)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += (loss.item())\n","        \n","        if batch_idx % 1000 == 0:\n","            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Training...\")\n","\n","    # Calculate word-level accuracy after every epoch\n","    val_acc,val_loss = calculate_word_level_accuracy(model,t_idx_to_char,val_loader,criterion)\n","    \n","    print(f\"Epoch: {epoch}, Loss: {epoch_loss / (len(train_loader))}, Val Acc: {val_acc}, Val loss: {val_loss}\")\n","    #wandb.log({'epoch': epoch, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n","    \n","\n","val_acc,val_loss,c_trg,c_src,c_pred,i_trg,i_src,i_pred = calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,test_loader,criterion)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:53:49.980403Z","iopub.status.busy":"2023-05-20T20:53:49.980024Z","iopub.status.idle":"2023-05-20T20:53:50.032016Z","shell.execute_reply":"2023-05-20T20:53:50.030980Z","shell.execute_reply.started":"2023-05-20T20:53:49.980353Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best model saved to best_model_AttnSeq2Seq.pth\n"]}],"source":["best_model_path = 'best_model_AttnSeq2Seq.pth'\n","torch.save(model.state_dict(), best_model_path)\n","print(f\"Best model saved to {best_model_path}\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:54:06.813591Z","iopub.status.busy":"2023-05-20T20:54:06.813216Z","iopub.status.idle":"2023-05-20T20:54:06.830664Z","shell.execute_reply":"2023-05-20T20:54:06.829489Z","shell.execute_reply.started":"2023-05-20T20:54:06.813556Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['थरमैक्स', 'कोल्हापुर', 'फूहड़ता', 'मुशाहिद', 'लिखकर', 'तलवे', 'सीमाई', 'रिवास', 'अनुसारका', 'बुएना', 'कैटलिन', 'वंडर', 'श्विंग', 'नीलाभ', 'मास', 'वेस्टिंग', 'कुर्वेती', 'मुकेश्वरी', 'श्रीमति', 'कबहा', 'सिंगार', 'पलटने', 'नगाड़ा', 'क्लेन', 'कोविना', 'फूस', 'नेको', 'कौशल', 'फरसे', 'मुर्गीपालन', 'संयुक्त', 'सभाएं', 'मैक्सवेल', 'दीपन', 'बैक्सटर', 'घासी', 'इकट्ठा', 'भूपसिंह', 'हर्ले', 'भ्रांति', 'बागो', 'क्रेन', 'फर्थ', 'सुहागिन', 'चढ़ने', 'रिडक्शन', 'एक्सेल', 'भाल', 'संख्याएं', 'हरपाल', 'एंजेलो', 'पोकर', 'अपवर्तित', 'शैली', 'देवी', 'आमदनी', 'ब्रोक', 'कुश', 'चढ़ें', 'वांगचुक', 'सचन', 'घोंटना', 'अकोला', 'फोड़ा', 'नूबिया', 'मंजिलें', 'जुराब', 'फुलाए', 'ब्लैक', 'जोहरी', 'वेल्डर', 'बहावलपुर', 'चौसिंगा', 'पफिन', 'टीकों', 'अथक', 'माप', 'सहूलियत', 'पेंडुलम', 'चिपकने', 'गोंड', 'युद्धक', 'वेगास', 'चित्रांशी', 'इबीजा', 'वर्णित', 'बरेली', 'जिंग', 'वेस्टिंगहाउस', 'त्रिया', 'हैतो', 'खेलमंत्री', 'भविष्य', 'प्रदेश', 'संवेदीकरण', 'लगुना', 'रेस', 'मंगला', 'यूक्लिड', 'किशुन', 'अमरोही', 'जेनर', 'शावकों', 'रूपा', 'वेरोनिका', 'वेब', 'पास्को', 'संशयों', 'उपस्थियों', 'लिवाल', 'शाकिर', 'रिज', 'रिश्वतखोर', 'दफ्तरवाले', 'लोग', 'जोधिका', 'मेघवाल', 'नंदोई', 'दुर्घटना', 'चमकना', 'देहरा', 'कुर्सियों', 'पहनो', 'बंगश', 'पार्नेल', 'मध्य', 'मशीनवत', 'ग्रे', 'भ्रांति', 'तेजी', 'मूलों', 'दबंगता', 'धुरिया', 'टोलेडो', 'धनिये', 'फाड़ी', 'माथलू', 'नौशाद', 'पिक', 'कताल', 'बेसिल', 'व्यापक', 'दमाए', 'वाको', 'हेस्टर', 'चयनितों', 'अमरपाल', 'फटने', 'टिंका', 'अर्थों', 'नीलाभ', 'बिलाई', 'भड़काया', 'अल्बा', 'टेक्सस', 'स्पेशल', 'लिगर', 'परवा', 'प्रदेशिका', 'घोस्ट', 'हेल्पर', 'बख्शने', 'रिसाली', 'ओझा', 'आवाज़ें', 'मूल', 'लैंड', 'सूची', 'नून', 'आरपी', 'बेचकर', 'स्टोन', 'बापा', 'रूपक', 'नदफ', 'अगरतला', 'आपदाएं', 'आगाशे', 'भीरा', 'सिल्वा', 'निचला', 'बुरहानपुर', 'सहूलियत', 'लब्ध', 'पूल', 'अजित', 'झ', 'भुज', 'अनोंदिता', 'जड़ें', 'मुफलिसी', 'रेजा', 'मिटाओ', 'मुनाफ', 'महावर', 'भूस्वामी', 'विज्ञान', 'क्यूट', 'बेवकूफाना', 'विलास', 'सुपाच्य', 'सिरती', 'विष्णु', 'चौधरी', 'पटियाला', 'दमाए', 'मुकेश्वरी', 'चरस', 'भदौरिया', 'विज्ञानों', 'गंझू', 'आस्तिक', 'रिग्स', 'चुने', 'होगिस', 'विंटर', 'घुमाई', 'बख्शते', 'संवेदीकरण', 'फरहीन', 'रेडिंग', 'शक़', 'किसानों', 'स्टीन', 'भदौरिया', 'आदान', 'मालेकर', 'विक्रम', 'आर्टेमिस', 'पुरवासियों', 'प्रधान', 'हिजबुल', 'विद्याओं', 'छविंद्र', 'छुपकर', 'हेमवती', 'झीलों', 'वसूलता', 'मस्टेक', 'दिमाक', 'जोधपुर', 'मकरानी', 'रीता', 'दबंगों', 'तरस', 'क्लेयर', 'विरोध', 'बोका', 'टंकार', 'खटकती', 'जीएम', 'करवायेगी', 'भूतल', 'विज्ञानम', 'अटवाल', 'फार्गो', 'अलीम', 'एचआर', 'गौर', 'सेंधा', 'बगड़', 'पटकथाएं', 'भोगते', 'होल्डिंग', 'बढ़ाती', 'थेन', 'लहसुन', 'बहुंत', 'ब्रावो', 'अमानत', 'झुठला', 'बुश', 'अपर', 'सिंघाड़े', 'मैनर', 'हिमाली', 'बीताने', 'बेसिन', 'पराधीन', 'कार्सन', 'डेल्टा', 'महाजन', 'रूकेंगे', 'मारिया', 'रागों', 'महलों', 'रज़ा', 'सरका', 'अहितकारी', 'बेलिंडा', 'अलयमनी', 'बरसाई', 'तो', 'बेचकर', 'कौशलता', 'गानो', 'एमटीए', 'समृद्धियों', 'रॉबिंस', 'उस्तरा', 'कुमारी', 'वेरोनिका', 'पॉटर', 'छहों', 'बावरा', 'हमलावर', 'खलती', 'बचती', 'मोहिनी', 'आर्यिका', 'छींक', 'पकौड़े', 'लूसी', 'फतेहगढ़', 'उभारा', 'बेक', 'कपूर', 'सेवाला', 'मोगली', 'मिचेल', 'धोराजी', 'ची', 'शासक', 'रहमत', 'व्हीलिंग', 'विप्रो', 'पुलम', 'पोर्टिलो', 'पेडू', 'अरदास', 'अमारिलो', 'तोलासन', 'गिरकर', 'वैज्ञानिक', 'फूलती', 'गंध', 'चक्षु', 'औरतो', 'अम्बिकाओं', 'इशाक', 'ईए', 'अजस्र', 'संप्रभुता', 'टोकने', 'फलीभूत', 'पहनाकर', 'किसानी', 'पॉलिमर', 'रक्ताभ', 'जैन', 'रसायनिक', 'अर्जेंट', 'जिनी', 'असुरक्षित', 'अलंकार', 'राधे', 'निगर', 'मोंटे', 'सुनीति', 'अम्बिकाओं', 'प्रहलाद', 'बीस्ट', 'संजीवन', 'चढ़ेगा', 'वर्थ', 'आस्तीन', 'आकाश', 'बोना', 'कहलाया', 'धारक', 'बंटता', 'श्रीकर', 'मिटाओ', 'भूपसिंह', 'फ्लिन', 'शरभ', 'अरेलानो', 'कुमार', 'सिंगार', 'खिसकते', 'एनिड', 'मुनाफा', 'ओर्टेगा', 'रोवे', 'अटके', 'यत्न', 'सिटी', 'रेडिको', 'दुकानों', 'अवरु', 'हुगली', 'एरिका', 'खिंचकर', 'कून', 'लगातीं', 'पहुंचोगी', 'अमित', 'शाओ', 'वीक्स', 'बस्तियो', 'हरिजन', 'आर्यों', 'दुखियों', 'यारी', 'विधमान', 'अवस्थी', 'मेजबानी', 'डेहरी', 'स्कूटरों', 'शीलभंग', 'कलमें', 'मंडी', 'हत्यारोपित', 'बर्मन', 'जीवनपर्यंत', 'रोजलिन', 'गोपालक', 'मैसोर', 'उसकी', 'कैपलिन', 'स्वामी', 'टाइलर', 'पहनो', 'नवा', 'बनाम', 'राजों', 'नैमुल', 'एंटोनी', 'निपटारा', 'सुझाती', 'टाइड', 'स्टैन', 'जुलाहों', 'शिप्रा', 'पाखंडी', 'सोहावल', 'रोजना', 'सेव', 'छतेनी', 'दस्तानें', 'धोना', 'आमदी', 'अवरोधक', 'रॉस', 'अच्युत', 'हरावल', 'दाव', 'देहधारी', 'नॉर्मल', 'ग्रेग', 'ली', 'कश्मीर', 'मारके', 'धुरिया', 'ऑथोरिटी', 'आगाशे', 'चौका', 'सप्रमाण', 'टिक', 'अज्ञात', 'जोधिका', 'बदलवा', 'भोपाल', 'फलीभूत', 'उठाकर', 'सोमी', 'कांचीपुरम', 'वेल्डिंग', 'वेरोनिका', 'अतुल', 'प्लानो', 'खोमचे', 'मुकेश्वरी', 'तस्वारों', 'वेश', 'चौक', 'चेस्ट', 'लूवे', 'धावकों', 'लिम', 'यौनिकता', 'हाथे', 'वैज्ञानिक', 'संप्रति', 'सविता', 'जलगति', 'चौकी', 'आपका', 'धम्म', 'ऐठन', 'वर्तमान', 'बनवा', 'प्रसाथ', 'देशकाल', 'थानवी', 'सेरानो', 'धारक', 'गरुड़', 'रिया', 'ची', 'कैल्यूमेट', 'अंकोर', 'रैपिंग', 'दाव', 'मसूद', 'उल्हासपुर', 'पहनते', 'धावकों', 'जलवायु', 'भिजवाई', 'परोसने', 'विचारकों', 'आइंथू', 'जायके', 'धुनाई', 'श्विंग', 'कपड़ा', 'काजू', 'संचार', 'हॉलीवुड', 'छेड़ना', 'प्राना', 'डेनवर', 'लकीर', 'जागृति', 'बुलेटिन', 'रहेगी', 'सुधारती', 'वसामुक्त', 'जेवाब', 'सोटो', 'डोसे', 'आर्य', 'घोट', 'टॉनिक', 'गुजारने', 'वाटर', 'वरुण', 'शराबों', 'अवस्थाएं', 'भूस्वामी', 'जुए', 'घोंटना', 'रोपने', 'फाड़े', 'मोलिना', 'टोंक', 'देवापुर', 'सिम', 'लग्नेश', 'मूलचंद', 'सिरती', 'कीति', 'मंगेश', 'सेखर', 'फुरामे', 'कठुआ', 'पलारी', 'सेल्फी', 'वादक', 'जंक्शन', 'जिया', 'एडेन', 'फीड्स', 'बावजी', 'विज्ञानों', 'सिहुंता', 'अधिवास', 'वजूद', 'रिया', 'स्लाइडर', 'स्पेल', 'आपका', 'जागृति', 'गुजरात', 'नाभा', 'लेगा', 'दस्तार', 'मेंशराब', 'रामकोट', 'सोलापुर', 'मेजबानी', 'फ्लोरेंस', 'झाड़ने', 'बोकारो', 'परोसने', 'ढैया', 'तस्वारों', 'केली', 'मेजबान', 'फुलाए', 'प्रूट', 'प्रशासनिक', 'चित्रकथा', 'अवशिष्ट', 'एवरी', 'बैरकपुर', 'स्किपर', 'चिको', 'लैब्स', 'मनिया', 'कंचन', 'पिएगा', 'अभिमत', 'चौड़ा', 'मिल्क', 'सुतली', 'वेल्थ', 'पैम', 'बलजीत', 'भटनागर', 'जातकों', 'पकड़ाए', 'उपलब्धियों', 'आर्य', 'दिखाइये', 'अनिष्टता', 'झुठलाने', 'लड़की', 'रूपी', 'यूजी', 'राज', 'विंध्या', 'फलने', 'घटाएगी', 'एरिका', 'भोला', 'रुमानी', 'साहू', 'हमला', 'कुशवाहा', 'फार्मिक', 'अर्थ', 'कूच', 'सुलखान', 'संत्री', 'इच्छी', 'हाबरा', 'पालियों', 'सेथ', 'जगताप', 'हर्षित', 'तासीर', 'डॉर्क', 'जीनों', 'उखड', 'मंजिल', 'निकेतन', 'ट्विन', 'दिव्य', 'निगमायुक्त', 'गबली', 'रिश्वतखोर', 'पिंटा', 'राखे', 'जेठा', 'छुड़वाया', 'रतनपुर', 'विशेषणों', 'पुलिंदे', 'महोनी', 'खरे', 'सवाल', 'आलोचक', 'फाँस', 'अमझेरा', 'रिसने', 'जोस', 'बीएसएसएस', 'नजीब', 'अवस्थाएं', 'बुल', 'रिपन', 'मिथ्या', 'बिगेस्ट', 'जोड़ता', 'ओरछा', 'प्रिंस', 'मेज़ा', 'उबाल', 'चार्ट', 'सरगी', 'भाते', 'विप्ख', 'बयाने', 'बोवी', 'कमलजीत', 'एसेटल', 'चौहान', 'फलने', 'एपेक्स', 'मास', 'अंदाजी', 'फोंड', 'पुतला', 'ट्रुथ', 'मोहिम', 'मोल्ड', 'हूपर', 'सोया', 'तरीको', 'पहला', 'ले', 'सतुआ', 'अस्मिताओं', 'वांट', 'काजी', 'लिंच', 'स्टेट', 'सिरपुर', 'अरुणाचल', 'सियाह', 'गुजारने', 'पाली', 'चंदन', 'मेगा', 'नवकेतन', 'तपन', 'हैम्बलिन', 'आराम', 'वोखा', 'परवीन', 'अवरोधक', 'गणेशा', 'भूमी', 'रंगते', 'विद्याओं', 'केशरी', 'टकरा', 'बिजी', 'क्रोम', 'चमच', 'पटकी', 'कंगन', 'सिपाही', 'उसक', 'नागपुर', 'फ्लेश', 'सुभाय', 'फंड', 'मिश्रा', 'सरगुजा', 'मुंडवा', 'तोलासन', 'कनाल', 'फोर्टिस', 'जैनों', 'खिताबों', 'कुमावत', 'धाक', 'संप्रभुता', 'जौहरी', 'खिताबों', 'घिसे', 'बाधित', 'कोलिंग', 'समाधियों', 'दुर्घटना', 'सिंगम', 'उठनी', 'हंडिया', 'इकाई', 'पिट्स', 'रोपी', 'चौकियां', 'वकरंगी', 'भूगोल', 'सिवान', 'महानिदेशक', 'कन्याएं', 'ओडोम', 'क्यूएसएस', 'सपने', 'चुनीं', 'उसकी', 'फ्यूजी', 'भूसी', 'हलिया', 'पोप', 'वसूलता', 'उदर', 'सुदीप', 'भटकाना', 'लिमा', 'साह', 'विला', 'लिंडा', 'खरिया', 'चेलानी', 'कुर्मी', 'घुमाई', 'खेमिक', 'तुगलकी', 'भका', 'ओब्रायन', 'कृत्या', 'नजरियों', 'मयूर', 'फेका', 'टिंका', 'जातियों', 'वेड', 'कमज़ोर', 'सुमन', 'पड़नी', 'दुखों', 'नेस्को', 'फूलती', 'टूल्स', 'मित्रा', 'देवनार', 'कर', 'दफ्तरवाले', 'ज्वार', 'धातुएं', 'सियाह', 'जिता', 'मुकरते', 'खंगाली', 'बोदरा', 'रोजलिन', 'ट्रक', 'नेशंस', 'सेवकों', 'अधिवास', 'तरीको', 'काव', 'चौरसिया', 'क्ले', 'देवापुर', 'निया', 'आगमन', 'आश्रितों', 'लोरदा', 'मोशे', 'लोहाती', 'सिवान', 'थकी', 'लेटकर', 'चरित', 'असाही', 'पहन', 'फारेंसिस', 'हरपाल', 'खैराती', 'देशके', 'सेहो']\n","['thermax', 'kolhapur', 'foohadta', 'mushahid', 'likhkar', 'talve', 'seemaai', 'rivas', 'anusaarakaa', 'buena', 'catlin', 'wonder', 'shving', 'nilaabh', 'maas', 'westing', 'kurvetee', 'mukeshvari', 'shrimati', 'kabahaa', 'singar', 'palatne', 'nagaada', 'klein', 'covina', 'phoos', 'neko', 'kaushal', 'pharase', 'murgipalan', 'sanyukt', 'sabhaen', 'maxwell', 'deepan', 'baxter', 'ghasi', 'ikattha', 'bhupsingh', 'hurley', 'bhranti', 'bago', 'crane', 'ferth', 'suhagin', 'chadhne', 'reduction', 'excel', 'bhal', 'sankhyaen', 'harpal', 'angelo', 'pokar', 'apvartit', 'shaili', 'devi', 'aamdani', 'broke', 'kush', 'chadhen', 'vangchuk', 'sachan', 'ghontanaa', 'akola', 'foda', 'noobiyaa', 'manjilen', 'juraab', 'fulae', 'black', 'johari', 'welder', 'bahawalpur', 'chausinga', 'pafin', 'teekon', 'athak', 'maap', 'sahuuliyata', 'pendulum', 'chipakane', 'gond', 'yuddhak', 'vegas', 'chitranshi', 'ibija', 'varnit', 'bareli', 'jing', 'westinghouse', 'triya', 'haito', 'khelmantree', 'bhavishya', 'pradesh', 'samvedikaran', 'laguna', 'race', 'mangla', 'euclid', 'kishun', 'amrohi', 'jenner', 'shaavakon', 'rupa', 'weronika', 'webb', 'pasco', 'sanshayon', 'upasthiyon', 'livaal', 'shaakir', 'ridge', 'rishvatkhor', 'daftarvaale', 'log', 'jodhika', 'meghwal', 'nandoi', 'durghtna', 'chamakna', 'dehra', 'kursiyon', 'pahno', 'bangsh', 'parnel', 'madhya', 'masheenawat', 'gray', 'bhraanti', 'teji', 'moolon', 'dabangata', 'dhuriya', 'toledo', 'dhaniye', 'faadi', 'maathaloo', 'naushad', 'pik', 'katal', 'besil', 'vyapak', 'damaae', 'waco', 'hester', 'chayaniton', 'amarpal', 'phatne', 'tinka', 'arthon', 'neelaabh', 'bilae', 'bhadkaayaa', 'alba', 'texas', 'special', 'ligar', 'parwa', 'pradeshikaa', 'ghost', 'helper', 'bakhshane', 'risaalee', 'ojha', 'aawaazen', 'mool', 'land', 'suchi', 'noon', 'rp', 'bechkar', 'stone', 'baapa', 'roopak', 'nadaf', 'agartala', 'aapdaein', 'aagashe', 'bheera', 'silva', 'nichalaa', 'burhanpur', 'sahooliyata', 'labdh', 'poole', 'ajit', 'jha', 'bhuj', 'anonditaa', 'jaden', 'mufalisi', 'reja', 'mitao', 'munaf', 'mahawar', 'bhusvami', 'vigyan', 'quete', 'bevkufana', 'vilas', 'supachye', 'siratee', 'vishnu', 'chaudhari', 'patiala', 'damae', 'mukeshwaree', 'charas', 'bhadauriya', 'vigyaanon', 'ganjhoo', 'aastik', 'riggs', 'chune', 'hogis', 'winter', 'ghumaee', 'bakhshate', 'sanvedikaran', 'farheen', 'redding', 'shaq', 'kisanon', 'stein', 'bhadauria', 'aadan', 'malekar', 'vikram', 'artemis', 'purvaasiyon', 'pradhan', 'hijbul', 'vidyaon', 'chhawindra', 'chhupkar', 'hemwati', 'jheelon', 'vasoolataa', 'mastek', 'dimaak', 'jodhpur', 'makrani', 'reeta', 'dabangon', 'taras', 'clair', 'virodh', 'boca', 'tankaar', 'khatakati', 'gm', 'karavaayegee', 'bhootal', 'vigyaanam', 'atwaal', 'fargo', 'aleem', 'hr', 'gaur', 'sendha', 'bagad', 'patkathayen', 'bhogte', 'holding', 'badhaati', 'then', 'lahsun', 'bahunt', 'bravo', 'amaanat', 'jhuthla', 'bush', 'apar', 'singhaadey', 'manner', 'himali', 'beetaane', 'besin', 'paradhin', 'carson', 'delta', 'mhajan', 'rookenge', 'maariya', 'raagon', 'mahalon', 'raza', 'sarkaa', 'ahitakaaree', 'belinda', 'alayamanee', 'barsaai', 'toh', 'bechakar', 'kaushalta', 'gaano', 'mta', 'samruddhiyon', 'robins', 'ustara', 'kumari', 'veronika', 'potter', 'chhahon', 'baawaraa', 'hamalavar', 'khalati', 'bachti', 'mohini', 'aaryika', 'chheenk', 'pakaude', 'lucie', 'fatehgadh', 'ubhaara', 'beck', 'kapoor', 'sevaala', 'mogali', 'mitchell', 'dhoraaji', 'chee', 'shasak', 'rahamat', 'wheeling', 'wipro', 'pulam', 'portillo', 'pedoo', 'aradas', 'amarillo', 'tolaasan', 'girkar', 'vaigyaanik', 'phoolatee', 'gandha', 'chakshu', 'aurato', 'ambikaaon', 'ishaak', 'ea', 'ajasra', 'samprabhuta', 'tokne', 'falibhoot', 'pahnaakar', 'kisaani', 'polimer', 'raktaabh', 'jain', 'rasaayanik', 'urgent', 'jini', 'asurakshit', 'alnkaar', 'raadhe', 'nigar', 'monte', 'suneeti', 'ambikaon', 'prahlaad', 'beast', 'sanjivan', 'chadhega', 'worth', 'aasteen', 'aakaash', 'bona', 'kahlaya', 'dhaarak', 'bantata', 'shrikar', 'mitaao', 'bhoopsingh', 'flin', 'sharabh', 'arellano', 'kumar', 'singaar', 'khisakate', 'enid', 'munafa', 'ortega', 'rowe', 'atke', 'yatna', 'city', 'radico', 'dukanon', 'avru', 'hugli', 'erika', 'khinchakar', 'coon', 'lagaateen', 'pahunchogee', 'amit', 'shaao', 'weeks', 'bastiyo', 'harijan', 'aryon', 'dukhiyon', 'yari', 'vidhamaan', 'awasthi', 'mejbani', 'dehri', 'scootaron', 'sheelabhang', 'kalmein', 'mandi', 'hatyaaropit', 'barman', 'jiwanparyant', 'rozlyn', 'gopaalak', 'maisore', 'usaki', 'caplin', 'swamy', 'tyler', 'pehno', 'navaa', 'banaam', 'raajon', 'naimul', 'antony', 'niptaaraa', 'sujhati', 'tide', 'stan', 'julaahon', 'shipra', 'pakhandi', 'sohaval', 'rojana', 'sev', 'chhateni', 'dastaanen', 'dhona', 'aamdi', 'avarodhak', 'ross', 'achyut', 'haraval', 'daaw', 'dehdhaari', 'normal', 'greg', 'lee', 'kashmir', 'maarake', 'dhuriyaa', 'authoritee', 'aagaashe', 'chauka', 'sapramaan', 'tik', 'agyat', 'jodhikaa', 'badalwa', 'bhopal', 'phalibhoot', 'uthaakar', 'somi', 'kanchipuram', 'welding', 'veronica', 'atul', 'plano', 'khhomche', 'mukeshwari', 'tasvaaron', 'vesh', 'chauk', 'chest', 'loowe', 'dhaavakon', 'lim', 'yauniktaa', 'haathe', 'vaigyanik', 'samprati', 'savita', 'jalgti', 'chauki', 'aapka', 'dhamm', 'aithan', 'vartamaan', 'banvaa', 'prasath', 'deshkaal', 'thanvi', 'serrano', 'dharak', 'garud', 'riya', 'chi', 'calumet', 'ankor', 'rapping', 'daav', 'masood', 'ulhaaspur', 'pahanate', 'dhawakon', 'jalvayu', 'bhijvaai', 'parosne', 'vichaarakon', 'aainthoo', 'jaayke', 'dhunai', 'shwing', 'kpada', 'kaju', 'sanchar', 'hollywood', 'chhednaa', 'prana', 'denver', 'lakeer', 'jagriti', 'bulletin', 'rahegi', 'sudhaaratee', 'wasaamukt', 'jewaab', 'soto', 'dose', 'arya', 'ghot', 'tonic', 'gujaarne', 'water', 'varun', 'sharabon', 'awasthaen', 'bhooswaami', 'jue', 'ghontana', 'ropane', 'phaade', 'molina', 'tonk', 'devaapur', 'sim', 'lagnesh', 'moolchand', 'sirtee', 'keeti', 'mangesh', 'sekhar', 'furaame', 'kathua', 'palaari', 'selfie', 'vaadak', 'junction', 'jiya', 'aden', 'feeds', 'bavji', 'vigyanon', 'sihunta', 'adhiwas', 'wajood', 'ria', 'slider', 'spell', 'aapkaa', 'jaagriti', 'gujarat', 'nabha', 'lega', 'dastaar', 'mensharaab', 'ramkot', 'solapur', 'mejbaani', 'florence', 'jhaadane', 'bokaro', 'parosane', 'dhaiya', 'taswaaron', 'kelly', 'mezbaan', 'fulaaye', 'pruitt', 'prashasanik', 'chitrakathaa', 'avshisht', 'avery', 'bairakpur', 'skipper', 'chico', 'labs', 'maniya', 'kanchan', 'piega', 'abhimat', 'chauda', 'milk', 'sutali', 'wealth', 'paim', 'baljit', 'bhatnagar', 'jaatakon', 'pakdaae', 'uplabdhiyon', 'aarya', 'dikhaaiye', 'anishtata', 'jhuthalane', 'ladki', 'roopi', 'yoojee', 'raaj', 'vindhya', 'falne', 'ghataaegee', 'erica', 'bhola', 'rumani', 'sahu', 'hamla', 'kushwaha', 'farmic', 'arth', 'cooch', 'sulkhan', 'santree', 'ichchhee', 'habra', 'paaliyon', 'seth', 'jagtap', 'harshit', 'tasir', 'dork', 'jeenon', 'ukhad', 'manzil', 'niketan', 'twin', 'divya', 'nigmaayukt', 'gabalee', 'rishwatkhor', 'pintaa', 'raakhe', 'jetha', 'chhudwaya', 'ratanpur', 'visheshanon', 'pulinde', 'mahoney', 'khare', 'sawaal', 'aalochak', 'phaans', 'amjhera', 'risane', 'jose', 'bsss', 'najib', 'avsthayen', 'bull', 'ripan', 'mithya', 'biggest', 'jodta', 'orchha', 'prince', 'meza', 'ubaal', 'chart', 'sargi', 'bhaate', 'wipkha', 'bayaane', 'bowie', 'kamaljeet', 'esetal', 'chouhan', 'phalne', 'apex', 'mass', 'andaajee', 'fond', 'putla', 'truth', 'mohim', 'mold', 'hooper', 'soya', 'tariko', 'pahla', 'le', 'satuaa', 'asmitaon', 'want', 'kaaji', 'lynch', 'state', 'sirpur', 'arunachal', 'siyah', 'gujaarane', 'pali', 'chandan', 'mega', 'navketan', 'tapan', 'haimbalin', 'aaraam', 'wokha', 'parween', 'avrodhak', 'ganesha', 'bhumi', 'rangate', 'vidyaaon', 'keshari', 'takara', 'biji', 'chrome', 'chamach', 'patki', 'kangan', 'sipahi', 'usak', 'nagpur', 'flesh', 'subhaay', 'fund', 'mishra', 'sarguja', 'mundwa', 'tolasan', 'kanaal', 'fortis', 'jainon', 'khitabon', 'kumavat', 'dhaak', 'samprbhuta', 'jauhari', 'khitaabon', 'ghise', 'baadhit', 'koling', 'samadhiyon', 'durghatna', 'singam', 'uthnee', 'handiyaa', 'ikai', 'pitts', 'ropi', 'chaukiyan', 'vakrangee', 'bhoogol', 'sivan', 'mahanideshak', 'kanyaein', 'odom', 'qss', 'sapane', 'chuneen', 'uski', 'fyuji', 'bhusi', 'haliyaa', 'pope', 'vasulta', 'udar', 'sudip', 'bhatkaanaa', 'lima', 'sah', 'villa', 'linda', 'khariya', 'chellani', 'kurmi', 'ghumaaee', 'khemik', 'tugalki', 'bhakaa', 'obrayan', 'krutya', 'najriyon', 'mayur', 'feka', 'tinkaa', 'jaatiyon', 'ved', 'kamzor', 'suman', 'padanee', 'dukhon', 'nesco', 'phooltee', 'tools', 'mitra', 'devnaar', 'kar', 'daftarwale', 'jwar', 'dhaatuein', 'siyaha', 'jita', 'mukarte', 'khangaali', 'bodara', 'rojlin', 'truck', 'nations', 'sevakon', 'adhivas', 'tareeko', 'kav', 'chaurasiya', 'clay', 'devapur', 'niya', 'aagaman', 'aashriton', 'lordaa', 'moshe', 'lohaati', 'siwan', 'thaki', 'letakar', 'charit', 'asahi', 'pehan', 'faarensis', 'harpaal', 'khairati', 'deshke', 'seho']\n","['थरमैक्स', 'कोल्हापुर', 'फूहड़ता', 'मुशाहिद', 'लिखकर', 'तलवे', 'सीमाई', 'रिवास', 'अनुसारका', 'बुएना', 'कैटलिन', 'वंडर', 'श्विंग', 'नीलाभ', 'मास', 'वेस्टिंग', 'कुर्वेती', 'मुकेश्वरी', 'श्रीमति', 'कबहा', 'सिंगार', 'पलटने', 'नगाड़ा', 'क्लेन', 'कोविना', 'फूस', 'नेको', 'कौशल', 'फरसे', 'मुर्गीपालन', 'संयुक्त', 'सभाएं', 'मैक्सवेल', 'दीपन', 'बैक्सटर', 'घासी', 'इकट्ठा', 'भूपसिंह', 'हर्ले', 'भ्रांति', 'बागो', 'क्रेन', 'फर्थ', 'सुहागिन', 'चढ़ने', 'रिडक्शन', 'एक्सेल', 'भाल', 'संख्याएं', 'हरपाल', 'एंजेलो', 'पोकर', 'अपवर्तित', 'शैली', 'देवी', 'आमदनी', 'ब्रोक', 'कुश', 'चढ़ें', 'वांगचुक', 'सचन', 'घोंटना', 'अकोला', 'फोड़ा', 'नूबिया', 'मंजिलें', 'जुराब', 'फुलाए', 'ब्लैक', 'जोहरी', 'वेल्डर', 'बहावलपुर', 'चौसिंगा', 'पफिन', 'टीकों', 'अथक', 'माप', 'सहूलियत', 'पेंडुलम', 'चिपकने', 'गोंड', 'युद्धक', 'वेगास', 'चित्रांशी', 'इबीजा', 'वर्णित', 'बरेली', 'जिंग', 'वेस्टिंगहाउस', 'त्रिया', 'हैतो', 'खेलमंत्री', 'भविष्य', 'प्रदेश', 'संवेदीकरण', 'लगुना', 'रेस', 'मंगला', 'यूक्लिड', 'किशुन', 'अमरोही', 'जेनर', 'शावकों', 'रूपा', 'वेरोनिका', 'वेब', 'पास्को', 'संशयों', 'उपस्थियों', 'लिवाल', 'शाकिर', 'रिज', 'रिश्वतखोर', 'दफ्तरवाले', 'लोग', 'जोधिका', 'मेघवाल', 'नंदोई', 'दुर्घटना', 'चमकना', 'देहरा', 'कुर्सियों', 'पहनो', 'बंगश', 'पार्नेल', 'मध्य', 'मशीनवत', 'ग्रे', 'भ्रांति', 'तेजी', 'मूलों', 'दबंगता', 'धुरिया', 'टोलेडो', 'धनिये', 'फाड़ी', 'माथलू', 'नौशाद', 'पिक', 'कताल', 'बेसिल', 'व्यापक', 'दमाए', 'वाको', 'हेस्टर', 'चयनितों', 'अमरपाल', 'फटने', 'टिंका', 'अर्थों', 'नीलाभ', 'बिलाई', 'भड़काया', 'अल्बा', 'टेक्सस', 'स्पेशल', 'लिगर', 'परवा', 'प्रदेशिका', 'घोस्ट', 'हेल्पर', 'बख्शने', 'रिसाली', 'ओझा', 'आवाज़ें', 'मूल', 'लैंड', 'सूची', 'नून', 'आरपी', 'बेचकर', 'स्टोन', 'बापा', 'रूपक', 'नदफ', 'अगरतला', 'आपदाएं', 'आगाशे', 'भीरा', 'सिल्वा', 'निचला', 'बुरहानपुर', 'सहूलियत', 'लब्ध', 'पूल', 'अजित', 'झ', 'भुज', 'अनोंदिता', 'जड़ें', 'मुफलिसी', 'रेजा', 'मिटाओ', 'मुनाफ', 'महावर', 'भूस्वामी', 'विज्ञान', 'क्यूट', 'बेवकूफाना', 'विलास', 'सुपाच्य', 'सिरती', 'विष्णु', 'चौधरी', 'पटियाला', 'दमाए', 'मुकेश्वरी', 'चरस', 'भदौरिया', 'विज्ञानों', 'गंझू', 'आस्तिक', 'रिग्स', 'चुने', 'होगिस', 'विंटर', 'घुमाई', 'बख्शते', 'संवेदीकरण', 'फरहीन', 'रेडिंग', 'शक़', 'किसानों', 'स्टीन', 'भदौरिया', 'आदान', 'मालेकर', 'विक्रम', 'आर्टेमिस', 'पुरवासियों', 'प्रधान', 'हिजबुल', 'विद्याओं', 'छविंद्र', 'छुपकर', 'हेमवती', 'झीलों', 'वसूलता', 'मस्टेक', 'दिमाक', 'जोधपुर', 'मकरानी', 'रीता', 'दबंगों', 'तरस', 'क्लेयर', 'विरोध', 'बोका', 'टंकार', 'खटकती', 'जीएम', 'करवायेगी', 'भूतल', 'विज्ञानम', 'अटवाल', 'फार्गो', 'अलीम', 'एचआर', 'गौर', 'सेंधा', 'बगड़', 'पटकथाएं', 'भोगते', 'होल्डिंग', 'बढ़ाती', 'थेन', 'लहसुन', 'बहुंत', 'ब्रावो', 'अमानत', 'झुठला', 'बुश', 'अपर', 'सिंघाड़े', 'मैनर', 'हिमाली', 'बीताने', 'बेसिन', 'पराधीन', 'कार्सन', 'डेल्टा', 'महाजन', 'रूकेंगे', 'मारिया', 'रागों', 'महलों', 'रज़ा', 'सरका', 'अहितकारी', 'बेलिंडा', 'अलयमनी', 'बरसाई', 'तो', 'बेचकर', 'कौशलता', 'गानो', 'एमटीए', 'समृद्धियों', 'रॉबिंस', 'उस्तरा', 'कुमारी', 'वेरोनिका', 'पॉटर', 'छहों', 'बावरा', 'हमलावर', 'खलती', 'बचती', 'मोहिनी', 'आर्यिका', 'छींक', 'पकौड़े', 'लूसी', 'फतेहगढ़', 'उभारा', 'बेक', 'कपूर', 'सेवाला', 'मोगली', 'मिचेल', 'धोराजी', 'ची', 'शासक', 'रहमत', 'व्हीलिंग', 'विप्रो', 'पुलम', 'पोर्टिलो', 'पेडू', 'अरदास', 'अमारिलो', 'तोलासन', 'गिरकर', 'वैज्ञानिक', 'फूलती', 'गंध', 'चक्षु', 'औरतो', 'अम्बिकाओं', 'इशाक', 'ईए', 'अजस्र', 'संप्रभुता', 'टोकने', 'फलीभूत', 'पहनाकर', 'किसानी', 'पॉलिमर', 'रक्ताभ', 'जैन', 'रसायनिक', 'अर्जेंट', 'जिनी', 'असुरक्षित', 'अलंकार', 'राधे', 'निगर', 'मोंटे', 'सुनीति', 'अम्बिकाओं', 'प्रहलाद', 'बीस्ट', 'संजीवन', 'चढ़ेगा', 'वर्थ', 'आस्तीन', 'आकाश', 'बोना', 'कहलाया', 'धारक', 'बंटता', 'श्रीकर', 'मिटाओ', 'भूपसिंह', 'फ्लिन', 'शरभ', 'अरेलानो', 'कुमार', 'सिंगार', 'खिसकते', 'एनिड', 'मुनाफा', 'ओर्टेगा', 'रोवे', 'अटके', 'यत्न', 'सिटी', 'रेडिको', 'दुकानों', 'अवरु', 'हुगली', 'एरिका', 'खिंचकर', 'कून', 'लगातीं', 'पहुंचोगी', 'अमित', 'शाओ', 'वीक्स', 'बस्तियो', 'हरिजन', 'आर्यों', 'दुखियों', 'यारी', 'विधमान', 'अवस्थी', 'मेजबानी', 'डेहरी', 'स्कूटरों', 'शीलभंग', 'कलमें', 'मंडी', 'हत्यारोपित', 'बर्मन', 'जीवनपर्यंत', 'रोजलिन', 'गोपालक', 'मैसोर', 'उसकी', 'कैपलिन', 'स्वामी', 'टाइलर', 'पहनो', 'नवा', 'बनाम', 'राजों', 'नैमुल', 'एंटोनी', 'निपटारा', 'सुझाती', 'टाइड', 'स्टैन', 'जुलाहों', 'शिप्रा', 'पाखंडी', 'सोहावल', 'रोजना', 'सेव', 'छतेनी', 'दस्तानें', 'धोना', 'आमदी', 'अवरोधक', 'रॉस', 'अच्युत', 'हरावल', 'दाव', 'देहधारी', 'नॉर्मल', 'ग्रेग', 'ली', 'कश्मीर', 'मारके', 'धुरिया', 'ऑथोरिटी', 'आगाशे', 'चौका', 'सप्रमाण', 'टिक', 'अज्ञात', 'जोधिका', 'बदलवा', 'भोपाल', 'फलीभूत', 'उठाकर', 'सोमी', 'कांचीपुरम', 'वेल्डिंग', 'वेरोनिका', 'अतुल', 'प्लानो', 'खोमचे', 'मुकेश्वरी', 'तस्वारों', 'वेश', 'चौक', 'चेस्ट', 'लूवे', 'धावकों', 'लिम', 'यौनिकता', 'हाथे', 'वैज्ञानिक', 'संप्रति', 'सविता', 'जलगति', 'चौकी', 'आपका', 'धम्म', 'ऐठन', 'वर्तमान', 'बनवा', 'प्रसाथ', 'देशकाल', 'थानवी', 'सेरानो', 'धारक', 'गरुड़', 'रिया', 'ची', 'कैल्यूमेट', 'अंकोर', 'रैपिंग', 'दाव', 'मसूद', 'उल्हासपुर', 'पहनते', 'धावकों', 'जलवायु', 'भिजवाई', 'परोसने', 'विचारकों', 'आइंथू', 'जायके', 'धुनाई', 'श्विंग', 'कपड़ा', 'काजू', 'संचार', 'हॉलीवुड', 'छेड़ना', 'प्राना', 'डेनवर', 'लकीर', 'जागृति', 'बुलेटिन', 'रहेगी', 'सुधारती', 'वसामुक्त', 'जेवाब', 'सोटो', 'डोसे', 'आर्य', 'घोट', 'टॉनिक', 'गुजारने', 'वाटर', 'वरुण', 'शराबों', 'अवस्थाएं', 'भूस्वामी', 'जुए', 'घोंटना', 'रोपने', 'फाड़े', 'मोलिना', 'टोंक', 'देवापुर', 'सिम', 'लग्नेश', 'मूलचंद', 'सिरती', 'कीति', 'मंगेश', 'सेखर', 'फुरामे', 'कठुआ', 'पलारी', 'सेल्फी', 'वादक', 'जंक्शन', 'जिया', 'एडेन', 'फीड्स', 'बावजी', 'विज्ञानों', 'सिहुंता', 'अधिवास', 'वजूद', 'रिया', 'स्लाइडर', 'स्पेल', 'आपका', 'जागृति', 'गुजरात', 'नाभा', 'लेगा', 'दस्तार', 'मेंशराब', 'रामकोट', 'सोलापुर', 'मेजबानी', 'फ्लोरेंस', 'झाड़ने', 'बोकारो', 'परोसने', 'ढैया', 'तस्वारों', 'केली', 'मेजबान', 'फुलाए', 'प्रूट', 'प्रशासनिक', 'चित्रकथा', 'अवशिष्ट', 'एवरी', 'बैरकपुर', 'स्किपर', 'चिको', 'लैब्स', 'मनिया', 'कंचन', 'पिएगा', 'अभिमत', 'चौड़ा', 'मिल्क', 'सुतली', 'वेल्थ', 'पैम', 'बलजीत', 'भटनागर', 'जातकों', 'पकड़ाए', 'उपलब्धियों', 'आर्य', 'दिखाइये', 'अनिष्टता', 'झुठलाने', 'लड़की', 'रूपी', 'यूजी', 'राज', 'विंध्या', 'फलने', 'घटाएगी', 'एरिका', 'भोला', 'रुमानी', 'साहू', 'हमला', 'कुशवाहा', 'फार्मिक', 'अर्थ', 'कूच', 'सुलखान', 'संत्री', 'इच्छी', 'हाबरा', 'पालियों', 'सेथ', 'जगताप', 'हर्षित', 'तासीर', 'डॉर्क', 'जीनों', 'उखड', 'मंजिल', 'निकेतन', 'ट्विन', 'दिव्य', 'निगमायुक्त', 'गबली', 'रिश्वतखोर', 'पिंटा', 'राखे', 'जेठा', 'छुड़वाया', 'रतनपुर', 'विशेषणों', 'पुलिंदे', 'महोनी', 'खरे', 'सवाल', 'आलोचक', 'फाँस', 'अमझेरा', 'रिसने', 'जोस', 'बीएसएसएस', 'नजीब', 'अवस्थाएं', 'बुल', 'रिपन', 'मिथ्या', 'बिगेस्ट', 'जोड़ता', 'ओरछा', 'प्रिंस', 'मेज़ा', 'उबाल', 'चार्ट', 'सरगी', 'भाते', 'विप्ख', 'बयाने', 'बोवी', 'कमलजीत', 'एसेटल', 'चौहान', 'फलने', 'एपेक्स', 'मास', 'अंदाजी', 'फोंड', 'पुतला', 'ट्रुथ', 'मोहिम', 'मोल्ड', 'हूपर', 'सोया', 'तरीको', 'पहला', 'ले', 'सतुआ', 'अस्मिताओं', 'वांट', 'काजी', 'लिंच', 'स्टेट', 'सिरपुर', 'अरुणाचल', 'सियाह', 'गुजारने', 'पाली', 'चंदन', 'मेगा', 'नवकेतन', 'तपन', 'हैम्बलिन', 'आराम', 'वोखा', 'परवीन', 'अवरोधक', 'गणेशा', 'भूमी', 'रंगते', 'विद्याओं', 'केशरी', 'टकरा', 'बिजी', 'क्रोम', 'चमच', 'पटकी', 'कंगन', 'सिपाही', 'उसक', 'नागपुर', 'फ्लेश', 'सुभाय', 'फंड', 'मिश्रा', 'सरगुजा', 'मुंडवा', 'तोलासन', 'कनाल', 'फोर्टिस', 'जैनों', 'खिताबों', 'कुमावत', 'धाक', 'संप्रभुता', 'जौहरी', 'खिताबों', 'घिसे', 'बाधित', 'कोलिंग', 'समाधियों', 'दुर्घटना', 'सिंगम', 'उठनी', 'हंडिया', 'इकाई', 'पिट्स', 'रोपी', 'चौकियां', 'वकरंगी', 'भूगोल', 'सिवान', 'महानिदेशक', 'कन्याएं', 'ओडोम', 'क्यूएसएस', 'सपने', 'चुनीं', 'उसकी', 'फ्यूजी', 'भूसी', 'हलिया', 'पोप', 'वसूलता', 'उदर', 'सुदीप', 'भटकाना', 'लिमा', 'साह', 'विला', 'लिंडा', 'खरिया', 'चेलानी', 'कुर्मी', 'घुमाई', 'खेमिक', 'तुगलकी', 'भका', 'ओब्रायन', 'कृत्या', 'नजरियों', 'मयूर', 'फेका', 'टिंका', 'जातियों', 'वेड', 'कमज़ोर', 'सुमन', 'पड़नी', 'दुखों', 'नेस्को', 'फूलती', 'टूल्स', 'मित्रा', 'देवनार', 'कर', 'दफ्तरवाले', 'ज्वार', 'धातुएं', 'सियाह', 'जिता', 'मुकरते', 'खंगाली', 'बोदरा', 'रोजलिन', 'ट्रक', 'नेशंस', 'सेवकों', 'अधिवास', 'तरीको', 'काव', 'चौरसिया', 'क्ले', 'देवापुर', 'निया', 'आगमन', 'आश्रितों', 'लोरदा', 'मोशे', 'लोहाती', 'सिवान', 'थकी', 'लेटकर', 'चरित', 'असाही', 'पहन', 'फारेंसिस', 'हरपाल', 'खैराती', 'देशके', 'सेहो']\n"]}],"source":["print(c_trg)\n","print(c_src)\n","print(c_pred)\n","# Here we got the lists of correct source,target,predicted words\n","# And incorrect source,target,predicted words\n","# now we're writing it into csv.\n","#and later plotting it using Wandb\n","\n","import csv\n","def save_to_csv(src_list, trg_list, pred_list, file_name):\n","    rows = zip(src_list, trg_list, pred_list)\n","\n","    with open(file_name, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['English', 'Target', 'Predicted'])\n","        writer.writerows(rows)\n","\n","save_to_csv(c_src,c_trg,c_pred,'correct_predictions.csv')\n","save_to_csv(i_src,i_trg,i_pred,'incorrect_predictions.csv')\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T21:17:51.098520Z","iopub.status.busy":"2023-05-20T21:17:51.098129Z","iopub.status.idle":"2023-05-20T21:18:05.084965Z","shell.execute_reply":"2023-05-20T21:18:05.083786Z","shell.execute_reply.started":"2023-05-20T21:17:51.098487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["from signal import signal,SIGPIPE, SIG_DFL\n","signal(SIGPIPE,SIG_DFL)\n","!pip install wandb -qU\n","import wandb\n","!wandb login da816d14625ef44d200ee4acaa517646962e6f9a"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T21:21:48.892182Z","iopub.status.busy":"2023-05-20T21:21:48.891289Z","iopub.status.idle":"2023-05-20T21:22:22.974281Z","shell.execute_reply":"2023-05-20T21:22:22.973448Z","shell.execute_reply.started":"2023-05-20T21:21:48.892146Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:xxa6evrd) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">radiant-meadow-10</strong> at: <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/xxa6evrd' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/xxa6evrd</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230520_211843-xxa6evrd/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:xxa6evrd). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230520_212149-1io3f9nc</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/1io3f9nc' target=\"_blank\">swept-totem-252</a></strong> to <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/1io3f9nc' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/1io3f9nc</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7f45f015d4e0>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Load the CSV file\n","ca_dataframe = pd.read_csv(\"/kaggle/working/correct_predictions.csv\")\n","a_table = wandb.Table(dataframe=ca_dataframe)\n","\n","# Add the table to an Artifact to increase the row \n","# limit to 200000 and make it easier to reuse\n","ca_table_artifact = wandb.Artifact(\n","    \"correct_predictions_Attn\", \n","    type=\"dataset\"\n","    )        \n","ca_table_artifact.add(a_table, \"Correct_predictions_Attn\")\n","\n","# Log the raw csv file within an artifact to preserve our data\n","ca_table_artifact.add_file(\"/kaggle/working/correct_predictions.csv\")\n","\n","# Display as a table\n","\n","\n","run = wandb.init(project='CS6910_Assignment3')\n","\n","# Log the table to visualize with a run...\n","run.log({\"Attn_correct_predictions_table\": a_table})\n","\n","# and Log as an Artifact to increase the available row limit!\n","run.log_artifact(ca_table_artifact)\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T20:56:15.216339Z","iopub.status.busy":"2023-05-20T20:56:15.215811Z","iopub.status.idle":"2023-05-20T20:57:15.026739Z","shell.execute_reply":"2023-05-20T20:57:15.025858Z","shell.execute_reply.started":"2023-05-20T20:56:15.216300Z"},"trusted":true},"outputs":[{"data":{"text/html":["Finishing last run (ID:be0h5dcu) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">robust-morning-7</strong> at: <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/be0h5dcu' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/be0h5dcu</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230520_205539-be0h5dcu/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:be0h5dcu). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230520_205615-6n6eeevs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/6n6eeevs' target=\"_blank\">valiant-frog-8</a></strong> to <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/6n6eeevs' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/6n6eeevs</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7f45f016b880>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Load the CSV file\n","i_dataframe = pd.read_csv(\"/kaggle/working/incorrect_predictions.csv\")\n","i_table = wandb.Table(dataframe=i_dataframe)\n","\n","# Add the table to an Artifact to increase the row \n","# limit to 200000 and make it easier to reuse\n","i_table_artifact = wandb.Artifact(\n","    \"incorrect_predictions_Attn\", \n","    type=\"dataset\"\n","    )        \n","i_table_artifact.add(i_table, \"Incorrect_predictions_Attn\")\n","\n","# Log the raw csv file within an artifact to preserve our data\n","i_table_artifact.add_file(\"/kaggle/working/incorrect_predictions.csv\")\n","\n","# Display as a table\n","\n","\n","run = wandb.init(project='CS6910_Assignment3')\n","\n","# Log the table to visualize with a run...\n","run.log({\"Attn_incorrect_predictions_table\": i_table})\n","\n","# and Log as an Artifact to increase the available row limit!\n","run.log_artifact(i_table_artifact)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T05:42:11.457227Z","iopub.status.busy":"2023-05-20T05:42:11.456871Z","iopub.status.idle":"2023-05-20T05:42:31.164193Z","shell.execute_reply":"2023-05-20T05:42:31.163023Z","shell.execute_reply.started":"2023-05-20T05:42:11.457199Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["from signal import signal,SIGPIPE, SIG_DFL\n","signal(SIGPIPE,SIG_DFL)\n","!pip install wandb -qU\n","import wandb\n","!wandb login da816d14625ef44d200ee4acaa517646962e6f9a"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T05:42:56.094992Z","iopub.status.busy":"2023-05-20T05:42:56.094607Z","iopub.status.idle":"2023-05-20T05:42:56.121675Z","shell.execute_reply":"2023-05-20T05:42:56.120804Z","shell.execute_reply.started":"2023-05-20T05:42:56.094956Z"},"trusted":true},"outputs":[],"source":["# wandb sweeps\n","\n","sweep_config= {\n","    \"name\" : \"CS6910_Assignment3\",\n","    \"method\" : \"bayes\",\n","    'metric': {\n","        'name': 'val_acc',\n","        'goal': 'maximize'\n","    },\n","    'parameters' : {\n","        'cell_type' : { 'values' : ['lstm','gru','rnn'] },\n","        'dropout' : { 'values' : [0,0.1,0.2,0.5]},\n","        'embedding_size' : {'values' : [64,128,256,512]},\n","        'num_layers' : {'values' : [1]},\n","        'batch_size' : {'values' : [32,64,128]},\n","        'hidden_size' : {'values' : [128,256,512]},\n","        'bidirectional' : {'values' : [True ,False]},\n","        'learning_rate':{\n","            \"values\": [0.001,0.002,0.0001,0.0002]\n","        },\n","        'optim':{\n","            \"values\": ['adam','nadam']\n","        },\n","        'teacher_forcing':{\"values\":[0.2,0.5,0.7]}\n","    }\n","}\n","\n","\n","\n","def train():\n","    wandb.init()\n","\n","    c= wandb.config\n","    name = \"cell_type_\"+str(c.cell_type)+\"_num_layers_\"+str(c.num_layers)+\"_dp_\"+str(c.dropout)+\"_bidir_\"+str(c.bidirectional)+\"_lr_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)\n","    wandb.run.name=name\n","  \n","    # Retrieve the hyperparameters from the config\n","    ct=c.cell_type\n","    dp = c.dropout\n","    em=c.embedding_size\n","    nlayer=c.num_layers\n","    bs = c.batch_size\n","    hs=c.hidden_size\n","    bidir = c.bidirectional\n","    lr = c.learning_rate\n","    opt= c.optim\n","    epochs = 25\n","    tf=c.teacher_forcing\n","    trg_pad_idx=0\n","\n","  \n","\n","    INPUT_DIM = 29\n","    OUTPUT_DIM = 67\n","\n","  \n","  # Load the dataset\n","    train_loader,val_loader,test_loader,idx_to_char=load_data(bs)\n","   \n","  #print(\"data loaded ====================================================\")\n","\n","  # Instantiate the Encoder and Decoder models\n","    encoder = Encoder(INPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)\n","    decoder = Decoder(OUTPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)\n","\n","  # Instantiate the Seq2Seq model with the Encoder and Decoder models\n","    model = Seq2Seq(encoder,decoder,ct,bidir).to(device)\n","  #print(\"model ini==============================================================\")\n"," \n","  # Define the loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()      \n","    if opt == \"adam\":\n","          optimizer = optim.Adam(model.parameters(),lr=lr)\n","    elif opt == \"nadam\":\n","          optimizer= optim.NAdam(model.parameters(),lr=lr)\n","  \n","  # Train Network\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        model.train()\n","\n","        for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n","            src = src.permute(1, 0)  # swapping the dimensions of src tensor\n","            trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n","\n","            src = src.to(device)\n","            trg = trg.to(device)\n","            #print(\"done\")\n","            optimizer.zero_grad()\n","            #print(\"doe\")\n","            output = model(src,trg,tf)\n","            #print(\"doe\")\n","\n","            # Ignore the first element of the output, which is initialized as all zeros\n","            # since we use it to store the output for the start-of-sequence token\n","            #print(output.shape[2])\n","\n","            output = output[1:].reshape(-1, output.shape[2])\n","            #print(output.shape)\n","            #print(trg.shape)\n","            trg = trg[1:].reshape(-1)\n","\n","            loss = criterion(output, trg)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","            if batch_idx % 1000 == 0:\n","                print(f\"Epoch: {epoch}, Batch: {batch_idx} , Training..\")\n","        \n","        # Calculate word-level accuracy after every epoch\n","        #train_acc ,train_loss= calculate_word_level_accuracy(model, train_loader,criterion)\n","        val_acc,val_loss = calculate_word_level_accuracy(model,idx_to_char, val_loader, criterion)\n","        test_acc,test_loss = calculate_word_level_accuracy(model,idx_to_char, test_loader, criterion)\n","     \n","    #print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n","\n","            \n","    # Log the metrics to WandB\n","        wandb.log({'epoch': epochs, 'train_loss': loss.item(), 'test_acc': test_acc,'val_acc': val_acc,'test_loss': test_loss,'val_loss': val_loss})\n","    # Save the best model\n","    wandb.run.save()\n","    wandb.run.finish()\n","    return\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T05:42:59.328989Z","iopub.status.busy":"2023-05-20T05:42:59.328624Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: deebsv1j with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22s015\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230520_054304-deebsv1j</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/deebsv1j' target=\"_blank\">tough-sweep-41</a></strong> to <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/deebsv1j' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/deebsv1j</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Batch: 0 , Training..\n","Epoch: 0, Batch: 1000 , Training..\n","Total 4096\n","Correct 564\n","Total 4096\n","Correct 668\n","Epoch: 1, Batch: 0 , Training..\n","Epoch: 1, Batch: 1000 , Training..\n","Total 4096\n","Correct 1044\n","Total 4096\n","Correct 1230\n","Epoch: 2, Batch: 0 , Training..\n","Epoch: 2, Batch: 1000 , Training..\n","Total 4096\n","Correct 1306\n","Total 4096\n","Correct 1534\n","Epoch: 3, Batch: 0 , Training..\n","Epoch: 3, Batch: 1000 , Training..\n","Total 4096\n","Correct 1402\n","Total 4096\n","Correct 1596\n","Epoch: 4, Batch: 0 , Training..\n","Epoch: 4, Batch: 1000 , Training..\n","Total 4096\n","Correct 1490\n","Total 4096\n","Correct 1718\n","Epoch: 5, Batch: 0 , Training..\n","Epoch: 5, Batch: 1000 , Training..\n","Total 4096\n","Correct 1596\n","Total 4096\n","Correct 1814\n","Epoch: 6, Batch: 0 , Training..\n","Epoch: 6, Batch: 1000 , Training..\n","Total 4096\n","Correct 1570\n","Total 4096\n","Correct 1826\n","Epoch: 7, Batch: 0 , Training..\n","Epoch: 7, Batch: 1000 , Training..\n","Total 4096\n","Correct 1552\n","Total 4096\n","Correct 1814\n","Epoch: 8, Batch: 0 , Training..\n","Epoch: 8, Batch: 1000 , Training..\n","Total 4096\n","Correct 1646\n","Total 4096\n","Correct 1952\n","Epoch: 9, Batch: 0 , Training..\n","Epoch: 9, Batch: 1000 , Training..\n","Total 4096\n","Correct 1654\n","Total 4096\n","Correct 1884\n","Epoch: 10, Batch: 0 , Training..\n","Epoch: 10, Batch: 1000 , Training..\n","Total 4096\n","Correct 1636\n","Total 4096\n","Correct 1854\n","Epoch: 11, Batch: 0 , Training..\n","Epoch: 11, Batch: 1000 , Training..\n","Total 4096\n","Correct 1648\n","Total 4096\n","Correct 1936\n","Epoch: 12, Batch: 0 , Training..\n","Epoch: 12, Batch: 1000 , Training..\n","Total 4096\n","Correct 1640\n","Total 4096\n","Correct 1878\n","Epoch: 13, Batch: 0 , Training..\n","Epoch: 13, Batch: 1000 , Training..\n","Total 4096\n","Correct 1692\n","Total 4096\n","Correct 1898\n","Epoch: 14, Batch: 0 , Training..\n","Epoch: 14, Batch: 1000 , Training..\n","Total 4096\n","Correct 1650\n","Total 4096\n","Correct 1926\n","Epoch: 15, Batch: 0 , Training..\n","Epoch: 15, Batch: 1000 , Training..\n","Total 4096\n","Correct 1722\n","Total 4096\n","Correct 1998\n","Epoch: 16, Batch: 0 , Training..\n","Epoch: 16, Batch: 1000 , Training..\n","Total 4096\n","Correct 1674\n","Total 4096\n","Correct 1998\n","Epoch: 17, Batch: 0 , Training..\n","Epoch: 17, Batch: 1000 , Training..\n","Total 4096\n","Correct 1686\n","Total 4096\n","Correct 1952\n","Epoch: 18, Batch: 0 , Training..\n","Epoch: 18, Batch: 1000 , Training..\n","Total 4096\n","Correct 1686\n","Total 4096\n","Correct 1934\n","Epoch: 19, Batch: 0 , Training..\n","Epoch: 19, Batch: 1000 , Training..\n","Total 4096\n","Correct 1668\n","Total 4096\n","Correct 1932\n","Epoch: 20, Batch: 0 , Training..\n","Epoch: 20, Batch: 1000 , Training..\n","Total 4096\n","Correct 1644\n","Total 4096\n","Correct 1986\n","Epoch: 21, Batch: 0 , Training..\n","Epoch: 21, Batch: 1000 , Training..\n","Total 4096\n","Correct 1638\n","Total 4096\n","Correct 1978\n","Epoch: 22, Batch: 0 , Training..\n","Epoch: 22, Batch: 1000 , Training..\n","Total 4096\n","Correct 1624\n","Total 4096\n","Correct 1928\n","Epoch: 23, Batch: 0 , Training..\n","Epoch: 23, Batch: 1000 , Training..\n","Total 4096\n","Correct 1704\n","Total 4096\n","Correct 1936\n","Epoch: 24, Batch: 0 , Training..\n","Epoch: 24, Batch: 1000 , Training..\n","Total 4096\n","Correct 1734\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"]},{"name":"stdout","output_type":"stream","text":["Total 4096\n","Correct 1970\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁▄▆▆▇▇▇▇█▇▇█▇▇███████████</td></tr><tr><td>test_loss</td><td>██▅▂▄▃▄▁▆▂▄▄▅▃▆▆▆▇▆▆▇▄▆█▇</td></tr><tr><td>train_loss</td><td>██▆▅▂▂▅▃▅▄▃▂▃▃▃▅▁▂▃▁▁▄▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▇▇▇▇▇█▇▇▇█▇█████▇▇▇██</td></tr><tr><td>val_loss</td><td>▅▆▃▁▃▃▃▂▄▂▅▅▄▃▆▆▇▆▆▆▇▅▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>test_acc</td><td>48.0957</td></tr><tr><td>test_loss</td><td>1.49381</td></tr><tr><td>train_loss</td><td>0.21212</td></tr><tr><td>val_acc</td><td>42.33398</td></tr><tr><td>val_loss</td><td>1.60856</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">tough-sweep-41</strong> at: <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/deebsv1j' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/deebsv1j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230520_054304-deebsv1j/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: euy6ydgm with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230520_061737-euy6ydgm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/euy6ydgm' target=\"_blank\">breezy-sweep-42</a></strong> to <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/sweeps/72j6yzep</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/euy6ydgm' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/euy6ydgm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Batch: 0 , Training..\n","Epoch: 0, Batch: 1000 , Training..\n","Total 4096\n","Correct 276\n","Total 4096\n","Correct 280\n","Epoch: 1, Batch: 0 , Training..\n","Epoch: 1, Batch: 1000 , Training..\n","Total 4096\n","Correct 524\n","Total 4096\n","Correct 598\n","Epoch: 2, Batch: 0 , Training..\n","Epoch: 2, Batch: 1000 , Training..\n","Total 4096\n","Correct 746\n","Total 4096\n","Correct 736\n","Epoch: 3, Batch: 0 , Training..\n","Epoch: 3, Batch: 1000 , Training..\n","Total 4096\n","Correct 820\n","Total 4096\n","Correct 898\n","Epoch: 4, Batch: 0 , Training..\n","Epoch: 4, Batch: 1000 , Training..\n","Total 4096\n","Correct 926\n","Total 4096\n","Correct 1000\n","Epoch: 5, Batch: 0 , Training..\n","Epoch: 5, Batch: 1000 , Training..\n","Total 4096\n","Correct 964\n","Total 4096\n","Correct 1062\n","Epoch: 6, Batch: 0 , Training..\n","Epoch: 6, Batch: 1000 , Training..\n","Total 4096\n","Correct 1010\n","Total 4096\n","Correct 1204\n","Epoch: 7, Batch: 0 , Training..\n","Epoch: 7, Batch: 1000 , Training..\n","Total 4096\n","Correct 1072\n","Total 4096\n","Correct 1288\n","Epoch: 8, Batch: 0 , Training..\n","Epoch: 8, Batch: 1000 , Training..\n","Total 4096\n","Correct 1132\n","Total 4096\n","Correct 1200\n","Epoch: 9, Batch: 0 , Training..\n","Epoch: 9, Batch: 1000 , Training..\n","Total 4096\n","Correct 1104\n","Total 4096\n","Correct 1286\n","Epoch: 10, Batch: 0 , Training..\n","Epoch: 10, Batch: 1000 , Training..\n","Total 4096\n","Correct 1166\n","Total 4096\n","Correct 1332\n","Epoch: 11, Batch: 0 , Training..\n","Epoch: 11, Batch: 1000 , Training..\n","Total 4096\n","Correct 1268\n","Total 4096\n","Correct 1406\n","Epoch: 12, Batch: 0 , Training..\n","Epoch: 12, Batch: 1000 , Training..\n","Total 4096\n","Correct 1250\n","Total 4096\n","Correct 1308\n","Epoch: 13, Batch: 0 , Training..\n","Epoch: 13, Batch: 1000 , Training..\n","Total 4096\n","Correct 1256\n","Total 4096\n","Correct 1386\n","Epoch: 14, Batch: 0 , Training..\n","Epoch: 14, Batch: 1000 , Training..\n","Total 4096\n","Correct 1246\n","Total 4096\n","Correct 1352\n","Epoch: 15, Batch: 0 , Training..\n","Epoch: 15, Batch: 1000 , Training..\n","Total 4096\n","Correct 1302\n","Total 4096\n","Correct 1456\n","Epoch: 16, Batch: 0 , Training..\n","Epoch: 16, Batch: 1000 , Training..\n","Total 4096\n","Correct 1352\n","Total 4096\n","Correct 1426\n","Epoch: 17, Batch: 0 , Training..\n","Epoch: 17, Batch: 1000 , Training..\n","Total 4096\n","Correct 1280\n","Total 4096\n","Correct 1422\n","Epoch: 18, Batch: 0 , Training..\n","Epoch: 18, Batch: 1000 , Training..\n","Total 4096\n","Correct 1394\n","Total 4096\n","Correct 1460\n","Epoch: 19, Batch: 0 , Training..\n","Epoch: 19, Batch: 1000 , Training..\n","Total 4096\n","Correct 1332\n","Total 4096\n","Correct 1434\n","Epoch: 20, Batch: 0 , Training..\n"]}],"source":["# final train\n","# Initialize the WandB sweep\n","# sweep_id = wandb.sweep(sweep_config, project='CS6910_Assignment3')\n","# print(sweep_id, type(sweep_id))\n","wandb.agent('72j6yzep', function=train,count=10, project='CS6910_Assignment3')\n","# wandb.agent(sweep_id, function=train,count=40)\n","# wandb.agent(sweep_id, function=train,count=20)\n","\n","\n","\n","#wandb.agent(sweep_id, function=train,count=10)\n","#wandb.agent(sweep_id, function=train,count=10)\n","#wandb.agent(sweep_id, function=train,count=10)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Define hyperparameters\n","INPUT_DIM = 29\n","OUTPUT_DIM = 67\n","embedding_size=512\n","HIDDEN_DIM = 512\n","NUM_LAYERS = 1\n","CELL_TYPE = 'gru'\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.0002\n","TEACHER_FORCING_RATIO = 0.7\n","EPOCHS = 25\n","\n","dropout=0.1\n","bidirectional=False\n","opt='adam'\n","\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","train_loader,test_loader,val_loader,idx_to_char=load_data(BATCH_SIZE)\n","#print(len(test_loader))\n","#print(len(train_loader))\n","#print(len(val_loader))\n","# Instantiate the Encoder and Decoder models\n","encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n","decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n","\n","# Instantiate the Seq2Seq model with the Encoder and Decoder models\n","model = Seq2Seq(encoder,decoder,CELL_TYPE,bidirectional).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","#optimizer=optimizer(model,opt,LEARNING_RATE)\n","\n","\n","\n","\n","# Train the model\n","for epoch in range(EPOCHS):\n","    epoch_loss = 0\n","    model.train()\n","\n","    for batch_idx, (src, trg, src_len, trg_len,trg_one_hot) in enumerate(train_loader):\n","        #print(batch_idx)\n","        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n","        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n","\n","        src = src.to(device)\n","        trg = trg.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg, TEACHER_FORCING_RATIO)\n","        \n","        # Ignore the first element of the output, which is initialized as all zeros\n","        # since we use it to store the output for the start-of-sequence token\n","        #print(output.shape[2])\n","        \n","        output = output[1:].reshape(-1, output.shape[2])\n","        #print(output.shape)\n","        #print(trg.shape)\n","        trg = trg[1:].reshape(-1)\n","        \n","        loss = criterion(output, trg)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += (loss.item())\n","        \n","        if batch_idx % 1000 == 0:\n","            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Training...\")\n","\n","    # Calculate word-level accuracy after every epoch\n","    #train_acc,train_loss= calculate_word_level_accuracy(model,idx_to_char,train_loader,criterion)\n","    val_acc,val_loss = calculate_word_level_accuracy(model,idx_to_char,val_loader,criterion)\n","    #test_acc,test_loss = calculate_word_level_accuracy(model,idx_to_char,test_loader,criterion)\n","    \n","    print(f\"Epoch: {epoch}, Loss: {epoch_loss / (len(train_loader))}, Val Acc: {val_acc}, Val loss: {val_loss}\")\n","    #wandb.log({'epoch': epoch, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n","    \n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
