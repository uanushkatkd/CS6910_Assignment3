{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5VE0rpwFn-7",
        "outputId": "80b0c4dd-5288-47ae-d7f3-18e2f051821d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-20 19:10:07--  https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.139, 108.177.126.100, 108.177.126.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vb5phg463eeqiiu6puhsnoat0v7omhgm/1684609800000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=71791eab-2428-4a13-ad4c-99ac6ce8fb36 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-20 19:10:14--  https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vb5phg463eeqiiu6puhsnoat0v7omhgm/1684609800000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=71791eab-2428-4a13-ad4c-99ac6ce8fb36\n",
            "Resolving doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)... 142.251.31.132, 2a00:1450:4013:c1a::84\n",
            "Connecting to doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)|142.251.31.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14030699 (13M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  13.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-20 19:10:14 (132 MB/s) - ‘dataset.zip’ saved [14030699/14030699]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw' -O dataset.zip && unzip -q dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys8_6uZsFwpc",
        "outputId": "5dd618fc-5702-4a20-e4a4-e717e1f4a5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "2.0.1+cu118\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "# Print the name of the CUDA device, if available\n",
        "print(torch.device('cuda:0'))\n",
        "# Print the version of the torch library\n",
        "print(torch.__version__)\n",
        "\n",
        "# Create a variable to store the device to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Print the device that will be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KPLdU191FxaW"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import (\n",
        "    DataLoader, random_split\n",
        ")  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sbHpQ-qgFzIa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "  This function sets the random seed for all major Python libraries.\n",
        "\n",
        "  Args:\n",
        "    seed (int): The random seed to use.\n",
        "\n",
        "  \"\"\"\n",
        "def seed_everything(seed=1):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WYH8LazuF1kK"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "'''\n",
        "Class Vovabulary is used to create vocabulary from the training dataset.\n",
        "'''\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      file_path (string): The path to the CSV file containing the training data.\n",
        "      src_lang (string): The name of the source language.\n",
        "      trg_lang (string): The name of the target language.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the file_path does not exist.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, src_lang, trg_lang):\n",
        "        # Read the CSV file into a Pandas DataFrame.\n",
        "        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n",
        "        # It will drop any rows with missing values\n",
        "        self.translations.dropna()\n",
        "        self.src_lang = src_lang\n",
        "        self.trg_lang = trg_lang\n",
        "        # Create a dictionary that maps each character in the source language to an integer index.\n",
        "        self.trg_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n",
        "        # Create a dictionary that maps each character in the target language to an integer index.\n",
        "        self.src_vocab = {char: i+3 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n",
        "        \n",
        "        # Add special tokens to the vocabularies.\n",
        "        self.trg_vocab['<'] = 0\n",
        "        self.src_vocab['<'] = 0\n",
        "\n",
        "        self.trg_vocab['<unk>'] = 2\n",
        "        self.src_vocab['<pad>'] = 1\n",
        "        self.trg_vocab['<pad>'] = 1\n",
        "        \n",
        "        self.src_vocab['<unk>'] = 2\n",
        "        \n",
        "        # Extract the unique characters in the source and target languages\n",
        "        src_chars = sorted(set(''.join(self.translations[src_lang])))\n",
        "        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n",
        "\n",
        "        # Assign an index to each character in the source and target languages\n",
        "        self.t_char_to_idx = {char: idx+3 for idx, char in enumerate(trg_chars)}\n",
        "        self.t_char_to_idx['<unk>']=2\n",
        "        self.t_idx_to_char = {idx: char for char, idx in self.t_char_to_idx.items()}\n",
        "        \n",
        "        self.s_char_to_idx = {char: idx+3 for idx, char in enumerate(src_chars)}\n",
        "        self.s_char_to_idx['<unk>']=2\n",
        "        self.s_idx_to_char = {idx: char for char, idx in self.s_char_to_idx.items()}\n",
        "        \n",
        "      \n",
        "\n",
        "\n",
        "    def get(self):\n",
        "         # This function returns the source and target vocabularies, as well as the dictionaries that map characters to integer indexes and vice versa.\n",
        "        return self.src_vocab,self.trg_vocab,self.t_char_to_idx,self.t_idx_to_char,self.s_char_to_idx,self.s_idx_to_char\n",
        "        \n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      file_path (string): The path to the CSV file containing the training data.\n",
        "      src_lang (string): The name of the source language.\n",
        "      trg_lang (string): The name of the target language.\n",
        "      src_vocab (Vocabulary): The vocabulary for the source language.\n",
        "      trg_vocab (Vocabulary): The vocabulary for the target language.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If the file_path does not exist.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, src_lang, trg_lang,src_vocab,trg_vocab,t_char_to_idx):\n",
        "        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n",
        "        self.translations.dropna()\n",
        "    \n",
        "        self.src_lang = src_lang\n",
        "        self.t_char_to_idx = t_char_to_idx\n",
        "        self.trg_lang = trg_lang\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])+1\n",
        "        #print(\"max src len\",self.max_src_len)\n",
        "        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])+1\n",
        "        #print(\"max trg len\",self.max_trg_len)\n",
        "    def __len__(self):\n",
        "        return len(self.translations)\n",
        "\n",
        "    def target_to_one_hot(self, target_word, char_to_idx):\n",
        "        num_trg_chars = len(char_to_idx)\n",
        "        max_target_len = self.max_trg_len\n",
        "        # Create a tensor of zeros for the one-hot encoding\n",
        "        one_hot = torch.zeros((max_target_len, num_trg_chars))\n",
        "        # Encode each character in the target word as a one-hot vector\n",
        "        for i, char in enumerate(target_word):\n",
        "            #print(i,char)\n",
        "            char_idx = char_to_idx[char if char in  char_to_idx else '<unk>']\n",
        "            #print(char_idx)\n",
        "            one_hot[i][char_idx] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_word = self.translations.iloc[idx][self.src_lang]\n",
        "        trg_word = self.translations.iloc[idx][self.trg_lang]\n",
        "        #print(src_word)\n",
        "        # Initialize the start-of-word token\n",
        "        sow=0\n",
        "        \n",
        "        # Convert source and target words to lists of vocabulary indices\n",
        "        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n",
        "        trg = [self.trg_vocab.get(char, self.src_vocab['<unk>']) for char in trg_word]\n",
        "        # Insert the start-of-word token at the beginning\n",
        "        src.insert(0, sow)\n",
        "        trg.insert(0, sow)\n",
        "\n",
        "        src_len = len(src)\n",
        "        trg_len = len(trg)\n",
        "\n",
        "        # Pad the source and target sequences with the <pad> token\n",
        "        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n",
        "        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n",
        "\n",
        "        # Extend the source and target sequences with padding\n",
        "        src.extend(src_pad)\n",
        "        trg.extend(trg_pad)\n",
        "\n",
        "        # Convert source and target sequences to tensors\n",
        "        src = torch.LongTensor(src)\n",
        "        trg = torch.LongTensor(trg)\n",
        "        #trg_one_hot = self.target_to_one_hot(trg_word, self.trg_vocab)\n",
        "        #src_one_hot = self.target_to_one_hot(src_word, self.src_vocab)\n",
        "\n",
        "        # This will return encoded source word ,target word and their length\n",
        "        return src, trg, src_len, trg_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W50h47NjF3tF"
      },
      "outputs": [],
      "source": [
        "def load_data(bs):\n",
        "    '''\n",
        "    This function loads data into batches provided the batch size as an argument.\n",
        "    '''\n",
        "    # Define the paths for the train, validation, and test CSV files\n",
        "    train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "    val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "    test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "    # Create a vocabulary object and retrieve the source and target vocabularies,\n",
        "    # character-to-index and index-to-character mappings\n",
        "    vocab = Vocabulary(train_path, 'src', 'trg')\n",
        "    src_vocab,trg_vocab,t_char_to_idx,t_idx_to_char,s_char_to_idx,s_idx_to_char=vocab.get()\n",
        "    #print(len(src_vocab))\n",
        "    #print(len(trg_vocab))\n",
        "    #print(\"char to idc outside\",char_to_idx)\n",
        "\n",
        "\n",
        "    # Create train, validation, and test datasets using TransliterationDataset\n",
        "    # with the appropriate source and target vocabularies and mappings\n",
        "    train_dataset = TransliterationDataset(train_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n",
        "    val_dataset = TransliterationDataset(val_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n",
        "    test_dataset = TransliterationDataset(test_path, 'src', 'trg',src_vocab,trg_vocab,t_char_to_idx)\n",
        "    \n",
        "    # Create train, validation, and test data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
        "    \n",
        "    return train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char\n",
        "\n",
        "\n",
        "    #Training and check accuracy function\n",
        " \n",
        "  \n",
        "#train_loader,test_loader,val_loader,idx_to_char=load_data(32)\n",
        "#print(idx_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VpRLyDoRGCi6"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Store the input dimensions, embedding size, hidden size, number of layers,\n",
        "        # bidirectional flag, cell type, and dropout probability\n",
        "        self.input_dim = input_dim\n",
        "        self.embedded_size=embedded_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "\n",
        "        # Determine the directionality of the encoder (1 for unidirectional, 2 for bidirectional)\n",
        "        if bidirectional:\n",
        "            self.dir=2\n",
        "        else:\n",
        "            self.dir=1  \n",
        "        # Create an embedding layer\n",
        "        self.embedding = nn.Embedding(input_dim,embedded_size)\n",
        "\n",
        "        # Create the recurrent layer based on the specified cell type\n",
        "        if cell_type == 'rnn':\n",
        "              self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'lstm':\n",
        "              self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "              self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")     \n",
        "\n",
        "    def forward(self, src):\n",
        "        # Apply dropout to the embedded input\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # If the cell type is LSTM, return both the output and the hidden and cell states in a single tuple\n",
        "        if self.cell_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "            return output, (hidden, cell)\n",
        "\n",
        "        else:\n",
        "            # For other cell types (RNN, GRU), return the output and the hidden state\n",
        "            output, hidden = self.rnn(embedded)\n",
        "            return output,hidden\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Store the input dimensions, embedding size, hidden size, number of layers,\n",
        "        # bidirectional flag, cell type, and dropout probabilit  \n",
        "        self.output_dim = output_dim\n",
        "        self.embedded_size=embedded_size     \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        if bidirectional:\n",
        "            self.dir=2\n",
        "        else:\n",
        "            self.dir=1  \n",
        "\n",
        "        # Create an embedding layer\n",
        "        self.embedding = nn.Embedding(output_dim,embedded_size)\n",
        "        # Create the recurrent layer based on the specified cell type\n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "        # Create the output fully connected layer\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # Pass the embedded input and hidden state through the decoder RNN\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        # Pass the decoder output through the fully connected layer\n",
        "        output = self.fc_out(output)\n",
        "        # Apply log softmax activation to the output\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder,cell_type,bidirectional):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type=cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        #print(batch_size)\n",
        "        max_len = trg.shape[0]\n",
        "        #print(max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(src)\n",
        "        #print(\"encoder hidden shape\",encoder_hidden.shape)\n",
        "        # Concatenate the last hidden state of the encoder from both directions\n",
        "        if self.bidirectional:\n",
        "            if self.cell_type=='lstm':\n",
        "                hidden_concat = torch.add(encoder_hidden[0][0:self.encoder.num_layers,:,:], encoder_hidden[1][0:self.encoder.num_layers,:,:])/2\n",
        "                cell_concat = torch.add(encoder_hidden[0][self.encoder.num_layers:,:,:], encoder_hidden[1][self.encoder.num_layers:,:,:])/2\n",
        "                hidden_concat = (hidden_concat, cell_concat)\n",
        "\n",
        "            else:\n",
        "                hidden_concat = torch.add(encoder_hidden[0:self.encoder.num_layers,:,:], encoder_hidden[self.encoder.num_layers:,:,:])/2\n",
        "        else:\n",
        "            hidden_concat= encoder_hidden\n",
        "        \n",
        "        decoder_hidden = hidden_concat\n",
        "        # Initialize decoder input with the start token\n",
        "        decoder_input = (trg[0,:]).unsqueeze(0)\n",
        "        #print(\"decoder input shape\",decoder_input.shape)\n",
        "        \n",
        "        for t in range(1,trg.shape[0] ):\n",
        "\n",
        "            # Pass the decoder input and hidden state through the decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            \n",
        "            # Store the decoder output in the outputs tensor\n",
        "            outputs[t] = decoder_output\n",
        "            # Determine the next decoder input using teacher forcing or predicted output\n",
        "            max_pr, idx=torch.max(decoder_output,dim=2)\n",
        "            #print(\"trg shape\",trg.shape)\n",
        "            idx=idx.view(trg.shape[1])\n",
        "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
        "            if teacher_force:\n",
        "                decoder_input= trg[t,:].unsqueeze(0)\n",
        "            else:\n",
        "                decoder_input= idx.unsqueeze(0)\n",
        "\n",
        "         # Pass the last decoder input and hidden state through the decoder\n",
        "        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "        \n",
        "        # Store the final decoder output in the outputs tensor\n",
        "        #outputs[-1] = decoder_output\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S4cPKpxWGALj"
      },
      "outputs": [],
      "source": [
        "def indices_to_string(trg, t_idx_to_char):\n",
        "    \"\"\"Converts a batch of indices to strings using the given index-to-char mapping\n",
        "    Args:\n",
        "    trg(Tensor):encoder words of size batch_size x sequence length\n",
        "    t_idx_to_char(Dict.): index to char mapping\n",
        "    \n",
        "    \"\"\"\n",
        "    strings = []\n",
        "    bs=trg.shape[0]\n",
        "    sq=trg.shape[1]\n",
        "    for i in range(bs):\n",
        "      chars = []\n",
        "      #print(i)\n",
        "      # Convert the sequence of indices to a sequence of characters using the index-to-char mapping\n",
        "      for j in range(sq):\n",
        "        #print(j)\n",
        "        #print(trg[i,j].item())            \n",
        "        if trg[i,j].item() in t_idx_to_char:\n",
        "          #print(trg[i,j])\n",
        "          char = t_idx_to_char[trg[i,j].item()]\n",
        "          chars.append(char)\n",
        "            #print(chars)\n",
        "      # Join the characters into a string\n",
        "      string = ''.join(chars)\n",
        "      #print(string)\n",
        "        # Append the string to the list of strings\n",
        "      strings.append(string)\n",
        "    return strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "24EGz_7-GK74"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_word_level_accuracy(model,t_idx_to_char,data_loader, criterion):\n",
        "  '''\n",
        "    This function will calculate word level accuracy after each epoch.\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        t_idx_to_char: Mapping from target indices to characters\n",
        "        data_loader: Data loader for the validation/test dataset\n",
        "        criterion: Loss criterion used for training the model\n",
        "\n",
        "\n",
        "    '''\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n",
        "            # Convert target indices to string for comparison\n",
        "            string_trg=indices_to_string(trg,t_idx_to_char)\n",
        "            # Move tensors to the device\n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            # Perform forward pass through the model\n",
        "            output = model(src, trg, 0)\n",
        "            # turn off teacher forcing\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n",
        "\n",
        "            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n",
        "            #print(\"trg after reshape\",trg.shape)\n",
        "            \n",
        "            # Calculate the loss\n",
        "            output = output.to(device)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            batch_size = trg_len.shape[0]\n",
        "            #print(\"bs\", batch_size)\n",
        "            seq_length = int(trg.numel() / batch_size)\n",
        "            \n",
        "\n",
        "            # Convert the output to predicted characters\n",
        "            predicted_indices = torch.argmax(output, dim=1)\n",
        "            predicted_indices = predicted_indices.reshape(seq_length,-1)\n",
        "            predicted_indices = predicted_indices.permute(1, 0)\n",
        "            # Convert predicted indices to strings\n",
        "            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n",
        "            #print(string_pred)\n",
        "            #print(string_trg)\n",
        "            \n",
        "            for i in range(batch_size):\n",
        "                num_total+=1\n",
        "                # Compare the predicted string with the target string\n",
        "                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n",
        "                    num_correct+=1\n",
        "\n",
        "    print(\"Total\",num_total)\n",
        "    print(\"Correct\",num_correct)\n",
        "    # Calculate word-level accuracy and average loss\n",
        "    return (num_correct /num_total) * 100, (epoch_loss/(len(data_loader)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7TTpm6ZXhyiw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,data_loader, criterion):\n",
        "  '''\n",
        "    This function is just extension of above function and\n",
        "    will calculate word level accuracy as well as store the correct and \n",
        "    incorrect words into list .\n",
        "    We'll call this function after training only once for test data.\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        t_idx_to_char: Mapping from target indices to characters\n",
        "        s_idx_to_char: Mapping from source indices to characters\n",
        "        data_loader: Data loader for the validation/test dataset\n",
        "        criterion: Loss criterion used for training the model\n",
        "\n",
        "\n",
        "    '''\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    epoch_loss = 0\n",
        "    c_trg=[]\n",
        "    c_src=[]\n",
        "    c_pred=[]\n",
        "    \n",
        "    i_trg=[]\n",
        "    i_src=[]\n",
        "    i_pred=[]\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, trg, src_len, trg_len) in enumerate(data_loader):\n",
        "            # Convert target indices to string for comparison\n",
        "            string_trg=indices_to_string(trg,t_idx_to_char)\n",
        "            string_src=indices_to_string(src,s_idx_to_char)\n",
        "            \n",
        "            # Move tensors to the device\n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            # Perform forward pass through the model\n",
        "            output = model(src, trg, 0)\n",
        "            # turn off teacher forcing\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            #print(\"op after \",output.shape) # exclude the start-of-sequence token\n",
        "\n",
        "            trg = trg[1:].reshape(-1) # exclude the start-of-sequence token\n",
        "            #print(\"trg after reshape\",trg.shape)\n",
        "            \n",
        "            # Calculate the loss\n",
        "            output = output.to(device)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            batch_size = trg_len.shape[0]\n",
        "            #print(\"bs\", batch_size)\n",
        "            seq_length = int(trg.numel() / batch_size)\n",
        "            \n",
        "\n",
        "            # Convert the output to predicted characters\n",
        "            predicted_indices = torch.argmax(output, dim=1)\n",
        "            predicted_indices = predicted_indices.reshape(seq_length,-1)\n",
        "            predicted_indices = predicted_indices.permute(1, 0)\n",
        "            # Convert predicted indices to strings\n",
        "            string_pred=indices_to_string(predicted_indices,t_idx_to_char)\n",
        "            #print(string_pred)\n",
        "            #print(string_trg)\n",
        "            \n",
        "            for i in range(batch_size):\n",
        "                num_total+=1\n",
        "                # Compare the predicted string with the target string\n",
        "                if string_pred[i][:len(string_trg[i])] == string_trg[i]:\n",
        "                  c_trg.append(string_trg[i])\n",
        "                  c_src.append(string_src[i])\n",
        "                  c_pred.append(string_pred[i][:len(string_trg[i])])\n",
        "                  num_correct+=1\n",
        "                else :\n",
        "                  i_trg.append(string_trg[i])\n",
        "                  i_src.append(string_src[i])\n",
        "                  i_pred.append(string_pred[i][:len(string_trg[i])])\n",
        "                  \n",
        "\n",
        "\n",
        "    print(\"Total\",num_total)\n",
        "    print(\"Correct\",num_correct)\n",
        "    # Calculate word-level accuracy and average loss\n",
        "    return (num_correct /num_total) * 100, (epoch_loss/(len(data_loader))),c_trg,c_src,c_pred,i_trg,i_src,i_pred\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ttOezGyGOgF",
        "outputId": "09c85988-7e54-405f-8484-4d8c7292808a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 69\n",
            "Epoch: 0, Loss: 1.285013024955988, Val Acc: 1.6845703125, Val loss: 0.8804972320795059\n",
            "Epoch: 1, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 680\n",
            "Epoch: 1, Loss: 0.6658895851671696, Val Acc: 16.6015625, Val loss: 0.5949776116758585\n",
            "Epoch: 2, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 990\n",
            "Epoch: 2, Loss: 0.4388636986911297, Val Acc: 24.169921875, Val loss: 0.5308835469186306\n",
            "Epoch: 3, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1211\n",
            "Epoch: 3, Loss: 0.35614405244588854, Val Acc: 29.5654296875, Val loss: 0.49517351295799017\n",
            "Epoch: 4, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1385\n",
            "Epoch: 4, Loss: 0.30208779361099003, Val Acc: 33.8134765625, Val loss: 0.4779027244076133\n",
            "Epoch: 5, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1427\n",
            "Epoch: 5, Loss: 0.2739394252002239, Val Acc: 34.8388671875, Val loss: 0.4605716001242399\n",
            "Epoch: 6, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1467\n",
            "Epoch: 6, Loss: 0.24255923368036747, Val Acc: 35.8154296875, Val loss: 0.4649025481194258\n",
            "Epoch: 7, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1548\n",
            "Epoch: 7, Loss: 0.22105610702186823, Val Acc: 37.79296875, Val loss: 0.4573290664702654\n",
            "Epoch: 8, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1586\n",
            "Epoch: 8, Loss: 0.2083393647149205, Val Acc: 38.720703125, Val loss: 0.45478605944663286\n",
            "Epoch: 9, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1575\n",
            "Epoch: 9, Loss: 0.19024122951552272, Val Acc: 38.4521484375, Val loss: 0.45167828537523746\n",
            "Epoch: 10, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1603\n",
            "Epoch: 10, Loss: 0.17083789967000484, Val Acc: 39.1357421875, Val loss: 0.4642919274047017\n",
            "Epoch: 11, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1666\n",
            "Epoch: 11, Loss: 0.16095056412741543, Val Acc: 40.673828125, Val loss: 0.44806504994630814\n",
            "Epoch: 12, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1646\n",
            "Epoch: 12, Loss: 0.14946026900783183, Val Acc: 40.185546875, Val loss: 0.4764967439696193\n",
            "Epoch: 13, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1722\n",
            "Epoch: 13, Loss: 0.1313504133746028, Val Acc: 42.041015625, Val loss: 0.4722130550071597\n",
            "Epoch: 14, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1699\n",
            "Epoch: 14, Loss: 0.12328857151791453, Val Acc: 41.4794921875, Val loss: 0.4780407026410103\n",
            "Epoch: 15, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1711\n",
            "Epoch: 15, Loss: 0.11322727736085653, Val Acc: 41.7724609375, Val loss: 0.490302755497396\n",
            "Epoch: 16, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1731\n",
            "Epoch: 16, Loss: 0.10464715220034122, Val Acc: 42.2607421875, Val loss: 0.49725814815610647\n",
            "Epoch: 17, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1723\n",
            "Epoch: 17, Loss: 0.09368541636504232, Val Acc: 42.0654296875, Val loss: 0.5113338772207499\n",
            "Epoch: 18, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1705\n",
            "Epoch: 18, Loss: 0.08630286603234709, Val Acc: 41.6259765625, Val loss: 0.5246214419603348\n",
            "Epoch: 19, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1751\n",
            "Epoch: 19, Loss: 0.07931486803106963, Val Acc: 42.7490234375, Val loss: 0.5495225982740521\n",
            "Epoch: 20, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1723\n",
            "Epoch: 20, Loss: 0.07093616159632803, Val Acc: 42.0654296875, Val loss: 0.569560986943543\n",
            "Epoch: 21, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1724\n",
            "Epoch: 21, Loss: 0.06593564319424332, Val Acc: 42.08984375, Val loss: 0.5753533253446221\n",
            "Epoch: 22, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1682\n",
            "Epoch: 22, Loss: 0.059780420558527114, Val Acc: 41.064453125, Val loss: 0.6026149168610573\n",
            "Epoch: 23, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1704\n",
            "Epoch: 23, Loss: 0.0548845934914425, Val Acc: 41.6015625, Val loss: 0.5836737975478172\n",
            "Epoch: 24, Batch: 0, Training...\n",
            "Total 4096\n",
            "Correct 1672\n",
            "Epoch: 24, Loss: 0.04898299029562622, Val Acc: 40.8203125, Val loss: 0.6370808435603976\n",
            "Best model saved to best_model_vanillaSeq2Seq.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define hyperparameters\n",
        "INPUT_DIM = 29\n",
        "OUTPUT_DIM = 67\n",
        "embedding_size=512\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 3\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.0002\n",
        "TEACHER_FORCING_RATIO = 0.7\n",
        "EPOCHS = 25\n",
        "\n",
        "dropout=0.1\n",
        "bidirectional=True\n",
        "opt='adam'\n",
        "\n",
        "\n",
        "# Load data and create data loaders\n",
        "train_loader,test_loader,val_loader,t_idx_to_char,s_idx_to_char=load_data(BATCH_SIZE)\n",
        "#print(len(test_loader))\n",
        "#print(len(train_loader))\n",
        "#print(len(val_loader))\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder,CELL_TYPE,bidirectional).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "#optimizer=optimizer(model,opt,LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += (loss.item())\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Training...\")\n",
        "\n",
        "    # Calculate word-level accuracy after every epoch\n",
        "    val_acc,val_loss = calculate_word_level_accuracy(model,t_idx_to_char,val_loader,criterion)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, Loss: {epoch_loss / (len(train_loader))}, Val Acc: {val_acc}, Val loss: {val_loss}\")\n",
        "    #wandb.log({'epoch': epoch, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n",
        "    \n",
        "   \n",
        "# Save best model\n",
        "best_model_path = 'best_model_vanillaSeq2Seq.pth'\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc8CgAudsUZw",
        "outputId": "d599ea80-7d7e-444f-a386-3e2070f10d4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total 4096\n",
            "Correct 1563\n"
          ]
        }
      ],
      "source": [
        "val_acc,val_loss,c_trg,c_src,c_pred,i_trg,i_src,i_pred = calculate_word_level_accuracy1(model,t_idx_to_char,s_idx_to_char,test_loader,criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbOnZb9bsYEn",
        "outputId": "38944665-44d4-46c4-feb3-9d4e503ea682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['थरमैक्स', 'सिखाएगा', 'लर्न', 'ट्विटर्स', 'तिरुनेलवेली', 'इंडिपेंडेंस', 'स्पेशियों', 'कोल्हापुर', 'अंक', 'ग्लेंडल', 'अम्बिकापुर', 'उखरुल', 'इक़बाल', 'दयालपुरा', 'सोहराई', 'साहित्योत्सव', 'शिकायतकर्ता', 'अंदरखाने', 'लीडरों', 'गलगंड', 'मुर्गीपालन', 'मुशाहिद', 'होल्ट', 'इजाजत', 'वनक्षेत्र', 'भूतल', 'स्वादप्रेमियों', 'फ्रेक्टर', 'नब्ज़', 'बौनी', 'सीमाई', 'दर्शनार्थी', 'रिवास', 'तर्कवाद', 'अनुसारका', 'कोचेला', 'परिषद', 'ग्रंथियों', 'बुएना', 'रिकैलीब्रेशन', 'शासनाधिकारियों', 'फिजूलखर्ची', 'कैटलिन', 'वंडर', 'श्विंग', 'पश्चमी', 'नीलाभ', 'उपनेता', 'मास', 'वेस्टिंग', 'मुकेश्वरी', 'श्रीमति', 'विवेकाधिकारों', 'कबहा', 'भ्रांतियों', 'जीवराज', 'विष्णुपुरा', 'घोटालेबाजों', 'सिंगार', 'बालकृष्ण', 'फब्ती', 'पलटने', 'नगाड़ा', 'क्लेन', 'कार्यभार', 'कोविना', 'बार्कर', 'कैटलिन', 'फूस', 'नेको', 'कौशल', 'दिलाएगा', 'फरसे', 'आंत्रित', 'मुर्गीपालन', 'लुइस', 'साइंस', 'सरकारें', 'गुरुपदो', 'संयुक्त', 'सभाएं', 'तनी', 'विनीत', 'किकियाना', 'मैक्सवेल', 'हिप्पो', 'दीपन', 'कर्माचारी', 'दुर्गापुर', 'त्रिभाषा', 'सूदखोरों', 'प्रस्तावों', 'वेलवेट', 'बैक्सटर', 'उठनी', 'इकट्ठा', 'भूपसिंह', 'बोसान', 'कार्की', 'मनोकामनाओं', 'परिचर', 'क्रेन', 'उल्हासनगर', 'प्रभानमंत्री', 'शिवालापुरवा', 'फर्थ', 'प्रतिरोधात्मक', 'स्वर्णिका', 'व्यवस्थापना', 'श्रीमहापूर्ण', 'चढ़ने', 'क्रिकेटिंग', 'रिडक्शन', 'चटकाते', 'कुबुद्दीन', 'आलोचकों', 'संचालनों', 'शासनाधिकारियों', 'महाधिपतियों', 'संख्याएं', 'हरपाल', 'ग्रोवर', 'एंजेलो', 'पोकर', 'अपवर्तित', 'प्रतिष्ठावाले', 'एमिल', 'शैली', 'छल्ला', 'लक्ष्यों', 'देवी', 'भूषण', 'रुकते', 'आमदनी', 'जूनागढ़', 'ओढ़कर', 'ब्रोक', 'तलाशा', 'परिवारवाले', 'कुश', 'चढ़ें', 'सचन', 'कैंटन', 'आत्ममुग्ध', 'घोंटना', 'मर्यादाओं', 'रतलाम', 'अकोला', 'नोटबंदीशुदा', 'फोड़ा', 'नूबिया', 'प्रतिपालपुर', 'नमकीन', 'मंजिलें', 'जुराब', 'ब्लैक', 'जोहरी', 'तेवरी', 'वेल्डर', 'भरष्टाचारियों', 'खातेदार', 'आजमाना', 'बहावलपुर', 'चौसिंगा', 'टीकों', 'अथक', 'माप', 'गजरा', 'सहूलियत', 'जीतनेवालों', 'डोडा', 'पेंडुलम', 'चिपकने', 'गोंड', 'चुंगियों', 'एरिका', 'युद्धक', 'वेगास', 'चित्रांशी', 'धर्मपूर्वी', 'अंबानी', 'इबीजा', 'झुठलाने', 'लिंडन', 'पुलमा', 'बरेली', 'जिंग', 'जीवन', 'पहनी', 'तनु', 'दीपिका', 'मौलवियों', 'लक्मे', 'त्रिया', 'खेलमंत्री', 'भविष्य', 'बेंडर', 'प्रदेश', 'संवेदीकरण', 'लगुना', 'रेस', 'प्रबोधिनी', 'मंगला', 'उदयगढ़ी', 'अकापुल्को', 'वैधृति', 'यूक्लिड', 'रक्तदाताओं', 'बरामदे', 'कुत्ता', 'किशुन', 'अमरोही', 'वर्षिय', 'शावकों', 'स्वार्थपरक', 'वेरोनिका', 'वेब', 'अग्र', 'वेस्टवर्क', 'पास्को', 'जाधव', 'संशयों', 'उपस्थियों', 'बैला', 'लिवाल', 'हिल्सबोरो', 'शाकिर', 'सलाहकार', 'विश्वकर्मा', 'रिश्वतखोर', 'प्राविंसेस', 'दफ्तरवाले', 'फिजिशियन', 'सिपाह', 'भरमा', 'प्रतिज्ञा', 'मेघवाल', 'नंदोई', 'फार्मूले', 'दुर्घटना', 'स्वतंत्रसिंह', 'बाल्टी', 'कोलंबिया', 'दुक्के', 'चमकना', 'देहरा', 'हस्ताक्षरों', 'तिरुपति', 'कुर्सियों', 'पहनो', 'बंगश', 'गुर्जर', 'समन्वयक', 'मध्य', 'ब्रिग्स', 'हाओकिप', 'कदमो', 'कुरमी', 'सिमटता', 'अगवाई', 'नागराज', 'रुकवाकर', 'ब्रह्मवर्ता', 'ग्रे', 'पुरूषोत्तमपुर', 'तेजी', 'मूलों', 'हमीरपुर', 'पोखरियाल', 'दबंगता', 'धुरिया', 'गाज़ी', 'इजाजत', 'पुतली', 'संचारी', 'नरनारी', 'धनिये', 'गम्मत', 'लफ्फाजी', 'माथलू', 'टेस्टिंग', 'बेसिल', 'जेरेमी', 'व्यापक', 'मैडी', 'सभासदों', 'दमाए', 'मिडवेस्ट', 'विमर्शों', 'सरसा', 'हेस्टर', 'उत्तर', 'कसेरा', 'दायित्वों', 'प्रेममूलक', 'मोरन', 'नेल्सन', 'अमरपाल', 'फटने', 'टिंका', 'गायिकाओ', 'मॉड', 'अर्थों', 'नीलाभ', 'भड़काया', 'नामधारी', 'अल्बा', 'खाताधारक', 'टेक्सस', 'स्पेशल', 'हरवंश', 'पीड़ियों', 'दिलाएगा', 'परवा', 'प्रदेशिका', 'विसंगतिया', 'घोस्ट', 'पिपली', 'हेल्पर', 'बख्शने', 'इन्द्रियोंके', 'बरहद', 'रिसाली', 'विश्वरक्त', 'अरुणाई', 'भौतिकवादियों', 'आवाज़ें', 'मूल', 'रोटेशन', 'बरपाया', 'लैंड', 'सूची', 'नून', 'आरपी', 'पालनपुर', 'इलेक्ट्रोस्टील', 'ऑस्टिन', 'टीसीएनएस', 'मूड', 'दौलताबाद', 'प्रतिकारा', 'बेचकर', 'स्टोन', 'बापा', 'सफरदगंज', 'रूपक', 'खंडेलवाल', 'जेहे', 'अगरतला', 'जमाया', 'परदेसी', 'आगाशे', 'अच्छी', 'स्वभावतः', 'भीरा', 'सिल्वा', 'श्रीचक्र', 'निचला', 'बुरहानपुर', 'सहूलियत', 'लब्ध', 'पूल', 'स्ट्रेशन', 'अजित', 'झ', 'प्रशासनिक', 'गिरिडीह', 'रीडिंग', 'रुकते', 'अनोंदिता', 'फोंटाना', 'धम्मा', 'पनामा', 'पुतले', 'आसापास', 'मुफलिसी', 'संसथान', 'दर्शनार्थी', 'लोकोत्सव', 'ग्राहकोंने', 'रेजा', 'मिटाओ', 'ओढ़कर', 'मेगवाल', 'सुगमता', 'मुनाफ', 'महावर', 'विज्ञान', 'विलास', 'सुपाच्य', 'विष्णु', 'चौधरी', 'शिपिंग', 'जानम', 'दमाए', 'चीखना', 'मुकेश्वरी', 'उम्मीदे', 'कहावह', 'कांतिपूर्ण', 'चरस', 'भदौरिया', 'विज्ञानों', 'गंझू', 'आस्तिक', 'निवेशकर्ता', 'संस्कृतियां', 'रौबदार', 'छरहरी', 'ज्वारों', 'संस्थिता', 'रिग्स', 'चुने', 'लिपाई', 'धंधेखोरी', 'उभरेगा', 'एंटोन', 'निहलानी', 'टिवोली', 'होगिस', 'विंटर', 'बदलवाकर', 'घुमाई', 'बख्शते', 'संवेदीकरण', 'फरहीन', 'रेडिंग', 'किसानों', 'चलेंगे', 'नन्हें', 'भदौरिया', 'पहुंचनी', 'ब्लॉसम', 'यात्राएं', 'नियुत्तियों', 'मुर्गीपालन', 'मावे', 'विक्रम', 'एसपीएफ', 'सबजूनियर', 'प्रधान', 'अभियुक्तोंके', 'हिजबुल', 'विद्याओं', 'छुपकर', 'डोरंडा', 'सिहुंता', 'हेमवती', 'कोषाधिकारी', 'आशंका', 'बीन', 'वैज्ञानिकता', 'झीलों', 'श्रीरामपुर', 'वसूलता', 'थकी', 'टरबाइन', 'दिमाक', 'जोधपुर', 'ब्रह्मसिद्धि', 'रीता', 'चर्चामंच', 'प्रतिकारा', 'तर्कवाद', 'दबंगों', 'हिचकिचा', 'शीर्षकों', 'डम', 'बहुमान', 'किंगखान', 'तरस', 'विरोध', 'अरेना', 'बोका', 'टंकार', 'खटकती', 'पुष्प', 'जीएम', 'कोरोला', 'प्यादे', 'बलात्कारियों', 'श्रेष्ठस्वरूप', 'शुभप्रभा', 'राष्ट्रभक्तों', 'भूतल', 'इंसानो', 'एमिल', 'विज्ञानम', 'अटवाल', 'अलीम', 'एचआर', 'इमानदारी', 'गंगाजल', 'गौर', 'बगड़', 'निवेशकर्ता', 'पटकथाएं', 'भादस', 'भोगते', 'होल्डिंग', 'धोरों', 'प्रतिज्ञा', 'बढ़ाती', 'थेन', 'सूजा', 'बहुंत', 'अमानत', 'झुठला', 'बुश', 'एस्थेटिक', 'क्षेत्रद्वारा', 'सज्जा', 'ठक्कर', 'हिमाली', 'नक्सलवाद', 'पेरेल्स', 'भपाई', 'बीताने', 'वाल्व', 'बेसिन', 'पराधीन', 'कार्सन', 'डेल्टा', 'त्रिपुरा', 'खेड़ा', 'शालीमार', 'मोसेस', 'इलेक्ट्रोमीटर', 'रूकेंगे', 'मारिया', 'रागों', 'फलितार्थ', 'महलों', 'भड़काया', 'साहित्योत्सव', 'चंद्रपुर', 'सरका', 'अहितकारी', 'अलयमनी', 'निर्दल', 'महबूबनगर', 'बरसाई', 'तो', 'बेचकर', 'कौशलता', 'मशीन', 'वायरलेस', 'क्यूआईपीएस', 'एमटीए', 'इंडस्ट्रियल', 'भारतवंशी', 'व्लादिमीर', 'उम्मीदवारी', 'साफगोई', 'उस्तरा', 'संघीयता', 'कुमारी', 'छुड़वाया', 'वेरोनिका', 'निर्देशिका', 'सेटो', 'रोडरेज', 'छहों', 'वंशवाद', 'रोहतक', 'अटकता', 'बावरा', 'अपारदर्शी', 'अंधापन', 'हमलावर', 'पूर्वापेक्षाओं', 'ओकलाहोमा', 'खलती', 'दायित्व', 'बचती', 'कैडमियम', 'आर्यिका', 'छींक', 'फलता', 'बागोरा', 'फतेहगढ़', 'मुर्गीपालन', 'उभारा', 'बेक', 'कपूर', 'सेवाला', 'वर्शन', 'चिढ़ते', 'ड्रेजर', 'गाजीपुर', 'मिचेल', 'स्तनधारी', 'धोराजी', 'सत्यार्थप्रकाश', 'ची', 'धटना', 'रहमत', 'व्हीलिंग', 'विप्रो', 'पुलम', 'पोर्टिलो', 'पेडू', 'पिपरा', 'जस्ट', 'खन्ना', 'तोलासन', 'जानाकरी', 'एंजेलिन', 'फ्लिंट', 'गिरकर', 'वैज्ञानिक', 'फूलती', 'गंध', 'चक्षु', 'लीडरों', 'स्तुति', 'औरतो', 'हेब्लिकर', 'प्रतियुक्ति', 'निभाएगी', 'ब्रिगेडियर', 'अम्बिकाओं', 'इशाक', 'सुमि', 'धर्मपत्रियां', 'ईए', 'ताजा', 'अजस्र', 'मंत्रयों', 'डीलरशिप', 'जिलावासियों', 'कसेगा', 'दीपन', 'टोकने', 'फलीभूत', 'शामजीभाई', 'पहनाकर', 'किसानी', 'उधारदाताओं', 'रामटेक', 'व्याख्यात्मकता', 'भुगता', 'कक्षांग', 'रसायनिक', 'औरोरा', 'देशसेवा', 'वर्कचार्ज', 'जिनी', 'असुरक्षित', 'अलंकार', 'अशक्तता', 'विधिवत', 'राधे', 'निकी', 'मनोवैज्ञानियों', 'परीक्षणकर्ताओं', 'रसायनों', 'वैत्रवती', 'स्खलन', 'पनामा', 'मोंटे', 'सुनीति', 'अम्बिकाओं', 'फोटोफ्रेम', 'प्रहलाद', 'बीसवां', 'द्विवेदी', 'देनेवाली', 'जताती', 'मालेगांव', 'महामंत्र', 'बीस्ट', 'दिव्यता', 'संजीवन', 'कुपोषित', 'चढ़ेगा', 'सराही', 'वर्थ', 'आस्तीन', 'स्वप्निका', 'मिमिक्री', 'बल्लभ', 'हैकेंसैक', 'आकाश', 'बोना', 'कहलाया', 'तेलीबाग', 'धारक', 'निकालनी', 'प्रश्नोत्तरी', 'भाग्योदय', 'मचाए', 'धर्मराजने', 'बंटता', 'हनुमानगढ़', 'वीरांगनाओं', 'गायत्री', 'संयोजकों', 'सरकारें', 'न्यूटेस्ट', 'श्रीकर', 'भूपसिंह', 'धागा', 'जूड', 'शरभ', 'लेनेवाली', 'कुमार', 'सिंगार', 'खिसकते', 'एनिड', 'मेगापिक्सल', 'मुनाफा', 'ओर्टेगा', 'रोवे', 'भाटिया', 'बागबगीचों', 'दुकानों', 'जलसंसाधन', 'बर्षीय', 'हुगली', 'नेग', 'महाबलेश्वर', 'प्लेसेंटिया', 'वर्साचे', 'एरिका', 'काव्यविधाओं', 'कून', 'मचाए', 'प्रवृत्रियों', 'ताजा', 'जीवनशैली', 'रक्तदाताओं', 'भौतिकवादियों', 'वंशवाद', 'अंशांकन', 'मतपत्रों', 'पहुंचोगी', 'अमित', 'वेस्ले', 'शाओ', 'वीक्स', 'बस्तियो', 'ग्रीनविल', 'नजीबाबाद', 'फाइनेंसर्स', 'हरिजन', 'आर्यों', 'खिलाना', 'दुखियों', 'यारी', 'स्टॉप्स', 'धकेलते', 'दमघोंटू', 'रत्नाभूषण', 'दुश्वारियों', 'अनुप्रयोगों', 'देशसेवा', 'डेहरी', 'डीवीसी', 'प्रक्रियागत', 'अर्धविश्वास', 'दरम्यानी', 'स्थितियां', 'धर्मानंद', 'विधानसभाएं', 'स्वर्णिका', 'हत्यारोपित', 'बाबाओ', 'रोजलिन', 'श्रीराम', 'गोपालक', 'मिस्त्रियां', 'मैसोर', 'उसकी', 'रुपेश', 'सुदीप्तों', 'स्वामी', 'व्यवस्थामूलक', 'पहनो', 'फिनकॉर्प', 'नवा', 'सोहावल', 'राजों', 'चीखना', 'एंटोनी', 'निर्देशिका', 'जुडने', 'आदिवादियों', 'जुलाहों', 'महाराजा', 'शिप्रा', 'वनविभाग', 'घटाटोप', 'पाखंडी', 'प्रोटेक्ट्स', 'लिंडेन', 'सोहावल', 'विद्रूप', 'रोजना', 'सेव', 'छतेनी', 'विश्वसनीय', 'दस्तानें', 'धोना', 'सूरीनाम', 'मंगलसिंह', 'बेचते', 'अनुचित', 'अवरोधक', 'रॉस', 'अच्युत', 'हरावल', 'गोल्ड', 'दाव', 'उतने', 'संस्कृतिओं', 'रसद', 'स्प्रे', 'मंत्रियोंके', 'ग्रेग', 'बैतूल', 'सुब्बनी', 'खटखटाकर', 'ली', 'जीवराज', 'थिरके', 'महानतम', 'कश्मीर', 'भुट्टो', 'मारके', 'धुरिया', 'महाविद्या', 'प्रधानमंत्रीकी', 'आगाशे', 'नवजागरण', 'उबाला', 'हारना', 'पुष्कर', 'शांतियों', 'चौका', 'मंचीय', 'सप्रमाण', 'टिक', 'अज्ञात', 'खटखटाएगी', 'जोधिका', 'बदलवा', 'छापेमार', 'भोपाल', 'फलीभूत', 'उठाकर', 'सोमी', 'कांचीपुरम', 'वेल्डिंग', 'वेरोनिका', 'अतुल', 'जांचना', 'मिथिलांचल', 'सजेती', 'प्लानो', 'विसंगतिया', 'खोमचे', 'मुकेश्वरी', 'तस्वारों', 'देव', 'वेश', 'ज्योथी', 'चौक', 'चेस्ट', 'कसेगा', 'रेनुका', 'लूवे', 'गुलाबाग', 'धावकों', 'लिम', 'रामराज', 'जनभावना', 'यौनिकता', 'हाथे', 'वैज्ञानिक', 'टोरेंट', 'संप्रति', 'मत्थे', 'सविता', 'अटलांटिक', 'चौकी', 'मौम', 'आपका', 'वार्नर', 'लिखेहैं', 'शावकों', 'यरूशलेम', 'नैनो', 'धम्म', 'सागरिका', 'ब्रेवरीज', 'वर्तमान', 'मराठा', 'बनवा', 'स्वर्णिका', 'सुवेन', 'झ', 'करघे', 'देशकाल', 'थानवी', 'सेरानो', 'ग्रीनवुड', 'नियंताओं', 'अन्याय', 'गुलशन', 'रिया', 'ची', 'बदलापुर', 'युद्धनौका', 'उदा', 'एयरवे', 'कैल्यूमेट', 'बुद्धिजीवों', 'एनएमडीसी', 'राष्ट्रपित', 'भल्ला', 'घटाएगी', 'सेवाग्राम', 'तंत्र', 'सीएसआई', 'मांड्या', 'अंकोर', 'रैपिंग', 'दाव', 'पाठ्यक्रमों', 'गांधीपुरा', 'नारायणगढ़', 'जयललिता', 'सुसंस्कृत', 'मसूद', 'संस्कृतियां', 'प्रतिनिर्देश', 'उल्हासपुर', 'धावकों', 'भिजवाई', 'सायरा', 'विवेकाधिकारों', 'मलौन', 'रेगिस्तान', 'परोसने', 'विचारकों', 'जनभावना', 'व्याख्यात्मकता', 'शैशव', 'जायके', 'कुरुक्षेत्र', 'चर्चा', 'निभाएगी', 'जनभावना', 'श्विंग', 'संतति', 'संचार', 'रोपने', 'हॉलीवुड', 'धम्मा', 'टेक्नोप्लास्ट', 'श्रंखलाओं', 'कार्बोजेन', 'व्यवधान', 'स्क्वाश', 'प्रयोगशास्त्र', 'पालियों', 'प्रतिस्थान', 'लकीर', 'जागृति', 'तलवार', 'जनशिक्षा', 'बुलेटिन', 'स्कैनिंग', 'रहेगी', 'जमशेदपुर', 'दर्शाना', 'सुधारती', 'वसामुक्त', 'जेवाब', 'डोसे', 'पगबाधा', 'इक्विटीज', 'अचीवमेंट', 'आर्य', 'घोट', 'नोरा', 'गुजारने', 'वाटर', 'सिरसा', 'ड्रेजर', 'धोना', 'कार्यकर्त्रियों', 'बंधुओं', 'डुबाना', 'प्रतिज्ञापूर्ण', 'श्रीकृष्णन', 'भूस्वामी', 'ताप्ती', 'घोंटना', 'अद्वैतवाद', 'लुभावना', 'रोपने', 'फाड़े', 'उन्नतियों', 'मोलिना', 'जुए', 'टोंक', 'देवापुर', 'नीतियों', 'पोरबंदर', 'सिम', 'लग्नेश', 'गायिकाओ', 'पृष्ठभूमियों', 'मार्गरिटा', 'प्रशिक्षु', 'कोरापुट', 'विद्युतीकृत', 'किकियाना', 'अरुणाई', 'मृदुभाषी', 'वाल्टर्स', 'ज़िलाधिकारी', 'जिलावासियों', 'प्लोस्की', 'सेखर', 'अर्धविश्वास', 'राजस्थान', 'विकासखण्ड', 'बाताया', 'बारहसिंगा', 'पलारी', 'वाबस्ता', 'वाटरबरी', 'सेल्फी', 'वादक', 'जिया', 'एडेन', 'फीड्स', 'बावजी', 'विज्ञानों', 'अट्रैक्ट', 'सराहा', 'नृत्यशैली', 'यक्षाधिपति', 'मुकद्दमों', 'सिहुंता', 'जीवन', 'कार्यकर्त्रियों', 'प्रतिबद्धताएं', 'अधिवास', 'विश्ववीर', 'क्रेस्ट', 'ब्रह्मलीन', 'विरेचन', 'जिम्मे', 'कुंडल', 'रिया', 'स्लाइडर', 'क्रिस्टिस', 'डिस्ट्रब्यूटर्स', 'स्पेल', 'साउथ', 'आपका', 'नहींकरवाई', 'जागृति', 'जीएसएम', 'नाभा', 'लेगा', 'तीमारदारों', 'सेरेब्रा', 'दस्तार', 'चढ़ेगा', 'मेंशराब', 'रामकोट', 'बैठते', 'सोलापुर', 'मेजबानी', 'झाड़ने', 'हिकारिको', 'बोकारो', 'मौर्या', 'चंद्रमाओं', 'परोसने', 'स्पिरिट्स', 'स्वार्थपरक', 'पूर्वजन्मों', 'एवरेस्ट', 'तस्वारों', 'प्रेमलता', 'वैमनस्य', 'केली', 'महामंत्र', 'शुमाली', 'फुलाए', 'प्रशासनिक', 'सीडीसीएल', 'आदिवादियों', 'चित्रकथा', 'अवशिष्ट', 'नन्हें', 'त्रिकालदर्शी', 'पालसाही', 'जमाती', 'पोंडिंग', 'देशकाल', 'हिचकिचा', 'गरीबी', 'सीताराम', 'बैरकपुर', 'कासोवो', 'स्किपर', 'प्रस्थानों', 'रेडियोप्रेमी', 'चिको', 'मैनवल', 'सतलुज', 'जातियों', 'टेलीलिंक्स', 'लैब्स', 'हरावल', 'मनिया', 'ट्रैक्टर्स', 'कार्यावधि', 'बागोरा', 'अभिमत', 'खातेदार', 'उपद्रवों', 'टिकियों', 'मिल्क', 'बेकर', 'बुद्धाराम', 'अर्धविश्वास', 'बलजीत', 'जातकों', 'स्वर्गदूत', 'ददरेवा', 'बिखेरने', 'हमलो', 'उपलब्धियों', 'पैकर्ड', 'नफ़ा', 'सिन्हा', 'आर्य', 'दिखाइये', 'अनिष्टता', 'पूर्वाभास', 'बिजेन्द्रसिंह', 'रूपी', 'कार्यकर्ताओं', 'अगस्ता', 'अनुमार', 'यूजी', 'राज', 'भरा', 'फलने', 'ज़माना', 'घटाएगी', 'दंगा', 'एरिका', 'भोला', 'व्याख्यात्मकता', 'प्रधाननगर', 'हमला', 'न्यूट्रियो', 'कुशवाहा', 'अर्थ', 'कूच', 'सुलखान', 'कमेरे', 'संत्री', 'पंचांग', 'नित्या', 'इच्छी', 'पालियों', 'तावीज', 'प्रभानमंत्री', 'सेथ', 'हर्षित', 'रक्तदाताओं', 'मेक्सिको', 'इसरानी', 'श्रीमहापूर्ण', 'जीनों', 'उखड', 'मंजिल', 'एलएक्स', 'दुभाषियों', 'रिकनेक्टिंग', 'विंड', 'निकेतन', 'ट्विन', 'दिव्य', 'गबली', 'रिश्वतखोर', 'गर्जना', 'निहलानी', 'पिंटा', 'ग्रीर', 'राखे', 'वर्ल्ड', 'उठनी', 'जेठा', 'बागबगीचों', 'सुचिकित्सा', 'नागमण्डल', 'प्रदाधिकारियों', 'उर्वरता', 'उपलक्ष्य', 'छुड़वाया', 'रतनपुर', 'गुना', 'एमटेक', 'देनेवाली', 'विशेषणों', 'संबित', 'पुलिंदे', 'महोनी', 'खरे', 'सवाल', 'फाँस', 'जयललिता', 'अमझेरा', 'अपनानी', 'रिसने', 'छुरे', 'जोस', 'जबलपुर', 'बुल', 'खग्यार', 'मेहराम', 'रिपन', 'मिथ्या', 'ललानिया', 'भूलों', 'बिगेस्ट', 'जोड़ता', 'महत्तरों', 'धैर्यवान', 'यूरोपिया', 'मेज़ा', 'बुद्धिजीवों', 'रदर', 'उबाल', 'जीवनशैली', 'भाते', 'विप्ख', 'बयाने', 'लोकोत्सव', 'तेवरी', 'मालवान', 'क़ब्ज़ा', 'सुसंस्कृत', 'संधोल', 'हाब्बन', 'बाउलिंग', 'रस्मे', 'बोवी', 'चेन्नई', 'कमलजीत', 'फलने', 'सिद्धार्थः', 'डाल्टन', 'मिथ्याचारियों', 'संस्कृतियों', 'छतेनी', 'दुष्क्रिया', 'सासाराम', 'चिढ़ते', 'मास', 'बरामद', 'क्रेमिस्ट्री', 'अंदाजी', 'फोंड', 'पुतला', 'मोहिम', 'सीपीएम', 'पाल्मर', 'एपकोटेक्स', 'मोल्ड', 'हूपर', 'वाटर्स', 'लुभाते', 'दंपति', 'सोया', 'बर्बरतापूर्ण', 'तरीको', 'पहला', 'सतुआ', 'लेबलिंग', 'अस्मिताओं', 'तड़पती', 'पटेल', 'काजी', 'स्टेट', 'अव्यवसायी', 'सिरपुर', 'खरुवार', 'धर्मविहीनता', 'अरुणाचल', 'संस्कारियों', 'सियाह', 'गुजारने', 'गजरा', 'उभरेगा', 'नरभक्षी', 'एंट्रीक्स', 'बडू', 'जेन्सन', 'पाली', 'वविद्यालय', 'चंदन', 'मेगा', 'नवकेतन', 'तपन', 'आराम', 'यशपाल', 'वोखा', 'छेत्री', 'परवीन', 'कुकरेजा', 'अवरोधक', 'तीरों', 'गणेशा', 'अनुपमा', 'तलबगार', 'बूदम', 'मेडीकल', 'रिस्पेक्टिंग', 'सक्शन', 'रंगते', 'कार्यकर्ताओं', 'विद्याओं', 'रैथ', 'पुष्पा', 'कारगिल', 'टकरा', 'क्रोम', 'स्तिथि', 'सुहानुभूति', 'विरोधाभासी', 'साभार', 'अव्यवसायी', 'प्राविंसेस', 'पर्चियों', 'प्रतिवाद', 'पंजिका', 'बातः', 'प्रेमात्मक', 'सिपाही', 'हल्ला', 'उसक', 'नागपुर', 'भपाई', 'फ्लेश', 'राष्ट्रक', 'राजभाई', 'सुभाय', 'उठने', 'सरगुजा', 'ग्रहणकाल', 'मुंडवा', 'तोलासन', 'नागमण्डल', 'कनाल', 'फेलिक्स', 'हस्ताक्षरों', 'फोर्टिस', 'जैनों', 'अर्थव्यवस्थाः', 'खिताबों', 'प्राटेस्टेंट', 'लक्ष्मीधर', 'सियासी', 'उपयोगितावादियों', 'कुमावत', 'नजराने', 'पशुचारा', 'वेबुनियादी', 'छल्ला', 'जौहरी', 'खिताबों', 'घिसे', 'सप्पांवाली', 'बाधित', 'ब्रह्मपुर', 'कोलिंग', 'समाधियों', 'छापेमार', 'लगेगा', 'दुर्घटना', 'नितियों', 'कैपधारी', 'अत्री', 'सिंगम', 'सबस्टेंसिस', 'पीलेपन', 'हंडिया', 'प्रतिरक्षी', 'दुकानों', 'सर्वजातीय', 'नरसंहारों', 'हेरेरा', 'इकाई', 'रोपी', 'चौकियां', 'लेटन', 'डेवलपमेंट', 'परदेसियों', 'भूगोल', 'बगीची', 'सिवान', 'स्ट्रे', 'आलसी', 'व्यू', 'महानिदेशक', 'भट्टाचार्य', 'कन्याएं', 'महाविद्या', 'ओडोम', 'मतियाना', 'रेहाना', 'अनावश्यक', 'रिझाया', 'ब्रिगेड', 'सपने', 'चुनीं', 'कोषाधिकारी', 'करब', 'उसकी', 'लामिनार', 'कौशिक', 'बेकाबू', 'तोमर', 'रोपी', 'बाबत', 'कोलन', 'हलिया', 'पोप', 'करखेले', 'रामजीलाल', 'मुख्यमंत्री', 'मारिया', 'उदर', 'सुदीप', 'निकालनी', 'भटकाना', 'रजिस्टर्ड', 'भंडारा', 'ललानिया', 'लिमा', 'लालकृष्ण', 'लिंडा', 'खरिया', 'ऋतुकांत', 'कुर्मी', 'दस्ताना', 'घुमाई', 'उपद्रवियों', 'धर्मराजने', 'खेमिक', 'तुगलकी', 'रेसर', 'भका', 'अंधभक्तो', 'नजरियों', 'फेका', 'टिंका', 'जातियों', 'सूबे', 'चंदन', 'ट्रैप', 'विद्युतीकृत', 'वेड', 'अखुआपाड़ा', 'सराहा', 'विज्ञानवादियों', 'कमज़ोर', 'पटकथाएं', 'जस्सी', 'सुमन', 'शंकचराचार्य', 'दुखों', 'नेस्को', 'फूलती', 'टूल्स', 'वर्गर', 'आत्ममुग्ध', 'रॉकफोर्ड', 'मित्रा', 'देवनार', 'ओकोई', 'उत्तरिया', 'एलिमिनेटर', 'कर', 'दफ्तरवाले', 'कंबलों', 'धातुएं', 'सियाह', 'जिता', 'क्षेत्रें', 'एंकेनी', 'बाल्टी', 'खंगाली', 'पुष्पित', 'पेरेड्डी', 'बोदरा', 'दूताय', 'काव्यविधाओं', 'जानकारों', 'रखना', 'रिश्तेदारो', 'श्रावणी', 'कुबुद्दीन', 'ज्वेलर्स', 'रोजलिन', 'हरदिल', 'खेलाने', 'स्ट्रेशन', 'लक्ष्यतीर्थ', 'निवेदनः', 'नाकाम', 'अधिवास', 'तरीको', 'चिकोपी', 'बरामद', 'होखत', 'चौरसिया', 'सीड', 'क्ले', 'देवापुर', 'मनवाया', 'निया', 'उधमपुर', 'छुपता', 'तुंगनाथ', 'रघुपति', 'गंगटोक', 'क्रिकेट', 'आगमन', 'आश्रितों', 'राघवन', 'स्वभावतः', 'सहरसा', 'मैके', 'मोशे', 'लोहाती', 'सिवान', 'थकी', 'हिकारिको', 'चरित', 'असाही', 'पहन', 'धनश्याम', 'फारेंसिस', 'लहना', 'द्वारिकेश', 'गुल्लरवाला', 'हरपाल', 'प्रेमेश्वर', 'देशके', 'सेहो']\n",
            "['thermax', 'sikhaaega', 'learn', 'twitters', 'tirunelveli', 'independence', 'speshiyon', 'kolhapur', 'anka', 'glendale', 'ambikapur', 'ukhrul', 'iqbal', 'dayaalapuraa', 'sohrai', 'saahityotsav', 'shikayatkarta', 'andarkhane', 'leedaron', 'galgand', 'murgipaalan', 'mushahid', 'holt', 'ijaajat', 'vankshetra', 'bhutal', 'swaadpremiyon', 'frektar', 'nabz', 'bouni', 'seemaai', 'darshnaarthi', 'rivas', 'tarkvaad', 'anusaarakaa', 'coachella', 'parishad', 'granthiyon', 'buena', 'rikailleebreshan', 'shasanadhikaariyon', 'fijoolkharchi', 'catlin', 'wonder', 'shving', 'pashchamee', 'nilaabh', 'upneta', 'maas', 'westing', 'mukeshvari', 'shrimati', 'vivekadhikaron', 'kabahaa', 'bhraantiyon', 'jivraj', 'vishnupura', 'ghotaalebaajon', 'singar', 'balkrishna', 'phabti', 'palatne', 'nagaada', 'klein', 'karyabhar', 'covina', 'barker', 'caitlyn', 'phoos', 'neko', 'kaushal', 'dilaaega', 'pharase', 'aantrit', 'murgipalan', 'luis', 'science', 'sarkarein', 'gurupado', 'sanyukt', 'sabhaen', 'tani', 'vineet', 'kikiyana', 'maxwell', 'hippo', 'deepan', 'karmaacharee', 'durgapur', 'tribhaashaa', 'soodkhoron', 'prastaavon', 'welwet', 'baxter', 'uthani', 'ikattha', 'bhupsingh', 'bosaan', 'karki', 'manokaamnaaon', 'parichar', 'crane', 'ulhasnagar', 'prabhanmantri', 'shiwalapurwa', 'ferth', 'pratirodhaatmak', 'svarnikaa', 'vyavasthapana', 'shreemahapoorn', 'chadhne', 'cricketing', 'reduction', 'chatakaate', 'kubuddin', 'aalochakon', 'sanchalanon', 'shaasanadhikariyon', 'mahadhipatiyon', 'sankhyaen', 'harpal', 'grover', 'angelo', 'pokar', 'apvartit', 'pratishthaavaale', 'emile', 'shaili', 'chhallaa', 'lakshyon', 'devi', 'bhushan', 'rukte', 'aamdani', 'junagadh', 'odhakar', 'broke', 'talaasha', 'parivaarwaale', 'kush', 'chadhen', 'sachan', 'canton', 'aatmamugdha', 'ghontanaa', 'maryadaon', 'ratlam', 'akola', 'notbandeeshuda', 'foda', 'noobiyaa', 'pratipaalpur', 'namkeen', 'manjilen', 'juraab', 'black', 'johari', 'tevree', 'welder', 'bharashtachaariyon', 'khaatedaar', 'aazmaanaa', 'bahawalpur', 'chausinga', 'teekon', 'athak', 'maap', 'gajra', 'sahuuliyata', 'jeetanewalon', 'doda', 'pendulum', 'chipakane', 'gond', 'chungiyon', 'arica', 'yuddhak', 'vegas', 'chitranshi', 'dharmapoorwee', 'ambaani', 'ibija', 'jhuthlane', 'lindon', 'pulama', 'bareli', 'jing', 'jivan', 'pehni', 'tanu', 'deepika', 'maulviyon', 'lakme', 'triya', 'khelmantree', 'bhavishya', 'bender', 'pradesh', 'samvedikaran', 'laguna', 'race', 'prabodhini', 'mangla', 'udaygadhi', 'akapulko', 'vaidhruti', 'euclid', 'raktadaataaon', 'baramade', 'kutta', 'kishun', 'amrohi', 'varshiya', 'shaavakon', 'swaarthparak', 'weronika', 'webb', 'agra', 'westwork', 'pasco', 'jadhav', 'sanshayon', 'upasthiyon', 'baila', 'livaal', 'hillsboro', 'shaakir', 'salahkaar', 'vishwakarma', 'rishvatkhor', 'praavinses', 'daftarvaale', 'physician', 'sipah', 'bharamaa', 'prtigyaa', 'meghwal', 'nandoi', 'formuley', 'durghtna', 'swatantrasingh', 'baalti', 'columbia', 'dukke', 'chamakna', 'dehra', 'hastaaksharon', 'tirupati', 'kursiyon', 'pahno', 'bangsh', 'gurjar', 'samanvayaka', 'madhya', 'briggs', 'haokip', 'kadmo', 'kurami', 'simtataa', 'agvaai', 'nagraj', 'rukwaakar', 'brahmvarta', 'gray', 'purooshottamapur', 'teji', 'moolon', 'hamirpur', 'pokhriyal', 'dabangata', 'dhuriya', 'gazi', 'ijajat', 'putli', 'sanchari', 'narnaari', 'dhaniye', 'gammat', 'laphphaajee', 'maathaloo', 'testing', 'besil', 'jeremy', 'vyapak', 'maddy', 'sabhasadon', 'damaae', 'midwest', 'vimarshon', 'sarsaa', 'hester', 'uttar', 'kasera', 'dayitvon', 'premamoolak', 'moran', 'nelson', 'amarpal', 'phatne', 'tinka', 'gaayikaao', 'mod', 'arthon', 'neelaabh', 'bhadkaayaa', 'namdhari', 'alba', 'khaatadhaarak', 'texas', 'special', 'harvansh', 'peediyon', 'dilaega', 'parwa', 'pradeshikaa', 'wisangatiyaa', 'ghost', 'pipli', 'helper', 'bakhshane', 'indriyonke', 'barhad', 'risaalee', 'vishvarakt', 'arunai', 'bhautikwadiyon', 'aawaazen', 'mool', 'rotation', 'barpaya', 'land', 'suchi', 'noon', 'rp', 'palanpur', 'electrosteel', 'austin', 'tcns', 'mood', 'daulatabad', 'pratikaara', 'bechkar', 'stone', 'baapa', 'safradganj', 'roopak', 'khandelwal', 'jehe', 'agartala', 'jamayaa', 'pardesi', 'aagashe', 'achchi', 'swabhawatah', 'bheera', 'silva', 'srichakra', 'nichalaa', 'burhanpur', 'sahooliyata', 'labdh', 'poole', 'strasion', 'ajit', 'jha', 'prashasnik', 'giridih', 'reading', 'rukate', 'anonditaa', 'fontana', 'dhammaa', 'panama', 'putle', 'aasaapaas', 'mufalisi', 'sansathaan', 'darshnaarthee', 'lokotsav', 'graahakonne', 'reja', 'mitao', 'odhkar', 'megavaal', 'sugamata', 'munaf', 'mahawar', 'vigyan', 'vilas', 'supachye', 'vishnu', 'chaudhari', 'shipping', 'jaanam', 'damae', 'cheekhna', 'mukeshwaree', 'ummeede', 'kahaavah', 'kaantipoorn', 'charas', 'bhadauriya', 'vigyaanon', 'ganjhoo', 'aastik', 'niveshkartaa', 'sanskritiyan', 'raubdar', 'chharhari', 'jvaaron', 'sansthita', 'riggs', 'chune', 'lipaaee', 'dhandhekhoree', 'ubhregaa', 'anton', 'nihlaani', 'tivolee', 'hogis', 'winter', 'badalavaakar', 'ghumaee', 'bakhshate', 'sanvedikaran', 'farheen', 'redding', 'kisanon', 'chalenge', 'nanhein', 'bhadauria', 'pahunchanee', 'blossom', 'yatraen', 'niyuttiyon', 'murgeepaalan', 'mavey', 'vikram', 'spf', 'sabajooniyar', 'pradhan', 'abhiyuktonke', 'hijbul', 'vidyaon', 'chhupkar', 'doranda', 'sihuntaa', 'hemwati', 'koshadhikaaree', 'ashanka', 'bean', 'vaigyanikata', 'jheelon', 'shrirampur', 'vasoolataa', 'thki', 'turbine', 'dimaak', 'jodhpur', 'brahmasiddhi', 'reeta', 'charchaamanch', 'pratikaaraa', 'tarkwaad', 'dabangon', 'hichkicha', 'sheershakon', 'dam', 'bahumaan', 'kingkhan', 'taras', 'virodh', 'arena', 'boca', 'tankaar', 'khatakati', 'pushpa', 'gm', 'korola', 'pyade', 'balaatkaariyon', 'shreshthaswaroop', 'shubhprabhaa', 'rashtrabhakton', 'bhootal', 'insaano', 'emily', 'vigyaanam', 'atwaal', 'aleem', 'hr', 'imaandaari', 'gangajal', 'gaur', 'bagad', 'niveshkarta', 'patkathayen', 'bhaadas', 'bhogte', 'holding', 'dhoron', 'pratigya', 'badhaati', 'then', 'suja', 'bahunt', 'amaanat', 'jhuthla', 'bush', 'asthetic', 'kshetradwara', 'sajja', 'thakkar', 'himali', 'naksalavaad', 'perels', 'bhapaee', 'beetaane', 'valve', 'besin', 'paradhin', 'carson', 'delta', 'tripura', 'kheda', 'shalimar', 'moses', 'ilectromeetar', 'rookenge', 'maariya', 'raagon', 'phalitartha', 'mahalon', 'bhadkaya', 'sahityotsav', 'chandrapur', 'sarkaa', 'ahitakaaree', 'alayamanee', 'nirdal', 'mahbubnagar', 'barsaai', 'toh', 'bechakar', 'kaushalta', 'machine', 'wireless', 'kyooaaeepeees', 'mta', 'industrial', 'bharatwanshee', 'vladimir', 'ummidwaaree', 'saafgoi', 'ustara', 'sangheeyataa', 'kumari', 'chhudawaaya', 'veronika', 'nirdeshika', 'seto', 'roadrej', 'chhahon', 'vanshvad', 'rohtak', 'atakta', 'baawaraa', 'apaardarshi', 'andhapan', 'hamalavar', 'purvapekshaaon', 'oklahoma', 'khalati', 'dayitva', 'bachti', 'cadmium', 'aaryika', 'chheenk', 'phalata', 'baagoraa', 'fatehgadh', 'murgeepalan', 'ubhaara', 'beck', 'kapoor', 'sevaala', 'vershan', 'chidhate', 'dredger', 'gajipur', 'mitchell', 'standhaari', 'dhoraaji', 'satyaarthprakash', 'chee', 'dhatna', 'rahamat', 'wheeling', 'wipro', 'pulam', 'portillo', 'pedoo', 'pipra', 'just', 'khanna', 'tolaasan', 'jaanaakaree', 'engeline', 'flint', 'girkar', 'vaigyaanik', 'phoolatee', 'gandha', 'chakshu', 'liidaron', 'stuti', 'aurato', 'heblikar', 'pratiyukti', 'nibhaaegee', 'brigadier', 'ambikaaon', 'ishaak', 'sumi', 'dharmapatriyaan', 'ea', 'taaja', 'ajasra', 'mantrayon', 'dealership', 'jilawaasiyon', 'kasegaa', 'deepn', 'tokne', 'falibhoot', 'shaamjeebhaaee', 'pahnaakar', 'kisaani', 'udhaaradaataaon', 'ramtek', 'wyaakhyaatmaktaa', 'bhugta', 'kakshaang', 'rasaayanik', 'aurora', 'deshsewa', 'workcharge', 'jini', 'asurakshit', 'alnkaar', 'ashaktataa', 'vidhivat', 'raadhe', 'nicky', 'manovaigyaaniyon', 'parikshankartaon', 'rasayanon', 'vaitravati', 'skhalan', 'panaamaa', 'monte', 'suneeti', 'ambikaon', 'photofrem', 'prahlaad', 'beesavaan', 'dvivedi', 'denewaali', 'jataati', 'malegaon', 'mahamantra', 'beast', 'divyata', 'sanjivan', 'kuposhit', 'chadhega', 'sarahi', 'worth', 'aasteen', 'svapnika', 'mimicry', 'ballabh', 'hackensack', 'aakaash', 'bona', 'kahlaya', 'teleebag', 'dhaarak', 'nikaalani', 'prashnottari', 'bhaagyoday', 'machaye', 'dharmarajne', 'bantata', 'hanumangarh', 'veeraangnaaon', 'gaytri', 'sanyojakon', 'sarkaren', 'newtest', 'shrikar', 'bhoopsingh', 'dhaagaa', 'jud', 'sharabh', 'lenevali', 'kumar', 'singaar', 'khisakate', 'enid', 'megapixal', 'munafa', 'ortega', 'rowe', 'bhatia', 'baagbageechon', 'dukanon', 'jalsansadhan', 'barsheey', 'hugli', 'neg', 'mahabaleshwar', 'placentia', 'varsaache', 'erika', 'kavyavidhaaon', 'coon', 'machaaye', 'pravritriyon', 'taajaa', 'jivanshaili', 'raktdaataaon', 'bhautikwaadiyon', 'vanshvaad', 'anshaankan', 'matpatron', 'pahunchogee', 'amit', 'vesle', 'shaao', 'weeks', 'bastiyo', 'greenville', 'najibabaad', 'financiers', 'harijan', 'aryon', 'khilana', 'dukhiyon', 'yari', 'stops', 'dhakelte', 'damghontu', 'ratnaabhooshan', 'dushwaariyon', 'anupryogon', 'deshseva', 'dehri', 'dvc', 'prakriyagat', 'ardhavishvas', 'daramyaanee', 'sthitiyan', 'dharmaanand', 'vidhansabhaen', 'swarnikaa', 'hatyaaropit', 'baabaao', 'rozlyn', 'sriram', 'gopaalak', 'mistriyan', 'maisore', 'usaki', 'rupesh', 'sudeepton', 'swamy', 'vyavasthaamoolak', 'pehno', 'fincorp', 'navaa', 'sohawal', 'raajon', 'cheekhana', 'antony', 'nirdeshikaa', 'judne', 'aadivaadiyon', 'julaahon', 'maharaja', 'shipra', 'wanvibhag', 'ghataatop', 'pakhandi', 'protekts', 'linden', 'sohaval', 'widroop', 'rojana', 'sev', 'chhateni', 'vishwasniiya', 'dastaanen', 'dhona', 'suriname', 'mangalsinh', 'bechate', 'anuchit', 'avarodhak', 'ross', 'achyut', 'haraval', 'gould', 'daaw', 'utaney', 'sanskrition', 'rasad', 'spray', 'mantriyonke', 'greg', 'baitool', 'subbanee', 'khatakhataakar', 'lee', 'jeevraaj', 'thirake', 'mahanatam', 'kashmir', 'bhutto', 'maarake', 'dhuriyaa', 'mhavidya', 'pradhanmantriki', 'aagaashe', 'navjaagaran', 'ubala', 'haaranaa', 'pushkar', 'shaantiyon', 'chauka', 'manchiy', 'sapramaan', 'tik', 'agyat', 'khatakhataaegee', 'jodhikaa', 'badalwa', 'chhaapemaar', 'bhopal', 'phalibhoot', 'uthaakar', 'somi', 'kanchipuram', 'welding', 'veronica', 'atul', 'jaanchana', 'mithilanchal', 'sajetee', 'plano', 'visangatiyaa', 'khhomche', 'mukeshwari', 'tasvaaron', 'dev', 'vesh', 'jyothy', 'chauk', 'chest', 'kasega', 'renuka', 'loowe', 'gulaabaag', 'dhaavakon', 'lim', 'ramraj', 'janbhavna', 'yauniktaa', 'haathe', 'vaigyanik', 'torrent', 'samprati', 'matthe', 'savita', 'atlantic', 'chauki', 'maum', 'aapka', 'warner', 'likhehain', 'shavkon', 'yarooshaleim', 'naino', 'dhamm', 'sagarika', 'breweries', 'vartamaan', 'maratha', 'banvaa', 'svarnika', 'suven', 'jh', 'karghe', 'deshkaal', 'thanvi', 'serrano', 'greenwood', 'niyantaaon', 'anyaya', 'gulshan', 'riya', 'chi', 'badlapur', 'yuddhanaukaa', 'udaa', 'airway', 'calumet', 'buddhijeewon', 'nmdc', 'raashtrapit', 'bhalla', 'ghataaegi', 'sevagram', 'tantra', 'csi', 'mandya', 'ankor', 'rapping', 'daav', 'pathyakramon', 'gandhipura', 'narayangadh', 'jaylalita', 'susanskrut', 'masood', 'sanskrutiyan', 'pratinirdesh', 'ulhaaspur', 'dhawakon', 'bhijvaai', 'sayra', 'wiwekadhikaron', 'maloun', 'registan', 'parosne', 'vichaarakon', 'janbhawna', 'wyakhyatmakta', 'shaishav', 'jaayke', 'kurukshetra', 'charcha', 'nibhaegi', 'janbhaavna', 'shwing', 'santati', 'sanchar', 'ropaney', 'hollywood', 'dhamma', 'technoplast', 'shrankhalaaon', 'carbogen', 'vyavadhan', 'squash', 'prayogshaastra', 'paliyon', 'pratisthaan', 'lakeer', 'jagriti', 'talwar', 'janashiksha', 'bulletin', 'scaining', 'rahegi', 'jamshedpur', 'darshaana', 'sudhaaratee', 'wasaamukt', 'jewaab', 'dose', 'pagabadha', 'equities', 'achivement', 'arya', 'ghot', 'nora', 'gujaarne', 'water', 'sirsa', 'dreijer', 'dhonaa', 'karyakartriyon', 'bandhuon', 'dubana', 'pratigyapoorn', 'shreekrushnan', 'bhooswaami', 'taapti', 'ghontana', 'advaitwad', 'lubhaavna', 'ropane', 'phaade', 'unnatiyon', 'molina', 'juye', 'tonk', 'devaapur', 'neetiyon', 'porbandar', 'sim', 'lagnesh', 'gaayikao', 'prishthabhoomiyon', 'margarita', 'prashikshu', 'koraput', 'vidyutikrit', 'kikiyaanaa', 'arunaai', 'mridubhaashee', 'walters', 'zilaadhikaari', 'jilavasiyon', 'ploskee', 'sekhar', 'ardhawishwaas', 'rajasthan', 'vikaskhand', 'baataaya', 'barahsinga', 'palaari', 'vaabastaa', 'waterbury', 'selfie', 'vaadak', 'jiya', 'aden', 'feeds', 'bavji', 'vigyanon', 'atraikt', 'saraaha', 'nrityashaili', 'yakshadhipati', 'mukaddamon', 'sihunta', 'jeewan', 'kaaryakartriyon', 'pratibaddhtaaen', 'adhiwas', 'wishvaweer', 'crest', 'brahmleen', 'virechan', 'jimme', 'kundal', 'ria', 'slider', 'cristis', 'distrabyuters', 'spell', 'south', 'aapkaa', 'nahinkarwaaee', 'jaagriti', 'gsm', 'nabha', 'lega', 'teemaardaron', 'cerebra', 'dastaar', 'chadhegaa', 'mensharaab', 'ramkot', 'baithate', 'solapur', 'mejbaani', 'jhaadane', 'hikaarico', 'bokaro', 'maurya', 'chandramaon', 'parosane', 'spirits', 'swarthparak', 'poorwajanmon', 'everest', 'taswaaron', 'premlata', 'vaimanasya', 'kelly', 'mahamantr', 'shumali', 'fulaaye', 'prashasanik', 'cdcl', 'aadiwadiyon', 'chitrakathaa', 'avshisht', 'nanhe', 'trikaldarshee', 'paalasaahee', 'jamati', 'ponding', 'deshkal', 'hichkichaa', 'garibi', 'sitaram', 'bairakpur', 'kaasovo', 'skipper', 'prasthaanon', 'radiopremi', 'chico', 'mainval', 'satluj', 'jatiyon', 'telelinks', 'labs', 'harawal', 'maniya', 'tractors', 'kaaryaawadhi', 'baagora', 'abhimat', 'khaatedar', 'upadrawon', 'tikiyon', 'milk', 'baker', 'buddharam', 'ardhavishwaas', 'baljit', 'jaatakon', 'swargdoot', 'dadreva', 'bikherane', 'hamlo', 'uplabdhiyon', 'packard', 'naphaa', 'sinha', 'aarya', 'dikhaaiye', 'anishtata', 'purvaabhaas', 'bijendrasinh', 'roopi', 'karykartaon', 'augusta', 'anumar', 'yoojee', 'raaj', 'bhara', 'falne', 'zamana', 'ghataaegee', 'danga', 'erica', 'bhola', 'vyakhyatmakta', 'pradhaannagar', 'hamla', 'nutrio', 'kushwaha', 'arth', 'cooch', 'sulkhan', 'kamere', 'santree', 'panchang', 'nitya', 'ichchhee', 'paaliyon', 'taavij', 'prabhaanmantree', 'seth', 'harshit', 'raktadataon', 'mexico', 'israni', 'shreemahapurn', 'jeenon', 'ukhad', 'manzil', 'lx', 'dubhashiyon', 'rikannekting', 'wind', 'niketan', 'twin', 'divya', 'gabalee', 'rishwatkhor', 'garjana', 'nihlani', 'pintaa', 'greer', 'raakhe', 'world', 'uthanee', 'jetha', 'baagbagichon', 'suchikitsaa', 'nagmandal', 'pradadhikariyon', 'urvarta', 'uplakshya', 'chhudwaya', 'ratanpur', 'guna', 'mtech', 'denevali', 'visheshanon', 'sambit', 'pulinde', 'mahoney', 'khare', 'sawaal', 'phaans', 'jayalalitaa', 'amjhera', 'apnani', 'risane', 'chhure', 'jose', 'jabalpur', 'bull', 'khagyaar', 'meharaam', 'ripan', 'mithya', 'lalaaniyaa', 'bhulon', 'biggest', 'jodta', 'mahattaron', 'dhairyavaan', 'yooropiyaa', 'meza', 'buddhizeevon', 'radar', 'ubaal', 'jeevanshaili', 'bhaate', 'wipkha', 'bayaane', 'lokotsaw', 'tewree', 'malwan', 'qabzaa', 'susanskrit', 'sandhol', 'haabban', 'bowling', 'rasme', 'bowie', 'chennai', 'kamaljeet', 'phalne', 'siddhaarthah', 'dalton', 'mithyaachariyon', 'sanskrutiyon', 'chhatenee', 'dushkriyaa', 'sasaram', 'chidhte', 'mass', 'baraamad', 'kremistry', 'andaajee', 'fond', 'putla', 'mohim', 'cpm', 'palmer', 'apcotex', 'mold', 'hooper', 'waters', 'lubhaate', 'dampati', 'soya', 'barbartapurna', 'tariko', 'pahla', 'satuaa', 'labelling', 'asmitaon', 'tadpatee', 'patel', 'kaaji', 'state', 'avyavasayi', 'sirpur', 'kharuvaar', 'dharmaviheenataa', 'arunachal', 'sanskariyon', 'siyah', 'gujaarane', 'gajara', 'ubhrega', 'narbhakshi', 'entreeks', 'badoo', 'jensen', 'pali', 'vavidyaalaya', 'chandan', 'mega', 'navketan', 'tapan', 'aaraam', 'yashpal', 'wokha', 'chhetri', 'parween', 'kukreja', 'avrodhak', 'teeron', 'ganesha', 'anupama', 'talabgaar', 'boodam', 'medical', 'respecting', 'suction', 'rangate', 'karykartaaon', 'vidyaaon', 'raith', 'pushpa', 'kargil', 'takara', 'chrome', 'stithi', 'suhanubhooti', 'virodhabhasi', 'saabhaar', 'avyavasayee', 'pravinses', 'parchiyon', 'prativad', 'panjika', 'baatah', 'premaatmak', 'sipahi', 'halla', 'usak', 'nagpur', 'bhapaai', 'flesh', 'rashtrak', 'raajbhaaee', 'subhaay', 'uthane', 'sarguja', 'grahankaal', 'mundwa', 'tolasan', 'naagmandal', 'kanaal', 'felix', 'hastaksharon', 'fortis', 'jainon', 'arthavyavasthaah', 'khitabon', 'pratestent', 'lakshmidhar', 'siyasi', 'upyogitawadiyon', 'kumavat', 'najraane', 'pashuchaaraa', 'webuniyadi', 'chhalla', 'jauhari', 'khitaabon', 'ghise', 'sappaanwaalee', 'baadhit', 'brahmapur', 'koling', 'samadhiyon', 'chhapemar', 'lagega', 'durghatna', 'nitiyon', 'capdhari', 'atri', 'singam', 'substensis', 'peelepan', 'handiyaa', 'pratirakshi', 'dukaanon', 'sarvjaateeya', 'narsanharon', 'herrera', 'ikai', 'ropi', 'chaukiyan', 'layton', 'development', 'pardesiyon', 'bhoogol', 'bagichi', 'sivan', 'stray', 'aalsi', 'view', 'mahanideshak', 'bhattacharya', 'kanyaein', 'mahavidya', 'odom', 'matiyaanaa', 'rehaana', 'anaavashyak', 'rijhaayaa', 'brigade', 'sapane', 'chuneen', 'koshadhikaari', 'karab', 'uski', 'laaminaar', 'kaushik', 'bekaaboo', 'tomar', 'ropee', 'baabat', 'colan', 'haliyaa', 'pope', 'karkhele', 'ramjeelaal', 'mukhyamantri', 'maria', 'udar', 'sudip', 'nikaalni', 'bhatkaanaa', 'registerd', 'bhandara', 'lalaniya', 'lima', 'lalkrishna', 'linda', 'khariya', 'ritukaant', 'kurmi', 'dastaanaa', 'ghumaaee', 'updraviyon', 'dharmaraajne', 'khemik', 'tugalki', 'racer', 'bhakaa', 'andhabhakto', 'najriyon', 'feka', 'tinkaa', 'jaatiyon', 'sube', 'chandana', 'trap', 'vidyuteekrit', 'ved', 'akhuaapaadaa', 'saraha', 'vigyaanvaadiyon', 'kamzor', 'patkathaein', 'jassee', 'suman', 'shankacharaachaarya', 'dukhon', 'nesco', 'phooltee', 'tools', 'verger', 'aatmamugdh', 'rockford', 'mitra', 'devnaar', 'ocoee', 'uttariya', 'eliminator', 'kar', 'daftarwale', 'kambalon', 'dhaatuein', 'siyaha', 'jita', 'kshetren', 'ankeny', 'balty', 'khangaali', 'pushpit', 'pereddy', 'bodara', 'dootaay', 'kaavyavidhaaon', 'jaankaaron', 'rakhna', 'rishtedaaro', 'shravani', 'kubuddeen', 'jewellers', 'rojlin', 'haradil', 'khelaane', 'stration', 'lakshayatirth', 'nivedanah', 'naakaam', 'adhivas', 'tareeko', 'chicopee', 'baramad', 'hokhat', 'chaurasiya', 'seide', 'clay', 'devapur', 'manwaayaa', 'niya', 'udhampur', 'chhupata', 'tungnath', 'raghupati', 'gangtok', 'cricket', 'aagaman', 'aashriton', 'raghavan', 'swabhavatah', 'saharsa', 'mckay', 'moshe', 'lohaati', 'siwan', 'thaki', 'hikariko', 'charit', 'asahi', 'pehan', 'dhanshyaam', 'faarensis', 'lahanaa', 'dwarikesh', 'gullaravaalaa', 'harpaal', 'premeshwar', 'deshke', 'seho']\n",
            "['थरमैक्स', 'सिखाएगा', 'लर्न', 'ट्विटर्स', 'तिरुनेलवेली', 'इंडिपेंडेंस', 'स्पेशियों', 'कोल्हापुर', 'अंक', 'ग्लेंडल', 'अम्बिकापुर', 'उखरुल', 'इक़बाल', 'दयालपुरा', 'सोहराई', 'साहित्योत्सव', 'शिकायतकर्ता', 'अंदरखाने', 'लीडरों', 'गलगंड', 'मुर्गीपालन', 'मुशाहिद', 'होल्ट', 'इजाजत', 'वनक्षेत्र', 'भूतल', 'स्वादप्रेमियों', 'फ्रेक्टर', 'नब्ज़', 'बौनी', 'सीमाई', 'दर्शनार्थी', 'रिवास', 'तर्कवाद', 'अनुसारका', 'कोचेला', 'परिषद', 'ग्रंथियों', 'बुएना', 'रिकैलीब्रेशन', 'शासनाधिकारियों', 'फिजूलखर्ची', 'कैटलिन', 'वंडर', 'श्विंग', 'पश्चमी', 'नीलाभ', 'उपनेता', 'मास', 'वेस्टिंग', 'मुकेश्वरी', 'श्रीमति', 'विवेकाधिकारों', 'कबहा', 'भ्रांतियों', 'जीवराज', 'विष्णुपुरा', 'घोटालेबाजों', 'सिंगार', 'बालकृष्ण', 'फब्ती', 'पलटने', 'नगाड़ा', 'क्लेन', 'कार्यभार', 'कोविना', 'बार्कर', 'कैटलिन', 'फूस', 'नेको', 'कौशल', 'दिलाएगा', 'फरसे', 'आंत्रित', 'मुर्गीपालन', 'लुइस', 'साइंस', 'सरकारें', 'गुरुपदो', 'संयुक्त', 'सभाएं', 'तनी', 'विनीत', 'किकियाना', 'मैक्सवेल', 'हिप्पो', 'दीपन', 'कर्माचारी', 'दुर्गापुर', 'त्रिभाषा', 'सूदखोरों', 'प्रस्तावों', 'वेलवेट', 'बैक्सटर', 'उठनी', 'इकट्ठा', 'भूपसिंह', 'बोसान', 'कार्की', 'मनोकामनाओं', 'परिचर', 'क्रेन', 'उल्हासनगर', 'प्रभानमंत्री', 'शिवालापुरवा', 'फर्थ', 'प्रतिरोधात्मक', 'स्वर्णिका', 'व्यवस्थापना', 'श्रीमहापूर्ण', 'चढ़ने', 'क्रिकेटिंग', 'रिडक्शन', 'चटकाते', 'कुबुद्दीन', 'आलोचकों', 'संचालनों', 'शासनाधिकारियों', 'महाधिपतियों', 'संख्याएं', 'हरपाल', 'ग्रोवर', 'एंजेलो', 'पोकर', 'अपवर्तित', 'प्रतिष्ठावाले', 'एमिल', 'शैली', 'छल्ला', 'लक्ष्यों', 'देवी', 'भूषण', 'रुकते', 'आमदनी', 'जूनागढ़', 'ओढ़कर', 'ब्रोक', 'तलाशा', 'परिवारवाले', 'कुश', 'चढ़ें', 'सचन', 'कैंटन', 'आत्ममुग्ध', 'घोंटना', 'मर्यादाओं', 'रतलाम', 'अकोला', 'नोटबंदीशुदा', 'फोड़ा', 'नूबिया', 'प्रतिपालपुर', 'नमकीन', 'मंजिलें', 'जुराब', 'ब्लैक', 'जोहरी', 'तेवरी', 'वेल्डर', 'भरष्टाचारियों', 'खातेदार', 'आजमाना', 'बहावलपुर', 'चौसिंगा', 'टीकों', 'अथक', 'माप', 'गजरा', 'सहूलियत', 'जीतनेवालों', 'डोडा', 'पेंडुलम', 'चिपकने', 'गोंड', 'चुंगियों', 'एरिका', 'युद्धक', 'वेगास', 'चित्रांशी', 'धर्मपूर्वी', 'अंबानी', 'इबीजा', 'झुठलाने', 'लिंडन', 'पुलमा', 'बरेली', 'जिंग', 'जीवन', 'पहनी', 'तनु', 'दीपिका', 'मौलवियों', 'लक्मे', 'त्रिया', 'खेलमंत्री', 'भविष्य', 'बेंडर', 'प्रदेश', 'संवेदीकरण', 'लगुना', 'रेस', 'प्रबोधिनी', 'मंगला', 'उदयगढ़ी', 'अकापुल्को', 'वैधृति', 'यूक्लिड', 'रक्तदाताओं', 'बरामदे', 'कुत्ता', 'किशुन', 'अमरोही', 'वर्षिय', 'शावकों', 'स्वार्थपरक', 'वेरोनिका', 'वेब', 'अग्र', 'वेस्टवर्क', 'पास्को', 'जाधव', 'संशयों', 'उपस्थियों', 'बैला', 'लिवाल', 'हिल्सबोरो', 'शाकिर', 'सलाहकार', 'विश्वकर्मा', 'रिश्वतखोर', 'प्राविंसेस', 'दफ्तरवाले', 'फिजिशियन', 'सिपाह', 'भरमा', 'प्रतिज्ञा', 'मेघवाल', 'नंदोई', 'फार्मूले', 'दुर्घटना', 'स्वतंत्रसिंह', 'बाल्टी', 'कोलंबिया', 'दुक्के', 'चमकना', 'देहरा', 'हस्ताक्षरों', 'तिरुपति', 'कुर्सियों', 'पहनो', 'बंगश', 'गुर्जर', 'समन्वयक', 'मध्य', 'ब्रिग्स', 'हाओकिप', 'कदमो', 'कुरमी', 'सिमटता', 'अगवाई', 'नागराज', 'रुकवाकर', 'ब्रह्मवर्ता', 'ग्रे', 'पुरूषोत्तमपुर', 'तेजी', 'मूलों', 'हमीरपुर', 'पोखरियाल', 'दबंगता', 'धुरिया', 'गाज़ी', 'इजाजत', 'पुतली', 'संचारी', 'नरनारी', 'धनिये', 'गम्मत', 'लफ्फाजी', 'माथलू', 'टेस्टिंग', 'बेसिल', 'जेरेमी', 'व्यापक', 'मैडी', 'सभासदों', 'दमाए', 'मिडवेस्ट', 'विमर्शों', 'सरसा', 'हेस्टर', 'उत्तर', 'कसेरा', 'दायित्वों', 'प्रेममूलक', 'मोरन', 'नेल्सन', 'अमरपाल', 'फटने', 'टिंका', 'गायिकाओ', 'मॉड', 'अर्थों', 'नीलाभ', 'भड़काया', 'नामधारी', 'अल्बा', 'खाताधारक', 'टेक्सस', 'स्पेशल', 'हरवंश', 'पीड़ियों', 'दिलाएगा', 'परवा', 'प्रदेशिका', 'विसंगतिया', 'घोस्ट', 'पिपली', 'हेल्पर', 'बख्शने', 'इन्द्रियोंके', 'बरहद', 'रिसाली', 'विश्वरक्त', 'अरुणाई', 'भौतिकवादियों', 'आवाज़ें', 'मूल', 'रोटेशन', 'बरपाया', 'लैंड', 'सूची', 'नून', 'आरपी', 'पालनपुर', 'इलेक्ट्रोस्टील', 'ऑस्टिन', 'टीसीएनएस', 'मूड', 'दौलताबाद', 'प्रतिकारा', 'बेचकर', 'स्टोन', 'बापा', 'सफरदगंज', 'रूपक', 'खंडेलवाल', 'जेहे', 'अगरतला', 'जमाया', 'परदेसी', 'आगाशे', 'अच्छी', 'स्वभावतः', 'भीरा', 'सिल्वा', 'श्रीचक्र', 'निचला', 'बुरहानपुर', 'सहूलियत', 'लब्ध', 'पूल', 'स्ट्रेशन', 'अजित', 'झ', 'प्रशासनिक', 'गिरिडीह', 'रीडिंग', 'रुकते', 'अनोंदिता', 'फोंटाना', 'धम्मा', 'पनामा', 'पुतले', 'आसापास', 'मुफलिसी', 'संसथान', 'दर्शनार्थी', 'लोकोत्सव', 'ग्राहकोंने', 'रेजा', 'मिटाओ', 'ओढ़कर', 'मेगवाल', 'सुगमता', 'मुनाफ', 'महावर', 'विज्ञान', 'विलास', 'सुपाच्य', 'विष्णु', 'चौधरी', 'शिपिंग', 'जानम', 'दमाए', 'चीखना', 'मुकेश्वरी', 'उम्मीदे', 'कहावह', 'कांतिपूर्ण', 'चरस', 'भदौरिया', 'विज्ञानों', 'गंझू', 'आस्तिक', 'निवेशकर्ता', 'संस्कृतियां', 'रौबदार', 'छरहरी', 'ज्वारों', 'संस्थिता', 'रिग्स', 'चुने', 'लिपाई', 'धंधेखोरी', 'उभरेगा', 'एंटोन', 'निहलानी', 'टिवोली', 'होगिस', 'विंटर', 'बदलवाकर', 'घुमाई', 'बख्शते', 'संवेदीकरण', 'फरहीन', 'रेडिंग', 'किसानों', 'चलेंगे', 'नन्हें', 'भदौरिया', 'पहुंचनी', 'ब्लॉसम', 'यात्राएं', 'नियुत्तियों', 'मुर्गीपालन', 'मावे', 'विक्रम', 'एसपीएफ', 'सबजूनियर', 'प्रधान', 'अभियुक्तोंके', 'हिजबुल', 'विद्याओं', 'छुपकर', 'डोरंडा', 'सिहुंता', 'हेमवती', 'कोषाधिकारी', 'आशंका', 'बीन', 'वैज्ञानिकता', 'झीलों', 'श्रीरामपुर', 'वसूलता', 'थकी', 'टरबाइन', 'दिमाक', 'जोधपुर', 'ब्रह्मसिद्धि', 'रीता', 'चर्चामंच', 'प्रतिकारा', 'तर्कवाद', 'दबंगों', 'हिचकिचा', 'शीर्षकों', 'डम', 'बहुमान', 'किंगखान', 'तरस', 'विरोध', 'अरेना', 'बोका', 'टंकार', 'खटकती', 'पुष्प', 'जीएम', 'कोरोला', 'प्यादे', 'बलात्कारियों', 'श्रेष्ठस्वरूप', 'शुभप्रभा', 'राष्ट्रभक्तों', 'भूतल', 'इंसानो', 'एमिल', 'विज्ञानम', 'अटवाल', 'अलीम', 'एचआर', 'इमानदारी', 'गंगाजल', 'गौर', 'बगड़', 'निवेशकर्ता', 'पटकथाएं', 'भादस', 'भोगते', 'होल्डिंग', 'धोरों', 'प्रतिज्ञा', 'बढ़ाती', 'थेन', 'सूजा', 'बहुंत', 'अमानत', 'झुठला', 'बुश', 'एस्थेटिक', 'क्षेत्रद्वारा', 'सज्जा', 'ठक्कर', 'हिमाली', 'नक्सलवाद', 'पेरेल्स', 'भपाई', 'बीताने', 'वाल्व', 'बेसिन', 'पराधीन', 'कार्सन', 'डेल्टा', 'त्रिपुरा', 'खेड़ा', 'शालीमार', 'मोसेस', 'इलेक्ट्रोमीटर', 'रूकेंगे', 'मारिया', 'रागों', 'फलितार्थ', 'महलों', 'भड़काया', 'साहित्योत्सव', 'चंद्रपुर', 'सरका', 'अहितकारी', 'अलयमनी', 'निर्दल', 'महबूबनगर', 'बरसाई', 'तो', 'बेचकर', 'कौशलता', 'मशीन', 'वायरलेस', 'क्यूआईपीएस', 'एमटीए', 'इंडस्ट्रियल', 'भारतवंशी', 'व्लादिमीर', 'उम्मीदवारी', 'साफगोई', 'उस्तरा', 'संघीयता', 'कुमारी', 'छुड़वाया', 'वेरोनिका', 'निर्देशिका', 'सेटो', 'रोडरेज', 'छहों', 'वंशवाद', 'रोहतक', 'अटकता', 'बावरा', 'अपारदर्शी', 'अंधापन', 'हमलावर', 'पूर्वापेक्षाओं', 'ओकलाहोमा', 'खलती', 'दायित्व', 'बचती', 'कैडमियम', 'आर्यिका', 'छींक', 'फलता', 'बागोरा', 'फतेहगढ़', 'मुर्गीपालन', 'उभारा', 'बेक', 'कपूर', 'सेवाला', 'वर्शन', 'चिढ़ते', 'ड्रेजर', 'गाजीपुर', 'मिचेल', 'स्तनधारी', 'धोराजी', 'सत्यार्थप्रकाश', 'ची', 'धटना', 'रहमत', 'व्हीलिंग', 'विप्रो', 'पुलम', 'पोर्टिलो', 'पेडू', 'पिपरा', 'जस्ट', 'खन्ना', 'तोलासन', 'जानाकरी', 'एंजेलिन', 'फ्लिंट', 'गिरकर', 'वैज्ञानिक', 'फूलती', 'गंध', 'चक्षु', 'लीडरों', 'स्तुति', 'औरतो', 'हेब्लिकर', 'प्रतियुक्ति', 'निभाएगी', 'ब्रिगेडियर', 'अम्बिकाओं', 'इशाक', 'सुमि', 'धर्मपत्रियां', 'ईए', 'ताजा', 'अजस्र', 'मंत्रयों', 'डीलरशिप', 'जिलावासियों', 'कसेगा', 'दीपन', 'टोकने', 'फलीभूत', 'शामजीभाई', 'पहनाकर', 'किसानी', 'उधारदाताओं', 'रामटेक', 'व्याख्यात्मकता', 'भुगता', 'कक्षांग', 'रसायनिक', 'औरोरा', 'देशसेवा', 'वर्कचार्ज', 'जिनी', 'असुरक्षित', 'अलंकार', 'अशक्तता', 'विधिवत', 'राधे', 'निकी', 'मनोवैज्ञानियों', 'परीक्षणकर्ताओं', 'रसायनों', 'वैत्रवती', 'स्खलन', 'पनामा', 'मोंटे', 'सुनीति', 'अम्बिकाओं', 'फोटोफ्रेम', 'प्रहलाद', 'बीसवां', 'द्विवेदी', 'देनेवाली', 'जताती', 'मालेगांव', 'महामंत्र', 'बीस्ट', 'दिव्यता', 'संजीवन', 'कुपोषित', 'चढ़ेगा', 'सराही', 'वर्थ', 'आस्तीन', 'स्वप्निका', 'मिमिक्री', 'बल्लभ', 'हैकेंसैक', 'आकाश', 'बोना', 'कहलाया', 'तेलीबाग', 'धारक', 'निकालनी', 'प्रश्नोत्तरी', 'भाग्योदय', 'मचाए', 'धर्मराजने', 'बंटता', 'हनुमानगढ़', 'वीरांगनाओं', 'गायत्री', 'संयोजकों', 'सरकारें', 'न्यूटेस्ट', 'श्रीकर', 'भूपसिंह', 'धागा', 'जूड', 'शरभ', 'लेनेवाली', 'कुमार', 'सिंगार', 'खिसकते', 'एनिड', 'मेगापिक्सल', 'मुनाफा', 'ओर्टेगा', 'रोवे', 'भाटिया', 'बागबगीचों', 'दुकानों', 'जलसंसाधन', 'बर्षीय', 'हुगली', 'नेग', 'महाबलेश्वर', 'प्लेसेंटिया', 'वर्साचे', 'एरिका', 'काव्यविधाओं', 'कून', 'मचाए', 'प्रवृत्रियों', 'ताजा', 'जीवनशैली', 'रक्तदाताओं', 'भौतिकवादियों', 'वंशवाद', 'अंशांकन', 'मतपत्रों', 'पहुंचोगी', 'अमित', 'वेस्ले', 'शाओ', 'वीक्स', 'बस्तियो', 'ग्रीनविल', 'नजीबाबाद', 'फाइनेंसर्स', 'हरिजन', 'आर्यों', 'खिलाना', 'दुखियों', 'यारी', 'स्टॉप्स', 'धकेलते', 'दमघोंटू', 'रत्नाभूषण', 'दुश्वारियों', 'अनुप्रयोगों', 'देशसेवा', 'डेहरी', 'डीवीसी', 'प्रक्रियागत', 'अर्धविश्वास', 'दरम्यानी', 'स्थितियां', 'धर्मानंद', 'विधानसभाएं', 'स्वर्णिका', 'हत्यारोपित', 'बाबाओ', 'रोजलिन', 'श्रीराम', 'गोपालक', 'मिस्त्रियां', 'मैसोर', 'उसकी', 'रुपेश', 'सुदीप्तों', 'स्वामी', 'व्यवस्थामूलक', 'पहनो', 'फिनकॉर्प', 'नवा', 'सोहावल', 'राजों', 'चीखना', 'एंटोनी', 'निर्देशिका', 'जुडने', 'आदिवादियों', 'जुलाहों', 'महाराजा', 'शिप्रा', 'वनविभाग', 'घटाटोप', 'पाखंडी', 'प्रोटेक्ट्स', 'लिंडेन', 'सोहावल', 'विद्रूप', 'रोजना', 'सेव', 'छतेनी', 'विश्वसनीय', 'दस्तानें', 'धोना', 'सूरीनाम', 'मंगलसिंह', 'बेचते', 'अनुचित', 'अवरोधक', 'रॉस', 'अच्युत', 'हरावल', 'गोल्ड', 'दाव', 'उतने', 'संस्कृतिओं', 'रसद', 'स्प्रे', 'मंत्रियोंके', 'ग्रेग', 'बैतूल', 'सुब्बनी', 'खटखटाकर', 'ली', 'जीवराज', 'थिरके', 'महानतम', 'कश्मीर', 'भुट्टो', 'मारके', 'धुरिया', 'महाविद्या', 'प्रधानमंत्रीकी', 'आगाशे', 'नवजागरण', 'उबाला', 'हारना', 'पुष्कर', 'शांतियों', 'चौका', 'मंचीय', 'सप्रमाण', 'टिक', 'अज्ञात', 'खटखटाएगी', 'जोधिका', 'बदलवा', 'छापेमार', 'भोपाल', 'फलीभूत', 'उठाकर', 'सोमी', 'कांचीपुरम', 'वेल्डिंग', 'वेरोनिका', 'अतुल', 'जांचना', 'मिथिलांचल', 'सजेती', 'प्लानो', 'विसंगतिया', 'खोमचे', 'मुकेश्वरी', 'तस्वारों', 'देव', 'वेश', 'ज्योथी', 'चौक', 'चेस्ट', 'कसेगा', 'रेनुका', 'लूवे', 'गुलाबाग', 'धावकों', 'लिम', 'रामराज', 'जनभावना', 'यौनिकता', 'हाथे', 'वैज्ञानिक', 'टोरेंट', 'संप्रति', 'मत्थे', 'सविता', 'अटलांटिक', 'चौकी', 'मौम', 'आपका', 'वार्नर', 'लिखेहैं', 'शावकों', 'यरूशलेम', 'नैनो', 'धम्म', 'सागरिका', 'ब्रेवरीज', 'वर्तमान', 'मराठा', 'बनवा', 'स्वर्णिका', 'सुवेन', 'झ', 'करघे', 'देशकाल', 'थानवी', 'सेरानो', 'ग्रीनवुड', 'नियंताओं', 'अन्याय', 'गुलशन', 'रिया', 'ची', 'बदलापुर', 'युद्धनौका', 'उदा', 'एयरवे', 'कैल्यूमेट', 'बुद्धिजीवों', 'एनएमडीसी', 'राष्ट्रपित', 'भल्ला', 'घटाएगी', 'सेवाग्राम', 'तंत्र', 'सीएसआई', 'मांड्या', 'अंकोर', 'रैपिंग', 'दाव', 'पाठ्यक्रमों', 'गांधीपुरा', 'नारायणगढ़', 'जयललिता', 'सुसंस्कृत', 'मसूद', 'संस्कृतियां', 'प्रतिनिर्देश', 'उल्हासपुर', 'धावकों', 'भिजवाई', 'सायरा', 'विवेकाधिकारों', 'मलौन', 'रेगिस्तान', 'परोसने', 'विचारकों', 'जनभावना', 'व्याख्यात्मकता', 'शैशव', 'जायके', 'कुरुक्षेत्र', 'चर्चा', 'निभाएगी', 'जनभावना', 'श्विंग', 'संतति', 'संचार', 'रोपने', 'हॉलीवुड', 'धम्मा', 'टेक्नोप्लास्ट', 'श्रंखलाओं', 'कार्बोजेन', 'व्यवधान', 'स्क्वाश', 'प्रयोगशास्त्र', 'पालियों', 'प्रतिस्थान', 'लकीर', 'जागृति', 'तलवार', 'जनशिक्षा', 'बुलेटिन', 'स्कैनिंग', 'रहेगी', 'जमशेदपुर', 'दर्शाना', 'सुधारती', 'वसामुक्त', 'जेवाब', 'डोसे', 'पगबाधा', 'इक्विटीज', 'अचीवमेंट', 'आर्य', 'घोट', 'नोरा', 'गुजारने', 'वाटर', 'सिरसा', 'ड्रेजर', 'धोना', 'कार्यकर्त्रियों', 'बंधुओं', 'डुबाना', 'प्रतिज्ञापूर्ण', 'श्रीकृष्णन', 'भूस्वामी', 'ताप्ती', 'घोंटना', 'अद्वैतवाद', 'लुभावना', 'रोपने', 'फाड़े', 'उन्नतियों', 'मोलिना', 'जुए', 'टोंक', 'देवापुर', 'नीतियों', 'पोरबंदर', 'सिम', 'लग्नेश', 'गायिकाओ', 'पृष्ठभूमियों', 'मार्गरिटा', 'प्रशिक्षु', 'कोरापुट', 'विद्युतीकृत', 'किकियाना', 'अरुणाई', 'मृदुभाषी', 'वाल्टर्स', 'ज़िलाधिकारी', 'जिलावासियों', 'प्लोस्की', 'सेखर', 'अर्धविश्वास', 'राजस्थान', 'विकासखण्ड', 'बाताया', 'बारहसिंगा', 'पलारी', 'वाबस्ता', 'वाटरबरी', 'सेल्फी', 'वादक', 'जिया', 'एडेन', 'फीड्स', 'बावजी', 'विज्ञानों', 'अट्रैक्ट', 'सराहा', 'नृत्यशैली', 'यक्षाधिपति', 'मुकद्दमों', 'सिहुंता', 'जीवन', 'कार्यकर्त्रियों', 'प्रतिबद्धताएं', 'अधिवास', 'विश्ववीर', 'क्रेस्ट', 'ब्रह्मलीन', 'विरेचन', 'जिम्मे', 'कुंडल', 'रिया', 'स्लाइडर', 'क्रिस्टिस', 'डिस्ट्रब्यूटर्स', 'स्पेल', 'साउथ', 'आपका', 'नहींकरवाई', 'जागृति', 'जीएसएम', 'नाभा', 'लेगा', 'तीमारदारों', 'सेरेब्रा', 'दस्तार', 'चढ़ेगा', 'मेंशराब', 'रामकोट', 'बैठते', 'सोलापुर', 'मेजबानी', 'झाड़ने', 'हिकारिको', 'बोकारो', 'मौर्या', 'चंद्रमाओं', 'परोसने', 'स्पिरिट्स', 'स्वार्थपरक', 'पूर्वजन्मों', 'एवरेस्ट', 'तस्वारों', 'प्रेमलता', 'वैमनस्य', 'केली', 'महामंत्र', 'शुमाली', 'फुलाए', 'प्रशासनिक', 'सीडीसीएल', 'आदिवादियों', 'चित्रकथा', 'अवशिष्ट', 'नन्हें', 'त्रिकालदर्शी', 'पालसाही', 'जमाती', 'पोंडिंग', 'देशकाल', 'हिचकिचा', 'गरीबी', 'सीताराम', 'बैरकपुर', 'कासोवो', 'स्किपर', 'प्रस्थानों', 'रेडियोप्रेमी', 'चिको', 'मैनवल', 'सतलुज', 'जातियों', 'टेलीलिंक्स', 'लैब्स', 'हरावल', 'मनिया', 'ट्रैक्टर्स', 'कार्यावधि', 'बागोरा', 'अभिमत', 'खातेदार', 'उपद्रवों', 'टिकियों', 'मिल्क', 'बेकर', 'बुद्धाराम', 'अर्धविश्वास', 'बलजीत', 'जातकों', 'स्वर्गदूत', 'ददरेवा', 'बिखेरने', 'हमलो', 'उपलब्धियों', 'पैकर्ड', 'नफ़ा', 'सिन्हा', 'आर्य', 'दिखाइये', 'अनिष्टता', 'पूर्वाभास', 'बिजेन्द्रसिंह', 'रूपी', 'कार्यकर्ताओं', 'अगस्ता', 'अनुमार', 'यूजी', 'राज', 'भरा', 'फलने', 'ज़माना', 'घटाएगी', 'दंगा', 'एरिका', 'भोला', 'व्याख्यात्मकता', 'प्रधाननगर', 'हमला', 'न्यूट्रियो', 'कुशवाहा', 'अर्थ', 'कूच', 'सुलखान', 'कमेरे', 'संत्री', 'पंचांग', 'नित्या', 'इच्छी', 'पालियों', 'तावीज', 'प्रभानमंत्री', 'सेथ', 'हर्षित', 'रक्तदाताओं', 'मेक्सिको', 'इसरानी', 'श्रीमहापूर्ण', 'जीनों', 'उखड', 'मंजिल', 'एलएक्स', 'दुभाषियों', 'रिकनेक्टिंग', 'विंड', 'निकेतन', 'ट्विन', 'दिव्य', 'गबली', 'रिश्वतखोर', 'गर्जना', 'निहलानी', 'पिंटा', 'ग्रीर', 'राखे', 'वर्ल्ड', 'उठनी', 'जेठा', 'बागबगीचों', 'सुचिकित्सा', 'नागमण्डल', 'प्रदाधिकारियों', 'उर्वरता', 'उपलक्ष्य', 'छुड़वाया', 'रतनपुर', 'गुना', 'एमटेक', 'देनेवाली', 'विशेषणों', 'संबित', 'पुलिंदे', 'महोनी', 'खरे', 'सवाल', 'फाँस', 'जयललिता', 'अमझेरा', 'अपनानी', 'रिसने', 'छुरे', 'जोस', 'जबलपुर', 'बुल', 'खग्यार', 'मेहराम', 'रिपन', 'मिथ्या', 'ललानिया', 'भूलों', 'बिगेस्ट', 'जोड़ता', 'महत्तरों', 'धैर्यवान', 'यूरोपिया', 'मेज़ा', 'बुद्धिजीवों', 'रदर', 'उबाल', 'जीवनशैली', 'भाते', 'विप्ख', 'बयाने', 'लोकोत्सव', 'तेवरी', 'मालवान', 'क़ब्ज़ा', 'सुसंस्कृत', 'संधोल', 'हाब्बन', 'बाउलिंग', 'रस्मे', 'बोवी', 'चेन्नई', 'कमलजीत', 'फलने', 'सिद्धार्थः', 'डाल्टन', 'मिथ्याचारियों', 'संस्कृतियों', 'छतेनी', 'दुष्क्रिया', 'सासाराम', 'चिढ़ते', 'मास', 'बरामद', 'क्रेमिस्ट्री', 'अंदाजी', 'फोंड', 'पुतला', 'मोहिम', 'सीपीएम', 'पाल्मर', 'एपकोटेक्स', 'मोल्ड', 'हूपर', 'वाटर्स', 'लुभाते', 'दंपति', 'सोया', 'बर्बरतापूर्ण', 'तरीको', 'पहला', 'सतुआ', 'लेबलिंग', 'अस्मिताओं', 'तड़पती', 'पटेल', 'काजी', 'स्टेट', 'अव्यवसायी', 'सिरपुर', 'खरुवार', 'धर्मविहीनता', 'अरुणाचल', 'संस्कारियों', 'सियाह', 'गुजारने', 'गजरा', 'उभरेगा', 'नरभक्षी', 'एंट्रीक्स', 'बडू', 'जेन्सन', 'पाली', 'वविद्यालय', 'चंदन', 'मेगा', 'नवकेतन', 'तपन', 'आराम', 'यशपाल', 'वोखा', 'छेत्री', 'परवीन', 'कुकरेजा', 'अवरोधक', 'तीरों', 'गणेशा', 'अनुपमा', 'तलबगार', 'बूदम', 'मेडीकल', 'रिस्पेक्टिंग', 'सक्शन', 'रंगते', 'कार्यकर्ताओं', 'विद्याओं', 'रैथ', 'पुष्पा', 'कारगिल', 'टकरा', 'क्रोम', 'स्तिथि', 'सुहानुभूति', 'विरोधाभासी', 'साभार', 'अव्यवसायी', 'प्राविंसेस', 'पर्चियों', 'प्रतिवाद', 'पंजिका', 'बातः', 'प्रेमात्मक', 'सिपाही', 'हल्ला', 'उसक', 'नागपुर', 'भपाई', 'फ्लेश', 'राष्ट्रक', 'राजभाई', 'सुभाय', 'उठने', 'सरगुजा', 'ग्रहणकाल', 'मुंडवा', 'तोलासन', 'नागमण्डल', 'कनाल', 'फेलिक्स', 'हस्ताक्षरों', 'फोर्टिस', 'जैनों', 'अर्थव्यवस्थाः', 'खिताबों', 'प्राटेस्टेंट', 'लक्ष्मीधर', 'सियासी', 'उपयोगितावादियों', 'कुमावत', 'नजराने', 'पशुचारा', 'वेबुनियादी', 'छल्ला', 'जौहरी', 'खिताबों', 'घिसे', 'सप्पांवाली', 'बाधित', 'ब्रह्मपुर', 'कोलिंग', 'समाधियों', 'छापेमार', 'लगेगा', 'दुर्घटना', 'नितियों', 'कैपधारी', 'अत्री', 'सिंगम', 'सबस्टेंसिस', 'पीलेपन', 'हंडिया', 'प्रतिरक्षी', 'दुकानों', 'सर्वजातीय', 'नरसंहारों', 'हेरेरा', 'इकाई', 'रोपी', 'चौकियां', 'लेटन', 'डेवलपमेंट', 'परदेसियों', 'भूगोल', 'बगीची', 'सिवान', 'स्ट्रे', 'आलसी', 'व्यू', 'महानिदेशक', 'भट्टाचार्य', 'कन्याएं', 'महाविद्या', 'ओडोम', 'मतियाना', 'रेहाना', 'अनावश्यक', 'रिझाया', 'ब्रिगेड', 'सपने', 'चुनीं', 'कोषाधिकारी', 'करब', 'उसकी', 'लामिनार', 'कौशिक', 'बेकाबू', 'तोमर', 'रोपी', 'बाबत', 'कोलन', 'हलिया', 'पोप', 'करखेले', 'रामजीलाल', 'मुख्यमंत्री', 'मारिया', 'उदर', 'सुदीप', 'निकालनी', 'भटकाना', 'रजिस्टर्ड', 'भंडारा', 'ललानिया', 'लिमा', 'लालकृष्ण', 'लिंडा', 'खरिया', 'ऋतुकांत', 'कुर्मी', 'दस्ताना', 'घुमाई', 'उपद्रवियों', 'धर्मराजने', 'खेमिक', 'तुगलकी', 'रेसर', 'भका', 'अंधभक्तो', 'नजरियों', 'फेका', 'टिंका', 'जातियों', 'सूबे', 'चंदन', 'ट्रैप', 'विद्युतीकृत', 'वेड', 'अखुआपाड़ा', 'सराहा', 'विज्ञानवादियों', 'कमज़ोर', 'पटकथाएं', 'जस्सी', 'सुमन', 'शंकचराचार्य', 'दुखों', 'नेस्को', 'फूलती', 'टूल्स', 'वर्गर', 'आत्ममुग्ध', 'रॉकफोर्ड', 'मित्रा', 'देवनार', 'ओकोई', 'उत्तरिया', 'एलिमिनेटर', 'कर', 'दफ्तरवाले', 'कंबलों', 'धातुएं', 'सियाह', 'जिता', 'क्षेत्रें', 'एंकेनी', 'बाल्टी', 'खंगाली', 'पुष्पित', 'पेरेड्डी', 'बोदरा', 'दूताय', 'काव्यविधाओं', 'जानकारों', 'रखना', 'रिश्तेदारो', 'श्रावणी', 'कुबुद्दीन', 'ज्वेलर्स', 'रोजलिन', 'हरदिल', 'खेलाने', 'स्ट्रेशन', 'लक्ष्यतीर्थ', 'निवेदनः', 'नाकाम', 'अधिवास', 'तरीको', 'चिकोपी', 'बरामद', 'होखत', 'चौरसिया', 'सीड', 'क्ले', 'देवापुर', 'मनवाया', 'निया', 'उधमपुर', 'छुपता', 'तुंगनाथ', 'रघुपति', 'गंगटोक', 'क्रिकेट', 'आगमन', 'आश्रितों', 'राघवन', 'स्वभावतः', 'सहरसा', 'मैके', 'मोशे', 'लोहाती', 'सिवान', 'थकी', 'हिकारिको', 'चरित', 'असाही', 'पहन', 'धनश्याम', 'फारेंसिस', 'लहना', 'द्वारिकेश', 'गुल्लरवाला', 'हरपाल', 'प्रेमेश्वर', 'देशके', 'सेहो']\n"
          ]
        }
      ],
      "source": [
        "print(c_trg)\n",
        "print(c_src)\n",
        "print(c_pred)\n",
        "import csv\n",
        "def save_to_csv(src_list, trg_list, pred_list, file_name):\n",
        "    rows = zip(src_list, trg_list, pred_list)\n",
        "\n",
        "    with open(file_name, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['English', 'Target', 'Predicted'])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "save_to_csv(c_src,c_trg,c_pred,'correct_predictions.csv')\n",
        "save_to_csv(i_src,i_trg,i_pred,'incorrect_predictions.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjo4OyZ3UyZa",
        "outputId": "899d03f2-1eb0-412d-c2d3-370c9ee16e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "from signal import signal,SIGPIPE, SIG_DFL\n",
        "signal(SIGPIPE,SIG_DFL)\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "!wandb login da816d14625ef44d200ee4acaa517646962e6f9a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "blBCql1puVLK",
        "outputId": "2238ad37-c1ad-4fe9-fe64-ace7ce8f4f7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22s015\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230520_212822-wxcry3p3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/wxcry3p3' target=\"_blank\">earnest-tree-253</a></strong> to <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/wxcry3p3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/wxcry3p3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_artifacts.Artifact at 0x7f026c679690>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Load the CSV file\n",
        "import pandas as pd\n",
        "c_dataframe = pd.read_csv(\"/content/correct_predictions.csv\")\n",
        "table = wandb.Table(dataframe=c_dataframe)\n",
        "\n",
        "# Add the table to an Artifact to increase the row \n",
        "# limit to 200000 and make it easier to reuse\n",
        "c_table_artifact = wandb.Artifact(\n",
        "    \"correct_predictions_vanilla\", \n",
        "    type=\"dataset\"\n",
        "    )        \n",
        "c_table_artifact.add(table, \"Correct_predictions\")\n",
        "\n",
        "# Log the raw csv file within an artifact to preserve our data\n",
        "c_table_artifact.add_file(\"/content/correct_predictions.csv\")\n",
        "\n",
        "# Display as a table\n",
        "\n",
        "\n",
        "run = wandb.init(project='CS6910_Assignment3')\n",
        "\n",
        "# Log the table to visualize with a run...\n",
        "run.log({\"Vanilla_correct_predictions_table\": table})\n",
        "\n",
        "# and Log as an Artifact to increase the available row limit!\n",
        "run.log_artifact(c_table_artifact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "obnBUXicxH98",
        "outputId": "1c88f798-a9c3-4714-ec7d-d8a0f30555be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:wxcry3p3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-tree-253</strong> at: <a href='https://wandb.ai/cs22s015/CS6910_Assignment3/runs/wxcry3p3' target=\"_blank\">https://wandb.ai/cs22s015/CS6910_Assignment3/runs/wxcry3p3</a><br/>Synced 4 W&B file(s), 1 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230520_212822-wxcry3p3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:wxcry3p3). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230520_212855-rmwaorjf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/rmwaorjf' target=\"_blank\">treasured-dream-11</a></strong> to <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/rmwaorjf' target=\"_blank\">https://wandb.ai/cs22s015/%22CS6910_Assignment3%22/runs/rmwaorjf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_artifacts.Artifact at 0x7f029955efe0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Load the CSV file\n",
        "i_dataframe = pd.read_csv(\"/content/incorrect_predictions.csv\")\n",
        "i_table = wandb.Table(dataframe=i_dataframe)\n",
        "\n",
        "# Add the table to an Artifact to increase the row \n",
        "# limit to 200000 and make it easier to reuse\n",
        "i_table_artifact = wandb.Artifact(\n",
        "    \"incorrect_predictions_vanilla\", \n",
        "    type=\"dataset\"\n",
        "    )        \n",
        "i_table_artifact.add(i_table, \"Incorrect_predictions\")\n",
        "\n",
        "# Log the raw csv file within an artifact to preserve our data\n",
        "i_table_artifact.add_file(\"/content/incorrect_predictions.csv\")\n",
        "\n",
        "# Display as a table\n",
        "\n",
        "\n",
        "run = wandb.init(project='\"CS6910_Assignment3\"')\n",
        "\n",
        "# Log the table to visualize with a run...\n",
        "run.log({\"Vanilla_incorrect_predictions_table\": i_table})\n",
        "\n",
        "# and Log as an Artifact to increase the available row limit!\n",
        "run.log_artifact(i_table_artifact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h9lJPg3DjBkT"
      },
      "outputs": [],
      "source": [
        "# wandb sweeps\n",
        "\n",
        "sweep_config= {\n",
        "    \"name\" : \"CS6910_Assignment3\",\n",
        "    \"method\" : \"bayes\",\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'cell_type' : { 'values' : ['lstm','gru','rnn'] },\n",
        "        'dropout' : { 'values' : [0,0.1,0.2,0.5]},\n",
        "        'embedding_size' : {'values' : [64,128,256,512]},\n",
        "        'num_layers' : {'values' : [1]},\n",
        "        'batch_size' : {'values' : [32,64,128]},\n",
        "        'hidden_size' : {'values' : [128,256,512]},\n",
        "        'bidirectional' : {'values' : [True ,False]},\n",
        "        'learning_rate':{\n",
        "            \"values\": [0.001,0.002,0.0001,0.0002]\n",
        "        },\n",
        "        'optim':{\n",
        "            \"values\": ['adam','nadam']\n",
        "        },\n",
        "        'teacher_forcing':{\"values\":[0.2,0.5,0.7]}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    wandb.init()\n",
        "\n",
        "    c= wandb.config\n",
        "    name = \"cell_type_\"+str(c.cell_type)+\"_num_layers_\"+str(c.num_layers)+\"_dp_\"+str(c.dropout)+\"_bidir_\"+str(c.bidirectional)+\"_lr_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)\n",
        "    wandb.run.name=name\n",
        "  \n",
        "    # Retrieve the hyperparameters from the config\n",
        "    ct=c.cell_type\n",
        "    dp = c.dropout\n",
        "    em=c.embedding_size\n",
        "    nlayer=c.num_layers\n",
        "    bs = c.batch_size\n",
        "    hs=c.hidden_size\n",
        "    bidir = c.bidirectional\n",
        "    lr = c.learning_rate\n",
        "    opt= c.optim\n",
        "    epochs = 25\n",
        "    tf=c.teacher_forcing\n",
        "    trg_pad_idx=0\n",
        "\n",
        "  \n",
        "\n",
        "    INPUT_DIM = 29\n",
        "    OUTPUT_DIM = 67\n",
        "\n",
        "  \n",
        "  # Load the dataset\n",
        "    train_loader,val_loader,test_loader,idx_to_char=load_data(bs)\n",
        "   \n",
        "  #print(\"data loaded ====================================================\")\n",
        "\n",
        "  # Instantiate the Encoder and Decoder models\n",
        "    encoder = Encoder(INPUT_DIM,em,hs,nlayer,False,ct,dp).to(device)\n",
        "    decoder = Decoder(OUTPUT_DIM,em,hs,nlayer,False,ct,dp).to(device)\n",
        "\n",
        "  # Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "    model = Seq2Seq(encoder,decoder,ct,False).to(device)\n",
        "  #print(\"model ini==============================================================\")\n",
        " \n",
        "  # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()      \n",
        "    if opt == \"adam\":\n",
        "          optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "    elif opt == \"nadam\":\n",
        "          optimizer= optim.NAdam(model.parameters(),lr=lr)\n",
        "  \n",
        "  # Train Network\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "            src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "            trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            #print(\"done\")\n",
        "            optimizer.zero_grad()\n",
        "            #print(\"doe\")\n",
        "            output = model(src,trg,tf)\n",
        "            #print(\"doe\")\n",
        "\n",
        "            # Ignore the first element of the output, which is initialized as all zeros\n",
        "            # since we use it to store the output for the start-of-sequence token\n",
        "            #print(output.shape[2])\n",
        "\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            #print(output.shape)\n",
        "            #print(trg.shape)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print(f\"Epoch: {epoch}, Batch: {batch_idx} , Training..\")\n",
        "        \n",
        "        # Calculate word-level accuracy after every epoch\n",
        "        #train_acc ,train_loss= calculate_word_level_accuracy(model, train_loader,criterion)\n",
        "        val_acc,val_loss = calculate_word_level_accuracy(model,idx_to_char, val_loader, criterion)\n",
        "        test_acc,test_loss = calculate_word_level_accuracy(model,idx_to_char, test_loader, criterion)\n",
        "     \n",
        "    #print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n",
        "\n",
        "            \n",
        "    # Log the metrics to WandB\n",
        "        wandb.log({'epoch': epochs, 'train_loss': loss.item(), 'test_acc': test_acc,'val_acc': val_acc,'test_loss': test_loss,'val_loss': val_loss})\n",
        "    # Save the best model\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhJ1jFFCjBxT",
        "outputId": "3f9fb3fa-bbe5-4149-9cfd-aa74db77419f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6v8czh2i with config:\n"
          ]
        }
      ],
      "source": [
        "# final train\n",
        "# Initialize the WandB sweep\n",
        "#sweep_id = wandb.sweep(sweep_config, project='CS6910_Assignment3')\n",
        "wandb.agent('pgjqjzjn', function=train,count=1, project='CS6910_Assignment3')\n",
        "#wandb.agent(sweep_id, function=train,count=10)\n",
        "#wandb.agent(sweep_id, function=train,count=10)\n",
        "#wandb.agent(sweep_id, function=train,count=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivKTzXOlGmzi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
