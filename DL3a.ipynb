{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uanushkatkd/CS6910_Assignment3/blob/main/DL3a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jg6JcE0kLuD",
        "outputId": "bcd26b29-79ff-43e8-811f-2c3dc17e5ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 11:46:27--  https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.134.113, 74.125.134.100, 74.125.134.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.134.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3kjvs5m9btm0mf59m95v067b65ioc3vv/1683805575000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=705c01b4-6763-4fd7-b519-66cdeb8f9149 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-11 11:46:33--  https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3kjvs5m9btm0mf59m95v067b65ioc3vv/1683805575000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=705c01b4-6763-4fd7-b519-66cdeb8f9149\n",
            "Resolving doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
            "Connecting to doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14030699 (13M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  13.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-11 11:46:33 (132 MB/s) - ‘dataset.zip’ saved [14030699/14030699]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: aksharantar_sampled/\n",
            "   creating: aksharantar_sampled/asm/\n",
            "  inflating: aksharantar_sampled/asm/asm_test.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_train.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_valid.csv  \n",
            "   creating: aksharantar_sampled/ben/\n",
            "  inflating: aksharantar_sampled/ben/ben_test.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_train.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_valid.csv  \n",
            "   creating: aksharantar_sampled/brx/\n",
            "  inflating: aksharantar_sampled/brx/brx_test.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_train.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_valid.csv  \n",
            "   creating: aksharantar_sampled/guj/\n",
            "  inflating: aksharantar_sampled/guj/guj_test.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_train.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_valid.csv  \n",
            "   creating: aksharantar_sampled/hin/\n",
            "  inflating: aksharantar_sampled/hin/hin_test.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_train.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_valid.csv  \n",
            "   creating: aksharantar_sampled/kan/\n",
            "  inflating: aksharantar_sampled/kan/kan_test.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_train.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_valid.csv  \n",
            "   creating: aksharantar_sampled/kas/\n",
            "  inflating: aksharantar_sampled/kas/kas_test.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_train.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_valid.csv  \n",
            "   creating: aksharantar_sampled/kok/\n",
            "  inflating: aksharantar_sampled/kok/kok_test.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_train.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_valid.csv  \n",
            "   creating: aksharantar_sampled/mai/\n",
            "  inflating: aksharantar_sampled/mai/mai_test.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_train.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_valid.csv  \n",
            "   creating: aksharantar_sampled/mal/\n",
            "  inflating: aksharantar_sampled/mal/mal_test.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_train.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_valid.csv  \n",
            "   creating: aksharantar_sampled/mar/\n",
            "  inflating: aksharantar_sampled/mar/mar_test.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_train.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_valid.csv  \n",
            "   creating: aksharantar_sampled/mni/\n",
            "  inflating: aksharantar_sampled/mni/mni_test.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_train.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_valid.csv  \n",
            "   creating: aksharantar_sampled/ori/\n",
            "  inflating: aksharantar_sampled/ori/ori_test.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_train.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_valid.csv  \n",
            "   creating: aksharantar_sampled/pan/\n",
            "  inflating: aksharantar_sampled/pan/pan_test.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_train.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_valid.csv  \n",
            "   creating: aksharantar_sampled/san/\n",
            "  inflating: aksharantar_sampled/san/san_test.csv  \n",
            "  inflating: aksharantar_sampled/san/san_train.csv  \n",
            "  inflating: aksharantar_sampled/san/san_valid.csv  \n",
            "   creating: aksharantar_sampled/sid/\n",
            "  inflating: aksharantar_sampled/sid/sid_test.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_train.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_valid.csv  \n",
            "   creating: aksharantar_sampled/tam/\n",
            "  inflating: aksharantar_sampled/tam/tam_test.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_train.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_valid.csv  \n",
            "   creating: aksharantar_sampled/tel/\n",
            "  inflating: aksharantar_sampled/tel/tel_test.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_train.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_valid.csv  \n",
            "   creating: aksharantar_sampled/urd/\n",
            "  inflating: aksharantar_sampled/urd/urd_test.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_train.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_valid.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw' -O dataset.zip && unzip dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zp0gRaSuyE-",
        "outputId": "52152bbb-c7c7-49c5-a6b1-cf6f2d55074c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "2.0.0+cu118\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.device('cuda:0'))\n",
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_AmhPeAfu4V_"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import (\n",
        "    DataLoader, random_split\n",
        ")  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ayQQaqJSu-3C"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F0DxXx1Qbqzu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, file_path, src_lang, trg_lang):\n",
        "        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n",
        "        self.src_lang = src_lang\n",
        "        self.trg_lang = trg_lang\n",
        "        self.src_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n",
        "        print(self.src_vocab)\n",
        "        self.src_vocab['<pad>'] = 0\n",
        "        self.src_vocab['<unk>'] = 1\n",
        "        self.trg_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n",
        "        \n",
        "\n",
        "\n",
        "        # Extract the unique characters in the source and target languages\n",
        "        src_chars = sorted(set(''.join(self.translations[src_lang])))\n",
        "        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n",
        "\n",
        "        # Assign an index to each character in the source and target languages\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(trg_chars)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "        \n",
        "        self.trg_vocab['<pad>'] = 0\n",
        "        self.trg_vocab['<unk>'] = 1\n",
        "        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])\n",
        "        print(self.max_src_len)\n",
        "        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])\n",
        "        print(\"trg vocab\",len(self.trg_vocab))\n",
        "        print(\"src vocab\",len(self.src_vocab))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.translations)\n",
        "\n",
        "    def target_to_one_hot(self,target_word, char_to_idx):\n",
        "        num_trg_chars = len(char_to_idx)\n",
        "        max_target_len = self.max_trg_len\n",
        "\n",
        "        # Create a tensor of zeros for the one-hot encoding\n",
        "        one_hot = torch.zeros((max_target_len, num_trg_chars))\n",
        "\n",
        "        # Encode each character in the target word as a one-hot vector\n",
        "        for i, char in enumerate(target_word):\n",
        "            char_idx = char_to_idx[char]\n",
        "            one_hot[i][char_idx] = 1\n",
        "\n",
        "        return one_hot\n",
        "    def __getitem__(self, idx):\n",
        "        src_word = self.translations.iloc[idx][self.src_lang]\n",
        "        trg_word = self.translations.iloc[idx][self.trg_lang]\n",
        "        #print(src_word)\n",
        "        #print(trg_word)\n",
        "\n",
        "        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n",
        "        trg = [self.trg_vocab.get(char, self.trg_vocab['<unk>']) for char in trg_word]\n",
        "\n",
        "        src_len = len(src)\n",
        "        trg_len = len(trg)\n",
        "\n",
        "        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n",
        "        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n",
        "\n",
        "        src.extend(src_pad)\n",
        "        trg.extend(trg_pad)\n",
        "\n",
        "        src = torch.LongTensor(src)\n",
        "        trg = torch.LongTensor(trg)\n",
        "        trg_one_hot = self.target_to_one_hot(trg_word,self.char_to_idx)\n",
        "        #print(trg_one_hot.shape)\n",
        "\n",
        "        return src, trg, src_len, trg_len,trg_one_hot\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ilyi5ifgw-gy"
      },
      "outputs": [],
      "source": [
        "# Print the source word, target word, and source length\n",
        "def load_data(batch_size=16):\n",
        "    train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "    val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "    test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "    train_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_train.csv', 'English', 'Devanagari')\n",
        "    val_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_valid.csv', 'English', 'Devanagari')\n",
        "    test_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_test.csv', 'English', 'Devanagari')\n",
        "\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(test_loader):\n",
        "        \n",
        "        print('Source word:',src.shape)\n",
        "        print('Target word:', trg.shape)\n",
        "        print('Target one hot word:', trg_one_hot.shape)\n",
        "\n",
        "        #print('Source length:', src_len)\n",
        "        print('Source length:', trg_len.shape[0])\n",
        "        if batch_idx==0:\n",
        "          break\n",
        "    return train_loader,test_loader,val_loader\n",
        "#print(\"decoder_target_train\",decoder_target_train[1::])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EldcLxiXvmBC"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.embedded_size=embedded_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        if bidirectional:\n",
        "          self.dir=2\n",
        "        else:\n",
        "          self.dir=1  \n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim,embedded_size)\n",
        "        if cell_type == 'rnn':\n",
        "              self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'lstm':\n",
        "              self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "              self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        else:\n",
        "          raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        if self.cell_type == 'lstm':\n",
        "            h_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            c_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            output, (hidden, cell) = self.rnn(embedded, (h_0, c_0))\n",
        "            return output, (hidden, cell)\n",
        "\n",
        "        else:\n",
        "            h_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            output, hidden = self.rnn(embedded, h_0)\n",
        "            return output,hidden\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.embedded_size=embedded_size\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        if bidirectional:\n",
        "          self.dir=2\n",
        "        else:\n",
        "          self.dir=1  \n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim,embedded_size)\n",
        "        \n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "        \n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        #input = input.unsqueeze(0)\n",
        "        #print(\"decoder input shape inside\",input.shape)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        # Concatenate the last hidden state of the encoder from both directions\n",
        "        #print(\"decoder embedded shape inside\",embedded.shape)\n",
        "        #print(\"decoder hidden shape inside\",hidden.shape)\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        #print(\"==============================================\")\n",
        "        \n",
        "        #output = output.squeeze(0)\n",
        "        output = self.fc_out(output)\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder,cell_type,bidirectional):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type=cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        #print(batch_size)\n",
        "        max_len = trg.shape[0]\n",
        "        #print(max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        #print(trg_vocab_size)\n",
        "        #print(\"====================================================\")\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(src)\n",
        "        #print(\"encoder hidden shape\",encoder_hidden.shape)\n",
        "        if bidirectional:\n",
        "          if self.cell_type=='lstm':\n",
        "            hidden_concat = torch.add(encoder_hidden[0][0:self.encoder.num_layers,:,:], encoder_hidden[1][0:self.encoder.num_layers,:,:])/2\n",
        "            cell_concat = torch.add(encoder_hidden[0][self.encoder.num_layers:,:,:], encoder_hidden[1][self.encoder.num_layers:,:,:])/2\n",
        "            hidden_concat = (hidden_concat, cell_concat)\n",
        "\n",
        "          else:\n",
        "              hidden_concat = torch.add(encoder_hidden[0:self.encoder.num_layers,:,:], encoder_hidden[self.encoder.num_layers:,:,:])/2\n",
        "        #hidden_concat = torch.cat((encoder_hidden[0:self.encoder.num_layers,:,:], encoder_hidden[self.encoder.num_layers:,:,:]), dim=2)\n",
        "        #hidden_concat = torch.mean(hidden_concat, dim=2)\n",
        "        else:\n",
        "          hidden_concat= encoder_hidden\n",
        "\n",
        "        #print(\"hidden concat shape\",hidden_concat.shape)\n",
        "\n",
        "        #print(\"=====================================================\")\n",
        "        decoder_hidden = hidden_concat\n",
        "        \n",
        "        decoder_input = (trg[0,:]).unsqueeze(0)\n",
        "        #print(\"decoder input shape\",decoder_input.shape)\n",
        "        \n",
        "        for t in range(1,trg.shape[1] ):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs[t-1] = decoder_output\n",
        "            max_pr, idx=torch.max(decoder_output,dim=2)\n",
        "            #print(\"trg shape\",trg.shape)\n",
        "            idx=idx.view(trg.shape[1])\n",
        "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
        "            if teacher_force:\n",
        "              decoder_input= trg[t,:].unsqueeze(0)\n",
        "            else:\n",
        "              decoder_input= idx.unsqueeze(0)\n",
        "         \n",
        "        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "        outputs[-1] = decoder_output\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_level_accuracy(model, data_loader, trg_pad_idx):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(data_loader):\n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "            \n",
        "            #print(\"op before\",output.shape)\n",
        "            \n",
        "            # Ignore the first element of the output, which is initialized as all zeros\n",
        "            # since we use it to store the output for the start-of-sequence token\n",
        "            output = output[0:].reshape(-1, output.shape[2])\n",
        "            #print(output.shape)\n",
        "            \n",
        "            trg = trg[0:].reshape(-1)\n",
        "            #print(trg.shape)\n",
        "        \n",
        "\n",
        "            # Convert the output to predicted indices\n",
        "            predicted_indices = output.argmax(dim=1)\n",
        "            #print(predicted_indices.shape)\n",
        "                        \n",
        "            # Convert the predicted indices and target tensor to a batch_size x seq_length shape\n",
        "            batch_size = trg_len.shape[0]\n",
        "            seq_length = int(trg.numel() / batch_size)\n",
        "            predicted_indices = predicted_indices.reshape(batch_size, seq_length)\n",
        "            trg = trg.reshape(batch_size, seq_length)\n",
        "\n",
        "            # Calculate the number of correct predictions in this batch\n",
        "\n",
        "            for idx in range(batch_size):\n",
        "              flag=True\n",
        "              num_total+=1\n",
        "              for j in range(1,seq_length):\n",
        "                \n",
        "                if trg[idx,j]!=0 and (predicted_indices[idx,j] != trg[idx,j]):\n",
        "                  flag = False\n",
        "                  break\n",
        "              if flag:\n",
        "                num_correct+=1\n",
        "              \n",
        "            '''correct = (predicted_indices == trg)\n",
        "            num_correct += correct.sum().item()\n",
        "            num_total += trg.numel()\n",
        "'''\n",
        "\n",
        "   \n",
        "    return (num_correct / num_total)*100\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hjrdi3k_zJV1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from signal import signal,SIGPIPE, SIG_DFL\n",
        "signal(SIGPIPE,SIG_DFL)\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "!wandb login da816d14625ef44d200ee4acaa517646962e6f9a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqEC39yFOBNe",
        "outputId": "d47fafc9-b798-4c81-f753-0cb3360f95d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JM2CzcISvpyK"
      },
      "outputs": [],
      "source": [
        "# wandb sweeps\n",
        "\n",
        "sweep_config= {\n",
        "    \"name\" : \"CS6910_Assignment_3\",\n",
        "    \"method\" : \"bayes\",\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'cell_type' : { 'values' : ['lstm','gru','rnn'] },\n",
        "        'dropout' : { 'values' : [0,0.1,0.2]},\n",
        "        'embedding_size' : {'values' : [64,128]},\n",
        "        'num_layers' : {'values' : [1,2,3]},\n",
        "        'batch_size' : {'values' : [16,32,64]},\n",
        "        'hidden_size' : {'values' : [128,256,512]},\n",
        "        'bidirectional' : {'values' : [True ,False]},\n",
        "        'learning_rate':{\n",
        "            \"values\": [0.001,0.01,0.02,0.002]\n",
        "        },\n",
        "        'optim':{\n",
        "            \"values\": ['adam','nadam']\n",
        "        },\n",
        "        'epoch':{\n",
        "            \"values\": [5,10]\n",
        "        },\n",
        "        'teacher_forcing':{\"values\":[0.2,0.5,0.7]}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "  wandb.init()\n",
        "\n",
        "  c= wandb.config\n",
        "  name = \"cell_type_\"+str(c.cell_type)+\"num_layers_\"+str(c.num_layers)+\"dp\"+str(c.dropout)+\"bidir_\"+str(c.bidirectional)+\"lr_\"+str(c.learning_rate)+\"bs_\"+str(c.batch_size)\n",
        "  wandb.run.name=name\n",
        "  \n",
        "  # Retrieve the hyperparameters from the config\n",
        "  ct=c.cell_type\n",
        "  dp = c.dropout\n",
        "  em=c.embedding_size\n",
        "  nlayer=c.num_layers\n",
        "  bs = c.batch_size\n",
        "  hs=c.hidden_size\n",
        "  bidir = c.bidirectional\n",
        "  lr = c.learning_rate\n",
        "  opt= c.optim\n",
        "  epochs = c.epoch\n",
        "  tf=0.5\n",
        "  trg_pad_idx=0\n",
        "\n",
        "  \n",
        "\n",
        "  INPUT_DIM = 28\n",
        "  OUTPUT_DIM = 66\n",
        "\n",
        "  \n",
        "  # Load the dataset\n",
        "  train_loader,val_loader,test_loader=load_data(bs)\n",
        "   \n",
        "  print(\"data loaded ====================================================\")\n",
        "\n",
        "  # Instantiate the Encoder and Decoder models\n",
        "  encoder = Encoder(INPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)\n",
        "  decoder = Decoder(OUTPUT_DIM,em,hs,nlayer,bidir,ct,dp).to(device)\n",
        "\n",
        "  # Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "  model = Seq2Seq(encoder,decoder,ct).to(device)\n",
        "  print(\"model ini==============================================================\")\n",
        " \n",
        "  # Define the loss function and optimizer\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)      \n",
        "  if opt == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "  elif opt == \"nadam\":\n",
        "        optimizer= optim.NAdam(model.parameters(),lr=lr)\n",
        "  \n",
        "  # Train Network\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        print(\"done\")\n",
        "        optimizer.zero_grad()\n",
        "        print(\"doe\")\n",
        "        output = model(src,trg,tf)\n",
        "        print(\"doe\")\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[0:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[0:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate word-level accuracy after every epoch\n",
        "    train_acc = calculate_word_level_accuracy(model, train_loader, trg_pad_idx)\n",
        "    val_acc = calculate_word_level_accuracy(model, val_loader, trg_pad_idx)\n",
        "    test_acc = calculate_word_level_accuracy(model, test_loader, trg_pad_idx)\n",
        "    \n",
        "    #print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n",
        "\n",
        "            \n",
        "    # Log the metrics to WandB\n",
        "    wandb.log({'epoch': epochs, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n",
        "            \n",
        "        \n",
        "  # Save the best model\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "  return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uplKL9EnvwhV"
      },
      "outputs": [],
      "source": [
        "# final train\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='CS6910_Assignment_3')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "embedding_size=256\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.2\n",
        "EPOCHS = 5\n",
        "trg_pad_idx=0\n",
        "dropout=0.1\n",
        "bidirectional=False\n",
        "opt='adam'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader,test_loader,val_loader=load_data(BATCH_SIZE)\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder,CELL_TYPE,bidirectional).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.NAdam(model.parameters(), lr=LEARNING_RATE)\n",
        "#optimizer=optimizer(model,opt,LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[0:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[0:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate word-level accuracy after every epoch\n",
        "    train_acc = calculate_word_level_accuracy(model, train_loader, trg_pad_idx)\n",
        "    val_acc = calculate_word_level_accuracy(model, val_loader, trg_pad_idx)\n",
        "    test_acc = calculate_word_level_accuracy(model, test_loader, trg_pad_idx)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n",
        "    #wandb.log({'epoch': epoch, 'loss': loss.item(), 'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FADAqsivPyYA",
        "outputId": "45b2152f-fbbc-4bb7-deb2-3479c5b4905c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "24\n",
            "trg vocab 66\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "22\n",
            "trg vocab 63\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "26\n",
            "trg vocab 65\n",
            "src vocab 28\n",
            "Source word: torch.Size([16, 26])\n",
            "Target word: torch.Size([16, 20])\n",
            "Target one hot word: torch.Size([16, 20, 63])\n",
            "Source length: 16\n",
            "Epoch: 0, Batch: 0, Loss: 4.197621822357178\n",
            "Epoch: 0, Batch: 1000, Loss: 2.759524345397949\n",
            "Epoch: 0, Batch: 2000, Loss: 2.338613986968994\n",
            "Epoch: 0, Batch: 3000, Loss: 2.3351948261260986\n",
            "Epoch: 0, Loss: 2.545414996594191, Train Acc: 34.908203125, Val Acc: 42.0654296875\n",
            "Epoch: 1, Batch: 0, Loss: 2.5528223514556885\n",
            "Epoch: 1, Batch: 1000, Loss: 2.7180795669555664\n",
            "Epoch: 1, Batch: 2000, Loss: 2.1085007190704346\n",
            "Epoch: 1, Batch: 3000, Loss: 2.38554048538208\n",
            "Epoch: 1, Loss: 2.404137489311397, Train Acc: 34.796875, Val Acc: 42.1142578125\n",
            "Epoch: 2, Batch: 0, Loss: 2.4322779178619385\n",
            "Epoch: 2, Batch: 1000, Loss: 2.3356451988220215\n",
            "Epoch: 2, Batch: 2000, Loss: 2.5128893852233887\n",
            "Epoch: 2, Batch: 3000, Loss: 2.680274724960327\n",
            "Epoch: 2, Loss: 2.4015589091554284, Train Acc: 34.833984375, Val Acc: 42.138671875\n",
            "Epoch: 3, Batch: 0, Loss: 2.774214744567871\n",
            "Epoch: 3, Batch: 1000, Loss: 2.492658853530884\n",
            "Epoch: 3, Batch: 2000, Loss: 2.572868824005127\n",
            "Epoch: 3, Batch: 3000, Loss: 2.2292516231536865\n",
            "Epoch: 3, Loss: 2.3932695846259593, Train Acc: 34.953125, Val Acc: 42.08984375\n",
            "Epoch: 4, Batch: 0, Loss: 2.2526373863220215\n",
            "Epoch: 4, Batch: 1000, Loss: 2.380783796310425\n",
            "Epoch: 4, Batch: 2000, Loss: 2.455293655395508\n",
            "Epoch: 4, Batch: 3000, Loss: 1.9714934825897217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxByKGy-vyYZ"
      },
      "outputs": [],
      "source": [
        "'''# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "EPOCHS = 10\n",
        "trg_pad_idx=0\n",
        "\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aDR-jUyvbRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "319db677-6554-4430-f3e9-6a4b4e29aa61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Dataset load and Preprocessing\\ndef data_preprocess(path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   english_texts = []\\n   target_texts = []\\n   for index, row in df.iterrows():\\n        ip_text = row[\\'English\\']\\n        op_text = row[\\'Translated\\']\\n        if ip_text == \\'\\' or op_text == \\'\\':\\n            continue\\n        op_text = \"\\t\" + op_text + \"\\n\"\\n        ip_text = \"\\t\" + ip_text + \"\\n\"\\n        \\n        english_texts.append(ip_text)\\n        target_texts.append(op_text)\\n   MAX_LEN_input = max([len(txt) for txt in english_texts])\\n   MAX_LEN_target = max([len(txt) for txt in target_texts])\\n\\n   hindi_vocab = set()\\n   english_vocab = set()\\n\\n   for word in target_texts:\\n     for char in word:\\n       hindi_vocab.add(char)\\n\\n   for word in english_texts:\\n      for char in word:\\n        english_vocab.add(char)\\n\\n   hindi_list = sorted(list(hindi_vocab))\\n   english_list = sorted(list(english_vocab))\\n\\n\\n   encoder_tokens = len(english_list)\\n   decoder_tokens = len(hindi_list)\\n\\n   print(\"###############################\",decoder_tokens)\\n   print(\"###############################\",encoder_tokens)\\n\\n\\n        # Dict for char to index\\n   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\\n   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\\n   target_token_index[\"UNK\"] = decoder_tokens\\n\\n   #print(input_token_index)\\n\\n        # Dict for index to char\\n   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\\n   #print(inv_input_token_index)\\n   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\\n   decoder_tokens+=1\\n   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\\n\\ndef get_data(path,train_path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\\n   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\\n\\n   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\\n   print(decoder_target_data.shape)\\n   for i,target_text in enumerate(df[\"Translated\"]):\\n            #print(\"Ths is i\",i,target_text)\\n\\n            target_text = \\'\\t\\'+target_text+\\'\\n\\'  \\n            for t, char in enumerate(target_text):\\n              #print(t,char)\\n              if t > 0:\\n                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\\n            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\\n   print(\"done\")\\n   return ([[input_token_index[letter] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"English\"]]),   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"Translated\"]]),decoder_target_data\\n    \\n\\ndef load_data():\\n    \\n  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\\n  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\\n  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\\n\\n\\n  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\\n  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\\n  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\\n\\n\\n  encoder_train = [torch.tensor(lst) for lst in encoder_train]\\n  decoder_train = [torch.tensor(lst) for lst in decoder_train]\\n\\n  encoder_val = [torch.tensor(lst) for lst in encoder_val]\\n  decoder_val = [torch.tensor(lst) for lst in decoder_val]\\n  \\n  encoder_test = [torch.tensor(lst) for lst in encoder_test]\\n  decoder_test = [torch.tensor(lst) for lst in decoder_test]\\n\\n  # Pad the list of tensors to make them the same length\\n  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\\n  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\\n\\n  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\\n  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\\n\\n  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\\n  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\\n  \\n  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "'''# Dataset load and Preprocessing\n",
        "def data_preprocess(path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   english_texts = []\n",
        "   target_texts = []\n",
        "   for index, row in df.iterrows():\n",
        "        ip_text = row['English']\n",
        "        op_text = row['Translated']\n",
        "        if ip_text == '' or op_text == '':\n",
        "            continue\n",
        "        op_text = \"\\t\" + op_text + \"\\n\"\n",
        "        ip_text = \"\\t\" + ip_text + \"\\n\"\n",
        "        \n",
        "        english_texts.append(ip_text)\n",
        "        target_texts.append(op_text)\n",
        "   MAX_LEN_input = max([len(txt) for txt in english_texts])\n",
        "   MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "   hindi_vocab = set()\n",
        "   english_vocab = set()\n",
        "\n",
        "   for word in target_texts:\n",
        "     for char in word:\n",
        "       hindi_vocab.add(char)\n",
        "\n",
        "   for word in english_texts:\n",
        "      for char in word:\n",
        "        english_vocab.add(char)\n",
        "\n",
        "   hindi_list = sorted(list(hindi_vocab))\n",
        "   english_list = sorted(list(english_vocab))\n",
        "\n",
        "\n",
        "   encoder_tokens = len(english_list)\n",
        "   decoder_tokens = len(hindi_list)\n",
        "\n",
        "   print(\"###############################\",decoder_tokens)\n",
        "   print(\"###############################\",encoder_tokens)\n",
        "\n",
        "\n",
        "        # Dict for char to index\n",
        "   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\n",
        "   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\n",
        "   target_token_index[\"UNK\"] = decoder_tokens\n",
        "\n",
        "   #print(input_token_index)\n",
        "\n",
        "        # Dict for index to char\n",
        "   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\n",
        "   #print(inv_input_token_index)\n",
        "   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\n",
        "   decoder_tokens+=1\n",
        "   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\n",
        "\n",
        "def get_data(path,train_path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\n",
        "   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\n",
        "\n",
        "   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\n",
        "   print(decoder_target_data.shape)\n",
        "   for i,target_text in enumerate(df[\"Translated\"]):\n",
        "            #print(\"Ths is i\",i,target_text)\n",
        "\n",
        "            target_text = '\\t'+target_text+'\\n'  \n",
        "            for t, char in enumerate(target_text):\n",
        "              #print(t,char)\n",
        "              if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\n",
        "            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\n",
        "   print(\"done\")\n",
        "   return ([[input_token_index[letter] for letter in list('\\t'+word+'\\n')] for word in df[\"English\"]]),\\\n",
        "   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list('\\t'+word+'\\n')] for word in df[\"Translated\"]]),decoder_target_data\n",
        "    \n",
        "\n",
        "def load_data():\n",
        "    \n",
        "  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "\n",
        "  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\n",
        "  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\n",
        "  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\n",
        "\n",
        "\n",
        "  encoder_train = [torch.tensor(lst) for lst in encoder_train]\n",
        "  decoder_train = [torch.tensor(lst) for lst in decoder_train]\n",
        "\n",
        "  encoder_val = [torch.tensor(lst) for lst in encoder_val]\n",
        "  decoder_val = [torch.tensor(lst) for lst in decoder_val]\n",
        "  \n",
        "  encoder_test = [torch.tensor(lst) for lst in encoder_test]\n",
        "  decoder_test = [torch.tensor(lst) for lst in decoder_test]\n",
        "\n",
        "  # Pad the list of tensors to make them the same length\n",
        "  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\n",
        "  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\n",
        "  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\n",
        "  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\n",
        "  \n",
        "  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fs2dax9v-SFC",
        "outputId": "1abe538e-9799-42e4-97a4-f2397331b4c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\\n\\nprint(\"##################################################################################################\")\\nprint(\"encoder_input\",encoder_train.shape)\\nprint(\"encoder_input\",encoder_train[0])\\n\\nprint(\"decoder_target\",decoder_train.shape)\\nprint(\"encoder_input\",decoder_train[0])\\n\\n\\nprint(\"decoder_target_train\",decoder_target_train[0])\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\n",
        "\n",
        "print(\"##################################################################################################\")\n",
        "print(\"encoder_input\",encoder_train.shape)\n",
        "print(\"encoder_input\",encoder_train[0])\n",
        "\n",
        "print(\"decoder_target\",decoder_train.shape)\n",
        "print(\"encoder_input\",decoder_train[0])\n",
        "\n",
        "\n",
        "print(\"decoder_target_train\",decoder_target_train[0])\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}