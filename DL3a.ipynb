{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uanushkatkd/CS6910_Assignment3/blob/main/DL3a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jg6JcE0kLuD",
        "outputId": "635de242-2b8e-4963-8b25-242c3b52c412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 17:11:32--  https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.182.138, 64.233.182.113, 64.233.182.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.182.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usdkj3i9tuf2jpr1e3mgluc6ff5djfis/1683738675000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=55cb2f6b-ca6a-4900-a9e6-b44a2460eebe [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-10 17:11:39--  https://doc-04-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/usdkj3i9tuf2jpr1e3mgluc6ff5djfis/1683738675000/15657800450702375309/*/1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw?e=download&uuid=55cb2f6b-ca6a-4900-a9e6-b44a2460eebe\n",
            "Resolving doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)... 142.250.1.132, 2607:f8b0:4001:c24::84\n",
            "Connecting to doc-04-1k-docs.googleusercontent.com (doc-04-1k-docs.googleusercontent.com)|142.250.1.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14030699 (13M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  13.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-10 17:11:39 (108 MB/s) - ‘dataset.zip’ saved [14030699/14030699]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "   creating: aksharantar_sampled/\n",
            "   creating: aksharantar_sampled/asm/\n",
            "  inflating: aksharantar_sampled/asm/asm_test.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_train.csv  \n",
            "  inflating: aksharantar_sampled/asm/asm_valid.csv  \n",
            "   creating: aksharantar_sampled/ben/\n",
            "  inflating: aksharantar_sampled/ben/ben_test.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_train.csv  \n",
            "  inflating: aksharantar_sampled/ben/ben_valid.csv  \n",
            "   creating: aksharantar_sampled/brx/\n",
            "  inflating: aksharantar_sampled/brx/brx_test.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_train.csv  \n",
            "  inflating: aksharantar_sampled/brx/brx_valid.csv  \n",
            "   creating: aksharantar_sampled/guj/\n",
            "  inflating: aksharantar_sampled/guj/guj_test.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_train.csv  \n",
            "  inflating: aksharantar_sampled/guj/guj_valid.csv  \n",
            "   creating: aksharantar_sampled/hin/\n",
            "  inflating: aksharantar_sampled/hin/hin_test.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_train.csv  \n",
            "  inflating: aksharantar_sampled/hin/hin_valid.csv  \n",
            "   creating: aksharantar_sampled/kan/\n",
            "  inflating: aksharantar_sampled/kan/kan_test.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_train.csv  \n",
            "  inflating: aksharantar_sampled/kan/kan_valid.csv  \n",
            "   creating: aksharantar_sampled/kas/\n",
            "  inflating: aksharantar_sampled/kas/kas_test.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_train.csv  \n",
            "  inflating: aksharantar_sampled/kas/kas_valid.csv  \n",
            "   creating: aksharantar_sampled/kok/\n",
            "  inflating: aksharantar_sampled/kok/kok_test.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_train.csv  \n",
            "  inflating: aksharantar_sampled/kok/kok_valid.csv  \n",
            "   creating: aksharantar_sampled/mai/\n",
            "  inflating: aksharantar_sampled/mai/mai_test.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_train.csv  \n",
            "  inflating: aksharantar_sampled/mai/mai_valid.csv  \n",
            "   creating: aksharantar_sampled/mal/\n",
            "  inflating: aksharantar_sampled/mal/mal_test.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_train.csv  \n",
            "  inflating: aksharantar_sampled/mal/mal_valid.csv  \n",
            "   creating: aksharantar_sampled/mar/\n",
            "  inflating: aksharantar_sampled/mar/mar_test.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_train.csv  \n",
            "  inflating: aksharantar_sampled/mar/mar_valid.csv  \n",
            "   creating: aksharantar_sampled/mni/\n",
            "  inflating: aksharantar_sampled/mni/mni_test.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_train.csv  \n",
            "  inflating: aksharantar_sampled/mni/mni_valid.csv  \n",
            "   creating: aksharantar_sampled/ori/\n",
            "  inflating: aksharantar_sampled/ori/ori_test.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_train.csv  \n",
            "  inflating: aksharantar_sampled/ori/ori_valid.csv  \n",
            "   creating: aksharantar_sampled/pan/\n",
            "  inflating: aksharantar_sampled/pan/pan_test.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_train.csv  \n",
            "  inflating: aksharantar_sampled/pan/pan_valid.csv  \n",
            "   creating: aksharantar_sampled/san/\n",
            "  inflating: aksharantar_sampled/san/san_test.csv  \n",
            "  inflating: aksharantar_sampled/san/san_train.csv  \n",
            "  inflating: aksharantar_sampled/san/san_valid.csv  \n",
            "   creating: aksharantar_sampled/sid/\n",
            "  inflating: aksharantar_sampled/sid/sid_test.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_train.csv  \n",
            "  inflating: aksharantar_sampled/sid/sid_valid.csv  \n",
            "   creating: aksharantar_sampled/tam/\n",
            "  inflating: aksharantar_sampled/tam/tam_test.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_train.csv  \n",
            "  inflating: aksharantar_sampled/tam/tam_valid.csv  \n",
            "   creating: aksharantar_sampled/tel/\n",
            "  inflating: aksharantar_sampled/tel/tel_test.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_train.csv  \n",
            "  inflating: aksharantar_sampled/tel/tel_valid.csv  \n",
            "   creating: aksharantar_sampled/urd/\n",
            "  inflating: aksharantar_sampled/urd/urd_test.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_train.csv  \n",
            "  inflating: aksharantar_sampled/urd/urd_valid.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw' -O dataset.zip && unzip dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zp0gRaSuyE-",
        "outputId": "f741e9fb-665a-400d-f57d-788d3c11d231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "2.0.0+cu118\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.device('cuda:0'))\n",
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_AmhPeAfu4V_"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import (\n",
        "    DataLoader, random_split\n",
        ")  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ayQQaqJSu-3C"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0DxXx1Qbqzu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, file_path, src_lang, trg_lang):\n",
        "        self.translations = pd.read_csv(file_path, header=None, names=[src_lang, trg_lang])\n",
        "        self.src_lang = src_lang\n",
        "        self.trg_lang = trg_lang\n",
        "        self.src_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[src_lang].tolist())))))}\n",
        "        print(self.src_vocab)\n",
        "        self.src_vocab['<pad>'] = 0\n",
        "        self.src_vocab['<unk>'] = 1\n",
        "        self.trg_vocab = {char: i+2 for i, char in enumerate(sorted(list(set(''.join(self.translations[trg_lang].tolist())))))}\n",
        "        \n",
        "\n",
        "\n",
        "        # Extract the unique characters in the source and target languages\n",
        "        src_chars = sorted(set(''.join(self.translations[src_lang])))\n",
        "        trg_chars = sorted(set(''.join(self.translations[trg_lang])))\n",
        "\n",
        "        # Assign an index to each character in the source and target languages\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(trg_chars)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "        \n",
        "        self.trg_vocab['<pad>'] = 0\n",
        "        self.trg_vocab['<unk>'] = 1\n",
        "        self.max_src_len = max([len(word) for word in self.translations[src_lang].tolist()])\n",
        "        print(self.max_src_len)\n",
        "        self.max_trg_len = max([len(word) for word in self.translations[trg_lang].tolist()])\n",
        "        print(\"trg vocab\",len(self.trg_vocab))\n",
        "        print(\"src vocab\",len(self.src_vocab))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.translations)\n",
        "\n",
        "    def target_to_one_hot(self,target_word, char_to_idx):\n",
        "        num_trg_chars = len(char_to_idx)\n",
        "        max_target_len = self.max_trg_len\n",
        "\n",
        "        # Create a tensor of zeros for the one-hot encoding\n",
        "        one_hot = torch.zeros((max_target_len, num_trg_chars))\n",
        "\n",
        "        # Encode each character in the target word as a one-hot vector\n",
        "        for i, char in enumerate(target_word):\n",
        "            char_idx = char_to_idx[char]\n",
        "            one_hot[i][char_idx] = 1\n",
        "\n",
        "        return one_hot\n",
        "    def __getitem__(self, idx):\n",
        "        src_word = self.translations.iloc[idx][self.src_lang]\n",
        "        trg_word = self.translations.iloc[idx][self.trg_lang]\n",
        "        #print(src_word)\n",
        "        #print(trg_word)\n",
        "\n",
        "        src = [self.src_vocab.get(char, self.src_vocab['<unk>']) for char in src_word]\n",
        "        trg = [self.trg_vocab.get(char, self.trg_vocab['<unk>']) for char in trg_word]\n",
        "\n",
        "        src_len = len(src)\n",
        "        trg_len = len(trg)\n",
        "\n",
        "        src_pad = [self.src_vocab['<pad>']] * (self.max_src_len - src_len)\n",
        "        trg_pad = [self.trg_vocab['<pad>']] * (self.max_trg_len - trg_len)\n",
        "\n",
        "        src.extend(src_pad)\n",
        "        trg.extend(trg_pad)\n",
        "\n",
        "        src = torch.LongTensor(src)\n",
        "        trg = torch.LongTensor(trg)\n",
        "        trg_one_hot = self.target_to_one_hot(trg_word,self.char_to_idx)\n",
        "        #print(trg_one_hot.shape)\n",
        "\n",
        "        return src, trg, src_len, trg_len,trg_one_hot\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilyi5ifgw-gy",
        "outputId": "b563521e-3b81-49f4-97bc-8e3626da9c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "24\n",
            "trg vocab 66\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "22\n",
            "trg vocab 63\n",
            "src vocab 28\n",
            "{'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "26\n",
            "trg vocab 65\n",
            "src vocab 28\n",
            "Source word: torch.Size([16, 26])\n",
            "Target word: torch.Size([16, 20])\n",
            "Target one hot word: torch.Size([16, 20, 63])\n",
            "Source length: 16\n"
          ]
        }
      ],
      "source": [
        "# Print the source word, target word, and source length\n",
        "\n",
        "train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "train_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_train.csv', 'English', 'Devanagari')\n",
        "val_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_valid.csv', 'English', 'Devanagari')\n",
        "test_dataset = TransliterationDataset('/content/aksharantar_sampled/hin/hin_test.csv', 'English', 'Devanagari')\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(test_loader):\n",
        "    \n",
        "    print('Source word:',src.shape)\n",
        "    print('Target word:', trg.shape)\n",
        "    print('Target one hot word:', trg_one_hot.shape)\n",
        "\n",
        "    #print('Source length:', src_len)\n",
        "    print('Source length:', trg_len.shape[0])\n",
        "    if batch_idx==0:\n",
        "      break\n",
        "\n",
        "#print(\"decoder_target_train\",decoder_target_train[1::])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EldcLxiXvmBC"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedded_size,hidden_dim, num_layers,bidirectional, cell_type,dp):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.embedded_size=embedded_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        if bidirectional:\n",
        "          self.dir=2\n",
        "        else:\n",
        "          self.dir=1  \n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim,embedded_size)\n",
        "        if cell_type == 'rnn':\n",
        "              self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'lstm':\n",
        "              self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        elif cell_type == 'gru':\n",
        "              self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers, dropout=dp,bidirectional=bidirectional)\n",
        "        else:\n",
        "          raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        if self.cell_type == 'lstm':\n",
        "            h_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            c_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            output, (hidden, cell) = self.rnn(embedded, (h_0, c_0))\n",
        "            return output, (hidden, cell)\n",
        "\n",
        "        else:\n",
        "            h_0 = torch.zeros(self.num_layers * self.dir, embedded.size(1), self.hidden_dim).to(embedded.device)\n",
        "            output, hidden = self.rnn(embedded, h_0)\n",
        "            return output,hidden\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim,embedded_size, hidden_dim, num_layers,bidirectional,cell_type,dp):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.embedded_size=embedded_size\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional=bidirectional\n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        if bidirectional:\n",
        "          self.dir=2\n",
        "        else:\n",
        "          self.dir=1  \n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim,embedded_size)\n",
        "        \n",
        "        if cell_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(embedded_size, hidden_dim, num_layers,dropout=dp)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid cell type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "        \n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        #input = input.unsqueeze(0)\n",
        "        #print(\"decoder input shape inside\",input.shape)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        # Concatenate the last hidden state of the encoder from both directions\n",
        "        #print(\"decoder embedded shape inside\",embedded.shape)\n",
        "        #print(\"decoder hidden shape inside\",hidden.shape)\n",
        "        \n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        #print(\"==============================================\")\n",
        "        \n",
        "        #output = output.squeeze(0)\n",
        "        output = self.fc_out(output)\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        #print(batch_size)\n",
        "        max_len = trg.shape[0]\n",
        "        #print(max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        #print(trg_vocab_size)\n",
        "        #print(\"====================================================\")\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(src)\n",
        "        #print(\"encoder hidden shape\",encoder_hidden.shape)\n",
        "        hidden_concat = torch.add(encoder_hidden[0:self.encoder.num_layers,:,:], encoder_hidden[self.encoder.num_layers:,:,:])/2\n",
        "        \n",
        "        #print(\"hidden concat shape\",hidden_concat.shape)\n",
        "\n",
        "        #print(\"=====================================================\")\n",
        "        decoder_hidden = hidden_concat\n",
        "        \n",
        "        decoder_input = (trg[0,:]).unsqueeze(0)\n",
        "        #print(\"decoder input shape\",decoder_input.shape)\n",
        "        \n",
        "        for t in range(1,trg.shape[1] ):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            outputs[t-1] = decoder_output\n",
        "            max_pr, idx=torch.max(decoder_output,dim=2)\n",
        "            #print(\"trg shape\",trg.shape)\n",
        "            idx=idx.view(trg.shape[1])\n",
        "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
        "            if teacher_force:\n",
        "              decoder_input= trg[t,:].unsqueeze(0)\n",
        "            else:\n",
        "              decoder_input= idx.unsqueeze(0)\n",
        "         \n",
        "        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "        outputs[-1] = decoder_output\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_level_accuracy(model, data_loader, trg_pad_idx):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(data_loader):\n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "            \n",
        "            #print(\"op before\",output.shape)\n",
        "            \n",
        "            # Ignore the first element of the output, which is initialized as all zeros\n",
        "            # since we use it to store the output for the start-of-sequence token\n",
        "            output = output[0:].reshape(-1, output.shape[2])\n",
        "            #print(output.shape)\n",
        "            \n",
        "            trg = trg[0:].reshape(-1)\n",
        "            #print(trg.shape)\n",
        "        \n",
        "\n",
        "            # Convert the output to predicted indices\n",
        "            predicted_indices = output.argmax(dim=1)\n",
        "            #print(predicted_indices.shape)\n",
        "                        \n",
        "            # Convert the predicted indices and target tensor to a batch_size x seq_length shape\n",
        "            batch_size = trg_len.shape[0]\n",
        "            seq_length = int(trg.numel() / batch_size)\n",
        "            predicted_indices = predicted_indices.reshape(batch_size, seq_length)\n",
        "            trg = trg.reshape(batch_size, seq_length)\n",
        "\n",
        "            # Calculate the number of correct predictions in this batch\n",
        "\n",
        "            for idx in range(batch_size):\n",
        "              flag=True\n",
        "              num_total+=1\n",
        "              for j in range(1,seq_length):\n",
        "                \n",
        "                if trg[idx,j]!=0 and (predicted_indices[idx,j] != trg[idx,j]):\n",
        "                  flag = False\n",
        "                  break\n",
        "              if flag:\n",
        "                num_correct+=1\n",
        "              \n",
        "            '''correct = (predicted_indices == trg)\n",
        "            num_correct += correct.sum().item()\n",
        "            num_total += trg.numel()\n",
        "'''\n",
        "\n",
        "   \n",
        "    return (num_correct / num_total)*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "embedding_size=256\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "EPOCHS = 5\n",
        "trg_pad_idx=0\n",
        "dropout=0.2\n",
        "bidirectional=True\n",
        "\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional, CELL_TYPE,dropout).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM,embedding_size,HIDDEN_DIM, NUM_LAYERS,bidirectional,CELL_TYPE,dropout).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.NAdam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[0:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[0:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate word-level accuracy after every epoch\n",
        "    train_acc = calculate_word_level_accuracy(model, train_loader, trg_pad_idx)\n",
        "    val_acc = calculate_word_level_accuracy(model, val_loader, trg_pad_idx)\n",
        "    print(f\"Epoch: {epoch}, Loss: {epoch_loss / len(train_loader)}, Train Acc: {train_acc}, Val Acc: {val_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jar5zHc1Dsv1",
        "outputId": "b33a8ced-3033-4fa9-e43c-fc9aaf6b3674"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 4.195296287536621\n",
            "Epoch: 0, Batch: 1000, Loss: 2.0312135219573975\n",
            "Epoch: 0, Batch: 2000, Loss: 1.8221125602722168\n",
            "Epoch: 0, Batch: 3000, Loss: 2.002939462661743\n",
            "Epoch: 0, Loss: 1.9744490771740675, Train Acc: 34.880859375, Val Acc: 41.9677734375\n",
            "Epoch: 1, Batch: 0, Loss: 2.4304134845733643\n",
            "Epoch: 1, Batch: 1000, Loss: 1.5487637519836426\n",
            "Epoch: 1, Batch: 2000, Loss: 1.7434154748916626\n",
            "Epoch: 1, Batch: 3000, Loss: 2.139113426208496\n",
            "Epoch: 1, Loss: 1.853368640113622, Train Acc: 34.890625, Val Acc: 42.1630859375\n",
            "Epoch: 2, Batch: 0, Loss: 1.9405092000961304\n",
            "Epoch: 2, Batch: 1000, Loss: 1.6377360820770264\n",
            "Epoch: 2, Batch: 2000, Loss: 1.7067900896072388\n",
            "Epoch: 2, Batch: 3000, Loss: 1.1528807878494263\n",
            "Epoch: 2, Loss: 1.8346574638970197, Train Acc: 34.748046875, Val Acc: 42.1630859375\n",
            "Epoch: 3, Batch: 0, Loss: 2.5232224464416504\n",
            "Epoch: 3, Batch: 1000, Loss: 2.491786003112793\n",
            "Epoch: 3, Batch: 2000, Loss: 2.091320037841797\n",
            "Epoch: 3, Batch: 3000, Loss: 1.1976791620254517\n",
            "Epoch: 3, Loss: 1.8085012224689125, Train Acc: 34.8515625, Val Acc: 42.1142578125\n",
            "Epoch: 4, Batch: 0, Loss: 1.5667555332183838\n",
            "Epoch: 4, Batch: 1000, Loss: 1.7582859992980957\n",
            "Epoch: 4, Batch: 2000, Loss: 1.6538785696029663\n",
            "Epoch: 4, Batch: 3000, Loss: 2.40010404586792\n",
            "Epoch: 4, Loss: 1.8258165014814585, Train Acc: 34.751953125, Val Acc: 42.138671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM2CzcISvpyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43cca3d-d68d-46d2-eac8-f0f22fda8fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 4.1905717849731445\n",
            "Epoch: 1, Batch: 0, Loss: 1.9107811450958252\n",
            "Epoch: 2, Batch: 0, Loss: 1.8196845054626465\n",
            "Epoch: 3, Batch: 0, Loss: 1.6872750520706177\n",
            "Epoch: 4, Batch: 0, Loss: 1.5151541233062744\n",
            "Epoch: 5, Batch: 0, Loss: 1.4219584465026855\n",
            "Epoch: 6, Batch: 0, Loss: 1.528254747390747\n",
            "Epoch: 7, Batch: 0, Loss: 1.746031403541565\n",
            "Epoch: 8, Batch: 0, Loss: 1.6486035585403442\n",
            "Epoch: 9, Batch: 0, Loss: 1.6557128429412842\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uplKL9EnvwhV"
      },
      "outputs": [],
      "source": [
        "# final train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxByKGy-vyYZ"
      },
      "outputs": [],
      "source": [
        "'''# Define hyperparameters\n",
        "INPUT_DIM = 28\n",
        "OUTPUT_DIM = 66\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "CELL_TYPE = 'gru'\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "EPOCHS = 10\n",
        "trg_pad_idx=0\n",
        "\n",
        "# Instantiate the Encoder and Decoder models\n",
        "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, CELL_TYPE).to(device)\n",
        "\n",
        "# Instantiate the Seq2Seq model with the Encoder and Decoder models\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (src, trg, src_len, trg_len, trg_one_hot) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        src = src.permute(1, 0)  # swapping the dimensions of src tensor\n",
        "        trg = trg.permute(1, 0)  # swapping the dimensions of trg tensor\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Ignore the first element of the output, which is initialized as all zeros\n",
        "        # since we use it to store the output for the start-of-sequence token\n",
        "        #print(output.shape[2])\n",
        "        \n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        #print(output.shape)\n",
        "        #print(trg.shape)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2aDR-jUyvbRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "319db677-6554-4430-f3e9-6a4b4e29aa61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Dataset load and Preprocessing\\ndef data_preprocess(path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   english_texts = []\\n   target_texts = []\\n   for index, row in df.iterrows():\\n        ip_text = row[\\'English\\']\\n        op_text = row[\\'Translated\\']\\n        if ip_text == \\'\\' or op_text == \\'\\':\\n            continue\\n        op_text = \"\\t\" + op_text + \"\\n\"\\n        ip_text = \"\\t\" + ip_text + \"\\n\"\\n        \\n        english_texts.append(ip_text)\\n        target_texts.append(op_text)\\n   MAX_LEN_input = max([len(txt) for txt in english_texts])\\n   MAX_LEN_target = max([len(txt) for txt in target_texts])\\n\\n   hindi_vocab = set()\\n   english_vocab = set()\\n\\n   for word in target_texts:\\n     for char in word:\\n       hindi_vocab.add(char)\\n\\n   for word in english_texts:\\n      for char in word:\\n        english_vocab.add(char)\\n\\n   hindi_list = sorted(list(hindi_vocab))\\n   english_list = sorted(list(english_vocab))\\n\\n\\n   encoder_tokens = len(english_list)\\n   decoder_tokens = len(hindi_list)\\n\\n   print(\"###############################\",decoder_tokens)\\n   print(\"###############################\",encoder_tokens)\\n\\n\\n        # Dict for char to index\\n   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\\n   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\\n   target_token_index[\"UNK\"] = decoder_tokens\\n\\n   #print(input_token_index)\\n\\n        # Dict for index to char\\n   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\\n   #print(inv_input_token_index)\\n   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\\n   decoder_tokens+=1\\n   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\\n\\ndef get_data(path,train_path):\\n   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\\n   df.dropna()\\n   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\\n   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\\n\\n   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\\n   print(decoder_target_data.shape)\\n   for i,target_text in enumerate(df[\"Translated\"]):\\n            #print(\"Ths is i\",i,target_text)\\n\\n            target_text = \\'\\t\\'+target_text+\\'\\n\\'  \\n            for t, char in enumerate(target_text):\\n              #print(t,char)\\n              if t > 0:\\n                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\\n            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\\n   print(\"done\")\\n   return ([[input_token_index[letter] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"English\"]]),   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list(\\'\\t\\'+word+\\'\\n\\')] for word in df[\"Translated\"]]),decoder_target_data\\n    \\n\\ndef load_data():\\n    \\n  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\\n  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\\n  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\\n\\n\\n  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\\n  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\\n  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\\n\\n\\n  encoder_train = [torch.tensor(lst) for lst in encoder_train]\\n  decoder_train = [torch.tensor(lst) for lst in decoder_train]\\n\\n  encoder_val = [torch.tensor(lst) for lst in encoder_val]\\n  decoder_val = [torch.tensor(lst) for lst in decoder_val]\\n  \\n  encoder_test = [torch.tensor(lst) for lst in encoder_test]\\n  decoder_test = [torch.tensor(lst) for lst in decoder_test]\\n\\n  # Pad the list of tensors to make them the same length\\n  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\\n  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\\n\\n  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\\n  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\\n\\n  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\\n  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\\n  \\n  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "'''# Dataset load and Preprocessing\n",
        "def data_preprocess(path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   english_texts = []\n",
        "   target_texts = []\n",
        "   for index, row in df.iterrows():\n",
        "        ip_text = row['English']\n",
        "        op_text = row['Translated']\n",
        "        if ip_text == '' or op_text == '':\n",
        "            continue\n",
        "        op_text = \"\\t\" + op_text + \"\\n\"\n",
        "        ip_text = \"\\t\" + ip_text + \"\\n\"\n",
        "        \n",
        "        english_texts.append(ip_text)\n",
        "        target_texts.append(op_text)\n",
        "   MAX_LEN_input = max([len(txt) for txt in english_texts])\n",
        "   MAX_LEN_target = max([len(txt) for txt in target_texts])\n",
        "\n",
        "   hindi_vocab = set()\n",
        "   english_vocab = set()\n",
        "\n",
        "   for word in target_texts:\n",
        "     for char in word:\n",
        "       hindi_vocab.add(char)\n",
        "\n",
        "   for word in english_texts:\n",
        "      for char in word:\n",
        "        english_vocab.add(char)\n",
        "\n",
        "   hindi_list = sorted(list(hindi_vocab))\n",
        "   english_list = sorted(list(english_vocab))\n",
        "\n",
        "\n",
        "   encoder_tokens = len(english_list)\n",
        "   decoder_tokens = len(hindi_list)\n",
        "\n",
        "   print(\"###############################\",decoder_tokens)\n",
        "   print(\"###############################\",encoder_tokens)\n",
        "\n",
        "\n",
        "        # Dict for char to index\n",
        "   input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\n",
        "   target_token_index = dict([(char, i) for i, char in enumerate(hindi_list)])\n",
        "   target_token_index[\"UNK\"] = decoder_tokens\n",
        "\n",
        "   #print(input_token_index)\n",
        "\n",
        "        # Dict for index to char\n",
        "   inv_input_token_index = dict({(value,key) for key,value in input_token_index.items()})\n",
        "   #print(inv_input_token_index)\n",
        "   inv_target_token_index = dict({(value,key) for key,value in target_token_index.items()})\n",
        "   decoder_tokens+=1\n",
        "   return MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index\n",
        "\n",
        "def get_data(path,train_path):\n",
        "   df = pd.read_csv(path, names=[\"English\", \"Translated\"], sep=\",\").astype(str)\n",
        "   df.dropna()\n",
        "   MAX_LEN_input,MAX_LEN_target,hindi_list,english_list,encoder_tokens,decoder_tokens,input_token_index,target_token_index=data_preprocess(train_path)\n",
        "   print(\"##############\",MAX_LEN_input,MAX_LEN_target)\n",
        "\n",
        "   decoder_target_data = np.zeros((df.shape[0], MAX_LEN_target,decoder_tokens), dtype=\"float32\")\n",
        "   print(decoder_target_data.shape)\n",
        "   for i,target_text in enumerate(df[\"Translated\"]):\n",
        "            #print(\"Ths is i\",i,target_text)\n",
        "\n",
        "            target_text = '\\t'+target_text+'\\n'  \n",
        "            for t, char in enumerate(target_text):\n",
        "              #print(t,char)\n",
        "              if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char if char in target_token_index else \"UNK\"]] = 1.0\n",
        "            decoder_target_data[i, t:,target_token_index[\"\\n\"]] = 1.0\n",
        "   print(\"done\")\n",
        "   return ([[input_token_index[letter] for letter in list('\\t'+word+'\\n')] for word in df[\"English\"]]),\\\n",
        "   ([[target_token_index[letter if letter in target_token_index else \"UNK\"] for letter in list('\\t'+word+'\\n')] for word in df[\"Translated\"]]),decoder_target_data\n",
        "    \n",
        "\n",
        "def load_data():\n",
        "    \n",
        "  train_path  =\"/content/aksharantar_sampled/hin/hin_train.csv\"\n",
        "  val_path  =\"/content/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "  test_path  =\"/content/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "\n",
        "  encoder_train,decoder_train,decoder_target_train =get_data(train_path,train_path)\n",
        "  encoder_val,decoder_val,decoder_target_val =get_data(val_path,train_path)\n",
        "  encoder_test,decoder_test,decoder_target_test =get_data(test_path,train_path)\n",
        "\n",
        "\n",
        "  encoder_train = [torch.tensor(lst) for lst in encoder_train]\n",
        "  decoder_train = [torch.tensor(lst) for lst in decoder_train]\n",
        "\n",
        "  encoder_val = [torch.tensor(lst) for lst in encoder_val]\n",
        "  decoder_val = [torch.tensor(lst) for lst in decoder_val]\n",
        "  \n",
        "  encoder_test = [torch.tensor(lst) for lst in encoder_test]\n",
        "  decoder_test = [torch.tensor(lst) for lst in decoder_test]\n",
        "\n",
        "  # Pad the list of tensors to make them the same length\n",
        "  encoder_train = torch.nn.utils.rnn.pad_sequence(encoder_train, batch_first=True, padding_value=0)\n",
        "  decoder_train = torch.nn.utils.rnn.pad_sequence(decoder_train, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_val = torch.nn.utils.rnn.pad_sequence(encoder_val, batch_first=True, padding_value=0)\n",
        "  decoder_val = torch.nn.utils.rnn.pad_sequence(decoder_val, batch_first=True, padding_value=0)\n",
        "\n",
        "  encoder_test = torch.nn.utils.rnn.pad_sequence(encoder_test, batch_first=True, padding_value=0)\n",
        "  decoder_test = torch.nn.utils.rnn.pad_sequence(decoder_test, batch_first=True, padding_value=0)\n",
        "  \n",
        "  return  encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fs2dax9v-SFC",
        "outputId": "1abe538e-9799-42e4-97a4-f2397331b4c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\\n\\nprint(\"##################################################################################################\")\\nprint(\"encoder_input\",encoder_train.shape)\\nprint(\"encoder_input\",encoder_train[0])\\n\\nprint(\"decoder_target\",decoder_train.shape)\\nprint(\"encoder_input\",decoder_train[0])\\n\\n\\nprint(\"decoder_target_train\",decoder_target_train[0])\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''encoder_train,decoder_train,decoder_target_train,encoder_val,decoder_val,decoder_target_val,encoder_test,decoder_test,decoder_target_test= load_data()\n",
        "\n",
        "print(\"##################################################################################################\")\n",
        "print(\"encoder_input\",encoder_train.shape)\n",
        "print(\"encoder_input\",encoder_train[0])\n",
        "\n",
        "print(\"decoder_target\",decoder_train.shape)\n",
        "print(\"encoder_input\",decoder_train[0])\n",
        "\n",
        "\n",
        "print(\"decoder_target_train\",decoder_target_train[0])\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}